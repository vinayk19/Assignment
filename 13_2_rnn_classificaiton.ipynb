{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13_2 rnn_classificaiton.ipynb",
      "provenance": [],
      "mount_file_id": "1LdgqIQf9vE_qbnyG6LhupxryJzoyd1lU",
      "authorship_tag": "ABX9TyO2dxgJLzMuhu8nI3puIcmq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayk19/Assignment/blob/master/13_2_rnn_classificaiton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6jXyrgcX5QY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14273e5f-aba2-4962-d4ec-db6c199790b3"
      },
      "source": [
        "cd /content/drive/My Drive/AI/basic"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/AI/basic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVW4Pf_iXnTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "# from torch.utils.data import DataLoader\n",
        "# from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import csv\n",
        "import gzip\n",
        "# from name_dataset import NameDataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6Wo1A2EX4B-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ec73736-84a0-494a-ae7b-8db7bca33415"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/AI/basic'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9sNgtDvh3rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NameDataset(Dataset):\n",
        "    \"\"\" Diabetes dataset.\"\"\"\n",
        "\n",
        "    # Initialize your data, download, etc.\n",
        "    def __init__(self, is_train_set=False):\n",
        "        filename = '/content/drive/My Drive/AI/basic/data/names_train/names_train.csv.gz' if is_train_set else '/content/drive/My Drive/AI/basic/data/names_train/names_test.csv.gz'\n",
        "        with gzip.open(filename, \"rt\") as f:\n",
        "            reader = csv.reader(f)\n",
        "            rows = list(reader)\n",
        "\n",
        "        self.names = [row[0] for row in rows]\n",
        "        self.countries = [row[1] for row in rows]\n",
        "        self.len = len(self.countries)\n",
        "\n",
        "        self.country_list = list(sorted(set(self.countries))) # learning it provides the complete list of the country\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.names[index], self.countries[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def get_countries(self):\n",
        "        return self.country_list\n",
        "\n",
        "    def get_country(self, id):\n",
        "        return self.country_list[id] #it will provide contry list (after wbing shorted) it will provide the ID\n",
        "\n",
        "    def get_country_id(self, country):\n",
        "        return self.country_list.index(country) #learnign to get the index of country ( secong collumn of csv file)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo1EFa8WQ_Gm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "863a8d85-e4e8-44db-94b7-5546799e780e"
      },
      "source": [
        "# Test the loader\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = NameDataset(False)\n",
        "    print(\"get_country\",dataset.get_countries())\n",
        "    print(dataset.get_country(4))\n",
        "    print(dataset.get_country_id('English'))\n",
        "\n",
        "    train_loader = DataLoader(dataset=dataset,\n",
        "                              batch_size=10,\n",
        "                              shuffle=True)\n",
        "\n",
        "    print(len(train_loader.dataset))\n",
        "    # for epoch in range(2):\n",
        "    #     for i, (names, countries) in enumerate(train_loader):\n",
        "    #         # Run your training process\n",
        "    #         print(epoch, i, \"names\", names, \"countries\", countries)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "get_country ['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese']\n",
            "English\n",
            "4\n",
            "6700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xUw1bTTX4OF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a83544d-913f-4a2c-ab44-96771f88d667"
      },
      "source": [
        "# Parameters and DataLoaders\n",
        "HIDDEN_SIZE = 100\n",
        "N_LAYERS = 2\n",
        "BATCH_SIZE = 256\n",
        "N_EPOCHS = 3\n",
        "\n",
        "test_dataset = NameDataset(is_train_set=False)\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "train_dataset = NameDataset(is_train_set=True)\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "N_COUNTRIES = len(train_dataset.get_countries())\n",
        "print(N_COUNTRIES, \"countries\")\n",
        "N_CHARS = 128  # ASCII"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18 countries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh84miSTz3dh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some utility functions\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def create_variable(tensor):\n",
        "    # Do cuda() before wrapping with variable\n",
        "    if torch.cuda.is_available():\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLqvfDXcz3aF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pad sequences and sort the tensor\n",
        "def pad_sequences(vectorized_seqs, seq_lengths, countries):\n",
        "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
        "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
        "\n",
        "    # Sort tensors by their length @can we ignore shortung ?\n",
        "    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
        "    seq_tensor = seq_tensor[perm_idx]\n",
        "\n",
        "    # Also sort the target (countries) in the same order\n",
        "    target = countries2tensor(countries)\n",
        "    if len(countries):\n",
        "        target = target[perm_idx]\n",
        "        \n",
        "       # Return variables\n",
        "    # DataParallel requires everything to be a Variable\n",
        "    return create_variable(seq_tensor), \\\n",
        "        create_variable(seq_lengths), \\\n",
        "        create_variable(target)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mjMYvzxz3WM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create necessary variables, lengths, and target\n",
        "def make_variables(names, countries):\n",
        "    print(\"\\n in input type\", type(names), \"input Contry \", type(countries))\n",
        "    print(\"\\nin input word\", names, \"\\ninput country\", countries)\n",
        "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
        "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
        "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
        "    print(\"\\n in vectorized seqs type\", type(vectorized_seqs), \"\\nSeq lengths \", type(seq_lengths))\n",
        "    print(\"\\n in vectorized seqs type\", vectorized_seqs, \"\\nSeq lengths \", seq_lengths, \"\\n outcountry\", countries, \"\\ncountry type\", type(countries))\n",
        "\n",
        "    return pad_sequences(vectorized_seqs, seq_lengths, countries)\n",
        "\n",
        "def str2ascii_arr(msg):\n",
        "    arr = [ord(c) for c in msg]\n",
        "    return arr, len(arr)\n",
        "\n",
        "def countries2tensor(countries):\n",
        "    country_ids = [train_dataset.get_country_id(\n",
        "        country) for country in countries]\n",
        "    return torch.LongTensor(country_ids)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdeSDquJz3Ss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    # Our model classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRIES, N_LAYERS)\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True): # output_size is for linear layer output ie N_country\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        print(\"inside model type\", type(input_size), type(hidden_size), type(output_size), type(n_layers))\n",
        "        print(\"inside model vlaue\", input_size, hidden_size, output_size, n_layers)\n",
        "        self.hidden_size = hidden_size # it is like output in RNN\n",
        "        self.n_layers = n_layers\n",
        "        self.n_directions = int(bidirectional) + 1\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          bidirectional=bidirectional)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, seq_lengths): #learning both ipput are Variable and torch.Tensor\n",
        "        # Note: we run this all at once (over the whole input sequence)\n",
        "        # input shape: B x S (input size)\n",
        "        # transpose to make S(sequence) x B (batch)\n",
        "        print(\"\\nforward input type\", type(input), \"\\nseq_lenght type\", type(seq_lengths))\n",
        "        print(\"\\nforwward input word\", input.shape, \"\\nseq_lenght\", seq_lengths.shape)\n",
        "        input = input.t()\n",
        "        batch_size = input.size(1)\n",
        "\n",
        "        # Make a hidden\n",
        "        hidden = self._init_hidden(batch_size)\n",
        "\n",
        "        print(\"\\n1embedding input\", input.shape)\n",
        "        # Embedding S x B -> S x B x I (embedding size)\n",
        "        embedded = self.embedding(input)\n",
        "        print(\"\\n2embedding\", embedded.shape)\n",
        "\n",
        "        print(\"\\n2seq_lengths.data.cpu().numpy()\", seq_lengths.data.cpu().numpy())\n",
        "        print(\"\\n2seq_lengths.data.cpu().numpy()\", type(seq_lengths.data.cpu().numpy()))\n",
        "        print(\"\\n2seq_lengths.data.cpu().numpy()\", seq_lengths.data.cpu().numpy().shape)\n",
        "\n",
        "        # print(\"\\n2seq_lengths.data.cpu().numpy()\", seq_lengths.data.cpu().numpy(), type(seq_lengths.data.cpu().numpy(), seq_lengths.data.cpu().numpy().shape))\n",
        "        # Pack them up nicely # what is pack padded sequence\n",
        "        gru_input = pack_padded_sequence(embedded, seq_lengths.data.cpu().numpy()) # LENGTH SHOULD BE TENSOR BUT IT IS NUMPY\n",
        "        print(\"\\n3Gru input\", type(gru_input), gru_input)\n",
        "\n",
        "        # To compact weights again call flatten_parameters().\n",
        "        self.gru.flatten_parameters() #learning what is in flattening \n",
        "        output, hidden = self.gru(gru_input, hidden)  #gru_iput is not tensor it is rnn_class\n",
        "\n",
        "        # Use the last layer output as FC's input\n",
        "        # No need to unpack, since we are going to use hidden\n",
        "        fc_output = self.fc(hidden[-1])\n",
        "        return fc_output\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.n_layers * self.n_directions,\n",
        "                             batch_size, self.hidden_size)\n",
        "        return create_variable(hidden)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RQ89FK-0zxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train cycle\n",
        "def train():\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (names, countries) in enumerate(train_loader, 1):\n",
        "        input, seq_lengths, target = make_variables(names, countries) #learning out are tensor and careate as Variable as well.\n",
        "        output = classifier(input, seq_lengths)\n",
        "        print(\"\\n output\", output.shape, type(output), type(target))\n",
        "        loss = criterion(output, target)\n",
        "        print(\"\\nloss value type is \", type(loss.data), type(loss.item()))\n",
        "        print(\"\\nloss value is \", loss.data, loss.item())\n",
        "        total_loss += loss.data[0]\n",
        "        # total_loss += loss.item()\n",
        "\n",
        "        classifier.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.2f}'.format(\n",
        "                time_since(start), epoch,  i *\n",
        "                len(names), len(train_loader.dataset),\n",
        "                100. * i * len(names) / len(train_loader.dataset),\n",
        "                total_loss / i * len(names)))\n",
        "\n",
        "    return total_loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtw04sCZ0hfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing cycle\n",
        "def test(name=None):\n",
        "    # Predict for a given name\n",
        "    if name:\n",
        "        input, seq_lengths, target = make_variables([name], [])\n",
        "        output = classifier(input, seq_lengths)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        country_id = pred.cpu().numpy()[0][0]\n",
        "        print(name, \"is\", train_dataset.get_country(country_id))\n",
        "        return\n",
        "\n",
        "    print(\"evaluating trained model ...\")\n",
        "    correct = 0\n",
        "    train_data_size = len(test_loader.dataset)\n",
        "\n",
        "    for names, countries in test_loader:\n",
        "        input, seq_lengths, target = make_variables(names, countries)\n",
        "        output = classifier(input, seq_lengths)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        correct, train_data_size, 100. * correct / train_data_size))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH_oz1Yz0hZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97bb42a9-a6f6-4a27-94bf-e43d2d84e6da"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    print(\"type\", type(N_CHARS), type(HIDDEN_SIZE), type(N_COUNTRIES), type(N_LAYERS))\n",
        "    print(\"value\", N_CHARS, HIDDEN_SIZE, N_COUNTRIES, N_LAYERS)\n",
        "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRIES, N_LAYERS)\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "        # dim = 0 [33, xxx] -> [11, ...], [11, ...], [11, ...] on 3 GPUs\n",
        "        classifier = nn.DataParallel(classifier)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        classifier.cuda()\n",
        "\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    start = time.time()\n",
        "    print(\"Training for %d epochs...\" % N_EPOCHS)\n",
        "    for epoch in range(1, N_EPOCHS + 1):\n",
        "        # Train cycle\n",
        "        train()\n",
        "\n",
        "        # Testing\n",
        "        test()\n",
        "\n",
        "        # Testing several samples\n",
        "        test(\"Sung\")\n",
        "        test(\"Jungwoo\")\n",
        "        test(\"Soojin\")\n",
        "        test(\"Nako\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type <class 'int'> <class 'int'> <class 'int'> <class 'int'>\n",
            "value 128 100 18 2\n",
            "inside model type <class 'int'> <class 'int'> <class 'int'> <class 'int'>\n",
            "inside model vlaue 128 100 18 2\n",
            "Training for 3 epochs...\n",
            "\n",
            " in input type <class 'tuple'> input Contry  <class 'tuple'>\n",
            "\n",
            "in input word ('Masih', 'Vikhrov', 'Paramoshkin', 'Asker', 'Adderley', 'Mahrov', 'Whiteley', 'Tulchinsky', 'Gerges', 'Gaber', 'Homentovsky', 'Komine', 'Belchenkov', 'Maciomhair', 'Anikin', 'Muntyan', 'Guzun', 'Dovgalevsky', 'Abboud', 'Abletsov', 'Lavigne', 'Dikusar', 'Mischenko', 'Bahtchivandzhi', 'Chepurenko', 'Shadhan', 'Nahas', 'Pendyurin', 'BeknazarYuzbashev', 'Timpe', 'Jankevich', 'Qureshi', 'Pechernikov', 'Estlick', 'Shamoon', 'Balagul', 'Ha', 'Tupikhin', 'Zimnitsky', 'Atopov', 'Mayer', 'Talkov', 'Sasada', 'Molevich', 'Divoky', 'Christie', 'Varaksin', 'Grosu', 'Najjar', 'Pyschev', 'Vilken', 'Cardozo', 'Zhivlyuk', 'Gauntlett', 'Jakhaev', 'Golovanov', 'Zavorokhin', 'Latham', 'Lake', 'Hlyupin', 'Minchenkov', 'Rosser', 'Nudatov', 'Varano', 'Mizoguchi', 'Rohin', 'Awetisoff', 'Moguchev', 'Gillespie', 'Dubanowski', 'Blackburn', 'Avik', 'Haenraets', 'Evdokimov', 'Roma', 'Chatterton', 'Tsakhilov', 'Craig', 'Awerbah', 'Vallance', 'Weston', 'Rich', 'Maloof', 'Hailov', 'Chuvilo', 'Veligorsky', 'Hajjar', 'Stephens', 'Lihosherstov', 'Goring', 'Farr', 'Matsushina', 'Emile', 'Murray', 'Riese', 'Haupt', 'Grot', 'Dubasov', 'Awad', 'Isaev', 'Avksentiev', 'Awtokratoff', 'Molyneux', 'Tchanov', 'Plamondon', 'Rahamimov', 'Libkin', 'Trampota', 'Walkden', 'Millington', 'Nurhamitov', 'Dzhunusov', 'Sneddon', 'Gronsky', 'Ujinov', 'Hatskevich', 'Anuprienko', 'Bekhtenev', 'Barclay', 'Mazza', 'Vedyashkin', 'Were', 'Alanson', 'Matsura', 'Deriglazov', 'Charlton', 'Finch', 'Coyle', 'Baiguzin', 'Agzamoff', 'Bajan', 'Fairhurst', 'Shamon', 'Wondracek', 'Tikhonov', 'Haanrade', 'Lynas', 'French', 'Mukke', 'Becke', 'Shamshurin', 'Abramtchikoff', 'Amersvoort', 'Lebinson', 'Clark', 'Abbatantuono', 'Shimamura', 'Devaney', 'Yudenkov', 'Casey', 'Norwood', 'Romero', 'Ingle', 'Terzi', 'Vikhlyaev', 'Tchangli', 'Tzarikaev', 'Murase', 'Cheptygmashev', 'Conneely', 'Khalifa', 'Raskob', 'Stumpf', 'Vilkovsky', 'Taube', 'Kedzierski', 'Yudakov', 'Venables', 'Austin', 'Kartaev', 'Bezrukih', 'Lutovich', 'Gorka', 'Chemlik', 'Makushev', 'Alybin', 'Ganem', 'Glockl', 'Dovjuk', 'Abagyan', 'Weng', 'Andrushkevich', 'Lian', 'Zhizhilev', 'Albinesku', 'Adriyanoff', 'Baheloff', 'Zozulya', 'Aspey', 'Mikhaltsev', 'Dikarevsky', 'Tudorovsky', 'Degarmo', 'Raikovsky', 'Atiyeh', 'Getsov', 'Rawlinson', 'Loi', 'Zhai', 'Lupichev', 'Hanek', 'Saikov', 'Kirwin', 'Divakov', 'Dernov', 'Porkhun', 'Gorchilin', 'Molloy', 'Sayegh', 'Mikhalchev', 'Tihonitsky', 'Denis', 'Janenko', 'Artyushkov', 'Turchenko', 'Dobryakov', 'Puscharovsky', 'Nagel', 'Baba', 'Jurko', 'Moshenkov', 'Onopko', 'Horri', 'Ondrikov', 'Jachnik', 'Roffey', 'Yakshin', 'Kinsella', 'Bishara', 'Zhemchugov', 'Maruschak', 'Ackary', 'Zini', 'Bahrah', 'Cowan', 'Baitoff', 'Serafin', 'Maksumov', 'Shahrai', 'To The First Page', 'Koury', 'Mansour', 'Avhimovitch', 'Finyutin', 'Bahr', 'Veltischev', 'Dalby', 'Saliba', 'Amelkin', 'Murogov', 'Zhikharev', 'Moghadam', 'Natochin', 'Mihelyus', 'Batchish', 'Golovatsky') \n",
            "input country ('Arabic', 'Russian', 'Russian', 'Arabic', 'English', 'Russian', 'English', 'Russian', 'Arabic', 'Arabic', 'Russian', 'Japanese', 'Russian', 'Irish', 'Russian', 'Russian', 'Russian', 'Russian', 'Arabic', 'Russian', 'French', 'Russian', 'Russian', 'Russian', 'Russian', 'Russian', 'Arabic', 'Russian', 'Russian', 'Czech', 'Russian', 'Arabic', 'Russian', 'English', 'Arabic', 'Russian', 'Korean', 'Russian', 'Russian', 'Russian', 'German', 'Russian', 'Japanese', 'Russian', 'Czech', 'English', 'Russian', 'Russian', 'Arabic', 'Russian', 'Russian', 'Portuguese', 'Russian', 'English', 'Russian', 'Russian', 'Russian', 'English', 'English', 'Russian', 'Russian', 'English', 'Russian', 'Italian', 'Japanese', 'Russian', 'Russian', 'Russian', 'English', 'Polish', 'English', 'Russian', 'Dutch', 'Russian', 'Italian', 'English', 'Russian', 'Scottish', 'Russian', 'English', 'English', 'English', 'Arabic', 'Russian', 'Russian', 'Russian', 'Arabic', 'English', 'Russian', 'English', 'English', 'Japanese', 'French', 'Scottish', 'German', 'German', 'Russian', 'Russian', 'Arabic', 'Russian', 'Russian', 'Russian', 'English', 'Russian', 'French', 'Russian', 'Russian', 'Czech', 'English', 'English', 'Russian', 'Russian', 'English', 'Russian', 'Russian', 'Russian', 'Russian', 'Russian', 'English', 'Italian', 'Russian', 'English', 'English', 'Japanese', 'Russian', 'English', 'English', 'English', 'Russian', 'Russian', 'Russian', 'English', 'Arabic', 'Czech', 'Russian', 'Dutch', 'English', 'English', 'Russian', 'German', 'Russian', 'Russian', 'Dutch', 'Russian', 'Scottish', 'Italian', 'Japanese', 'English', 'Russian', 'Irish', 'English', 'Spanish', 'English', 'Italian', 'Russian', 'Russian', 'Russian', 'Japanese', 'Russian', 'English', 'English', 'German', 'German', 'Russian', 'Russian', 'Polish', 'Russian', 'English', 'English', 'Russian', 'Russian', 'Russian', 'Polish', 'Czech', 'Russian', 'Russian', 'Arabic', 'Czech', 'Russian', 'Russian', 'Chinese', 'Russian', 'Chinese', 'Russian', 'Russian', 'Russian', 'Russian', 'Russian', 'English', 'Russian', 'Russian', 'Russian', 'French', 'Russian', 'Arabic', 'Russian', 'English', 'Russian', 'Chinese', 'Russian', 'Czech', 'Russian', 'English', 'Russian', 'Russian', 'Russian', 'Russian', 'Irish', 'Arabic', 'Russian', 'Russian', 'English', 'Russian', 'Russian', 'Russian', 'Russian', 'Russian', 'Dutch', 'Arabic', 'Russian', 'Russian', 'Russian', 'Japanese', 'Russian', 'Russian', 'English', 'Russian', 'English', 'Arabic', 'Russian', 'Russian', 'English', 'Italian', 'Russian', 'English', 'Russian', 'Polish', 'Russian', 'Russian', 'Russian', 'Arabic', 'Arabic', 'Russian', 'Russian', 'German', 'Russian', 'English', 'Arabic', 'Russian', 'Russian', 'Russian', 'Arabic', 'Russian', 'Russian', 'Russian', 'Russian')\n",
            "\n",
            " in vectorized seqs type <class 'list'> \n",
            "Seq lengths  <class 'torch.Tensor'>\n",
            "\n",
            " in vectorized seqs type [[77, 97, 115, 105, 104], [86, 105, 107, 104, 114, 111, 118], [80, 97, 114, 97, 109, 111, 115, 104, 107, 105, 110], [65, 115, 107, 101, 114], [65, 100, 100, 101, 114, 108, 101, 121], [77, 97, 104, 114, 111, 118], [87, 104, 105, 116, 101, 108, 101, 121], [84, 117, 108, 99, 104, 105, 110, 115, 107, 121], [71, 101, 114, 103, 101, 115], [71, 97, 98, 101, 114], [72, 111, 109, 101, 110, 116, 111, 118, 115, 107, 121], [75, 111, 109, 105, 110, 101], [66, 101, 108, 99, 104, 101, 110, 107, 111, 118], [77, 97, 99, 105, 111, 109, 104, 97, 105, 114], [65, 110, 105, 107, 105, 110], [77, 117, 110, 116, 121, 97, 110], [71, 117, 122, 117, 110], [68, 111, 118, 103, 97, 108, 101, 118, 115, 107, 121], [65, 98, 98, 111, 117, 100], [65, 98, 108, 101, 116, 115, 111, 118], [76, 97, 118, 105, 103, 110, 101], [68, 105, 107, 117, 115, 97, 114], [77, 105, 115, 99, 104, 101, 110, 107, 111], [66, 97, 104, 116, 99, 104, 105, 118, 97, 110, 100, 122, 104, 105], [67, 104, 101, 112, 117, 114, 101, 110, 107, 111], [83, 104, 97, 100, 104, 97, 110], [78, 97, 104, 97, 115], [80, 101, 110, 100, 121, 117, 114, 105, 110], [66, 101, 107, 110, 97, 122, 97, 114, 89, 117, 122, 98, 97, 115, 104, 101, 118], [84, 105, 109, 112, 101], [74, 97, 110, 107, 101, 118, 105, 99, 104], [81, 117, 114, 101, 115, 104, 105], [80, 101, 99, 104, 101, 114, 110, 105, 107, 111, 118], [69, 115, 116, 108, 105, 99, 107], [83, 104, 97, 109, 111, 111, 110], [66, 97, 108, 97, 103, 117, 108], [72, 97], [84, 117, 112, 105, 107, 104, 105, 110], [90, 105, 109, 110, 105, 116, 115, 107, 121], [65, 116, 111, 112, 111, 118], [77, 97, 121, 101, 114], [84, 97, 108, 107, 111, 118], [83, 97, 115, 97, 100, 97], [77, 111, 108, 101, 118, 105, 99, 104], [68, 105, 118, 111, 107, 121], [67, 104, 114, 105, 115, 116, 105, 101], [86, 97, 114, 97, 107, 115, 105, 110], [71, 114, 111, 115, 117], [78, 97, 106, 106, 97, 114], [80, 121, 115, 99, 104, 101, 118], [86, 105, 108, 107, 101, 110], [67, 97, 114, 100, 111, 122, 111], [90, 104, 105, 118, 108, 121, 117, 107], [71, 97, 117, 110, 116, 108, 101, 116, 116], [74, 97, 107, 104, 97, 101, 118], [71, 111, 108, 111, 118, 97, 110, 111, 118], [90, 97, 118, 111, 114, 111, 107, 104, 105, 110], [76, 97, 116, 104, 97, 109], [76, 97, 107, 101], [72, 108, 121, 117, 112, 105, 110], [77, 105, 110, 99, 104, 101, 110, 107, 111, 118], [82, 111, 115, 115, 101, 114], [78, 117, 100, 97, 116, 111, 118], [86, 97, 114, 97, 110, 111], [77, 105, 122, 111, 103, 117, 99, 104, 105], [82, 111, 104, 105, 110], [65, 119, 101, 116, 105, 115, 111, 102, 102], [77, 111, 103, 117, 99, 104, 101, 118], [71, 105, 108, 108, 101, 115, 112, 105, 101], [68, 117, 98, 97, 110, 111, 119, 115, 107, 105], [66, 108, 97, 99, 107, 98, 117, 114, 110], [65, 118, 105, 107], [72, 97, 101, 110, 114, 97, 101, 116, 115], [69, 118, 100, 111, 107, 105, 109, 111, 118], [82, 111, 109, 97], [67, 104, 97, 116, 116, 101, 114, 116, 111, 110], [84, 115, 97, 107, 104, 105, 108, 111, 118], [67, 114, 97, 105, 103], [65, 119, 101, 114, 98, 97, 104], [86, 97, 108, 108, 97, 110, 99, 101], [87, 101, 115, 116, 111, 110], [82, 105, 99, 104], [77, 97, 108, 111, 111, 102], [72, 97, 105, 108, 111, 118], [67, 104, 117, 118, 105, 108, 111], [86, 101, 108, 105, 103, 111, 114, 115, 107, 121], [72, 97, 106, 106, 97, 114], [83, 116, 101, 112, 104, 101, 110, 115], [76, 105, 104, 111, 115, 104, 101, 114, 115, 116, 111, 118], [71, 111, 114, 105, 110, 103], [70, 97, 114, 114], [77, 97, 116, 115, 117, 115, 104, 105, 110, 97], [69, 109, 105, 108, 101], [77, 117, 114, 114, 97, 121], [82, 105, 101, 115, 101], [72, 97, 117, 112, 116], [71, 114, 111, 116], [68, 117, 98, 97, 115, 111, 118], [65, 119, 97, 100], [73, 115, 97, 101, 118], [65, 118, 107, 115, 101, 110, 116, 105, 101, 118], [65, 119, 116, 111, 107, 114, 97, 116, 111, 102, 102], [77, 111, 108, 121, 110, 101, 117, 120], [84, 99, 104, 97, 110, 111, 118], [80, 108, 97, 109, 111, 110, 100, 111, 110], [82, 97, 104, 97, 109, 105, 109, 111, 118], [76, 105, 98, 107, 105, 110], [84, 114, 97, 109, 112, 111, 116, 97], [87, 97, 108, 107, 100, 101, 110], [77, 105, 108, 108, 105, 110, 103, 116, 111, 110], [78, 117, 114, 104, 97, 109, 105, 116, 111, 118], [68, 122, 104, 117, 110, 117, 115, 111, 118], [83, 110, 101, 100, 100, 111, 110], [71, 114, 111, 110, 115, 107, 121], [85, 106, 105, 110, 111, 118], [72, 97, 116, 115, 107, 101, 118, 105, 99, 104], [65, 110, 117, 112, 114, 105, 101, 110, 107, 111], [66, 101, 107, 104, 116, 101, 110, 101, 118], [66, 97, 114, 99, 108, 97, 121], [77, 97, 122, 122, 97], [86, 101, 100, 121, 97, 115, 104, 107, 105, 110], [87, 101, 114, 101], [65, 108, 97, 110, 115, 111, 110], [77, 97, 116, 115, 117, 114, 97], [68, 101, 114, 105, 103, 108, 97, 122, 111, 118], [67, 104, 97, 114, 108, 116, 111, 110], [70, 105, 110, 99, 104], [67, 111, 121, 108, 101], [66, 97, 105, 103, 117, 122, 105, 110], [65, 103, 122, 97, 109, 111, 102, 102], [66, 97, 106, 97, 110], [70, 97, 105, 114, 104, 117, 114, 115, 116], [83, 104, 97, 109, 111, 110], [87, 111, 110, 100, 114, 97, 99, 101, 107], [84, 105, 107, 104, 111, 110, 111, 118], [72, 97, 97, 110, 114, 97, 100, 101], [76, 121, 110, 97, 115], [70, 114, 101, 110, 99, 104], [77, 117, 107, 107, 101], [66, 101, 99, 107, 101], [83, 104, 97, 109, 115, 104, 117, 114, 105, 110], [65, 98, 114, 97, 109, 116, 99, 104, 105, 107, 111, 102, 102], [65, 109, 101, 114, 115, 118, 111, 111, 114, 116], [76, 101, 98, 105, 110, 115, 111, 110], [67, 108, 97, 114, 107], [65, 98, 98, 97, 116, 97, 110, 116, 117, 111, 110, 111], [83, 104, 105, 109, 97, 109, 117, 114, 97], [68, 101, 118, 97, 110, 101, 121], [89, 117, 100, 101, 110, 107, 111, 118], [67, 97, 115, 101, 121], [78, 111, 114, 119, 111, 111, 100], [82, 111, 109, 101, 114, 111], [73, 110, 103, 108, 101], [84, 101, 114, 122, 105], [86, 105, 107, 104, 108, 121, 97, 101, 118], [84, 99, 104, 97, 110, 103, 108, 105], [84, 122, 97, 114, 105, 107, 97, 101, 118], [77, 117, 114, 97, 115, 101], [67, 104, 101, 112, 116, 121, 103, 109, 97, 115, 104, 101, 118], [67, 111, 110, 110, 101, 101, 108, 121], [75, 104, 97, 108, 105, 102, 97], [82, 97, 115, 107, 111, 98], [83, 116, 117, 109, 112, 102], [86, 105, 108, 107, 111, 118, 115, 107, 121], [84, 97, 117, 98, 101], [75, 101, 100, 122, 105, 101, 114, 115, 107, 105], [89, 117, 100, 97, 107, 111, 118], [86, 101, 110, 97, 98, 108, 101, 115], [65, 117, 115, 116, 105, 110], [75, 97, 114, 116, 97, 101, 118], [66, 101, 122, 114, 117, 107, 105, 104], [76, 117, 116, 111, 118, 105, 99, 104], [71, 111, 114, 107, 97], [67, 104, 101, 109, 108, 105, 107], [77, 97, 107, 117, 115, 104, 101, 118], [65, 108, 121, 98, 105, 110], [71, 97, 110, 101, 109], [71, 108, 111, 99, 107, 108], [68, 111, 118, 106, 117, 107], [65, 98, 97, 103, 121, 97, 110], [87, 101, 110, 103], [65, 110, 100, 114, 117, 115, 104, 107, 101, 118, 105, 99, 104], [76, 105, 97, 110], [90, 104, 105, 122, 104, 105, 108, 101, 118], [65, 108, 98, 105, 110, 101, 115, 107, 117], [65, 100, 114, 105, 121, 97, 110, 111, 102, 102], [66, 97, 104, 101, 108, 111, 102, 102], [90, 111, 122, 117, 108, 121, 97], [65, 115, 112, 101, 121], [77, 105, 107, 104, 97, 108, 116, 115, 101, 118], [68, 105, 107, 97, 114, 101, 118, 115, 107, 121], [84, 117, 100, 111, 114, 111, 118, 115, 107, 121], [68, 101, 103, 97, 114, 109, 111], [82, 97, 105, 107, 111, 118, 115, 107, 121], [65, 116, 105, 121, 101, 104], [71, 101, 116, 115, 111, 118], [82, 97, 119, 108, 105, 110, 115, 111, 110], [76, 111, 105], [90, 104, 97, 105], [76, 117, 112, 105, 99, 104, 101, 118], [72, 97, 110, 101, 107], [83, 97, 105, 107, 111, 118], [75, 105, 114, 119, 105, 110], [68, 105, 118, 97, 107, 111, 118], [68, 101, 114, 110, 111, 118], [80, 111, 114, 107, 104, 117, 110], [71, 111, 114, 99, 104, 105, 108, 105, 110], [77, 111, 108, 108, 111, 121], [83, 97, 121, 101, 103, 104], [77, 105, 107, 104, 97, 108, 99, 104, 101, 118], [84, 105, 104, 111, 110, 105, 116, 115, 107, 121], [68, 101, 110, 105, 115], [74, 97, 110, 101, 110, 107, 111], [65, 114, 116, 121, 117, 115, 104, 107, 111, 118], [84, 117, 114, 99, 104, 101, 110, 107, 111], [68, 111, 98, 114, 121, 97, 107, 111, 118], [80, 117, 115, 99, 104, 97, 114, 111, 118, 115, 107, 121], [78, 97, 103, 101, 108], [66, 97, 98, 97], [74, 117, 114, 107, 111], [77, 111, 115, 104, 101, 110, 107, 111, 118], [79, 110, 111, 112, 107, 111], [72, 111, 114, 114, 105], [79, 110, 100, 114, 105, 107, 111, 118], [74, 97, 99, 104, 110, 105, 107], [82, 111, 102, 102, 101, 121], [89, 97, 107, 115, 104, 105, 110], [75, 105, 110, 115, 101, 108, 108, 97], [66, 105, 115, 104, 97, 114, 97], [90, 104, 101, 109, 99, 104, 117, 103, 111, 118], [77, 97, 114, 117, 115, 99, 104, 97, 107], [65, 99, 107, 97, 114, 121], [90, 105, 110, 105], [66, 97, 104, 114, 97, 104], [67, 111, 119, 97, 110], [66, 97, 105, 116, 111, 102, 102], [83, 101, 114, 97, 102, 105, 110], [77, 97, 107, 115, 117, 109, 111, 118], [83, 104, 97, 104, 114, 97, 105], [84, 111, 32, 84, 104, 101, 32, 70, 105, 114, 115, 116, 32, 80, 97, 103, 101], [75, 111, 117, 114, 121], [77, 97, 110, 115, 111, 117, 114], [65, 118, 104, 105, 109, 111, 118, 105, 116, 99, 104], [70, 105, 110, 121, 117, 116, 105, 110], [66, 97, 104, 114], [86, 101, 108, 116, 105, 115, 99, 104, 101, 118], [68, 97, 108, 98, 121], [83, 97, 108, 105, 98, 97], [65, 109, 101, 108, 107, 105, 110], [77, 117, 114, 111, 103, 111, 118], [90, 104, 105, 107, 104, 97, 114, 101, 118], [77, 111, 103, 104, 97, 100, 97, 109], [78, 97, 116, 111, 99, 104, 105, 110], [77, 105, 104, 101, 108, 121, 117, 115], [66, 97, 116, 99, 104, 105, 115, 104], [71, 111, 108, 111, 118, 97, 116, 115, 107, 121]] \n",
            "Seq lengths  tensor([ 5,  7, 11,  5,  8,  6,  8, 10,  6,  5, 11,  6, 10, 10,  6,  7,  5, 11,\n",
            "         6,  8,  7,  7,  9, 14, 10,  7,  5,  9, 17,  5,  9,  7, 11,  7,  7,  7,\n",
            "         2,  8,  9,  6,  5,  6,  6,  8,  6,  8,  8,  5,  6,  7,  6,  7,  8,  9,\n",
            "         7,  9, 10,  6,  4,  7, 10,  6,  7,  6,  9,  5,  9,  8,  9, 10,  9,  4,\n",
            "         9,  9,  4, 10,  9,  5,  7,  8,  6,  4,  6,  6,  7, 10,  6,  8, 12,  6,\n",
            "         4, 10,  5,  6,  5,  5,  4,  7,  4,  5, 10, 11,  8,  7,  9,  9,  6,  8,\n",
            "         7, 10, 10,  9,  7,  7,  6, 10, 10,  9,  7,  5, 10,  4,  7,  7, 10,  8,\n",
            "         5,  5,  8,  8,  5,  9,  6,  9,  8,  8,  5,  6,  5,  5, 10, 13, 10,  8,\n",
            "         5, 12,  9,  7,  8,  5,  7,  6,  5,  5,  9,  8,  9,  6, 13,  8,  7,  6,\n",
            "         6,  9,  5, 10,  7,  8,  6,  7,  8,  8,  5,  7,  8,  6,  5,  6,  6,  7,\n",
            "         4, 13,  4,  9,  9, 10,  8,  7,  5, 10, 10, 10,  7,  9,  6,  6,  9,  3,\n",
            "         4,  8,  5,  6,  6,  7,  6,  7,  9,  6,  6, 10, 10,  5,  7, 10,  9,  9,\n",
            "        12,  5,  4,  5,  9,  6,  5,  8,  7,  6,  7,  8,  7, 10,  9,  6,  4,  6,\n",
            "         5,  7,  7,  8,  7, 17,  5,  7, 11,  8,  4, 10,  5,  6,  7,  7,  9,  8,\n",
            "         8,  8,  8, 10]) \n",
            " outcountry ('Arabic', 'Russian', 'Russian', 'Arabic', 'English', 'Russian', 'English', 'Russian', 'Arabic', 'Arabic', 'Russian', 'Japanese', 'Russian', 'Irish', 'Russian', 'Russian', 'Russian', 'Russian', 'Arabic', 'Russian', 'French', 'Russian', 'Russian', 'Russian', 'Russian', 'Russian', 'Arabic', 'Russian', 'Russian', 'Czech', 'Russian', 'Arabic', 'Russian', 'English', 'Arabic', 'Russian', 'Korean', 'Russian', 'Russian', 'Russian', 'German', 'Russian', 'Japanese', 'Russian', 'Czech', 'English', 'Russian', 'Russian', 'Arabic', 'Russian', 'Russian', 'Portuguese', 'Russian', 'English', 'Russian', 'Russian', 'Russian', 'English', 'English', 'Russian', 'Russian', 'English', 'Russian', 'Italian', 'Japanese', 'Russian', 'Russian', 'Russian', 'English', 'Polish', 'English', 'Russian', 'Dutch', 'Russian', 'Italian', 'English', 'Russian', 'Scottish', 'Russian', 'English', 'English', 'English', 'Arabic', 'Russian', 'Russian', 'Russian', 'Arabic', 'English', 'Russian', 'English', 'English', 'Japanese', 'French', 'Scottish', 'German', 'German', 'Russian', 'Russian', 'Arabic', 'Russian', 'Russian', 'Russian', 'English', 'Russian', 'French', 'Russian', 'Russian', 'Czech', 'English', 'English', 'Russian', 'Russian', 'English', 'Russian', 'Russian', 'Russian', 'Russian', 'Russian', 'English', 'Italian', 'Russian', 'English', 'English', 'Japanese', 'Russian', 'English', 'English', 'English', 'Russian', 'Russian', 'Russian', 'English', 'Arabic', 'Czech', 'Russian', 'Dutch', 'English', 'English', 'Russian', 'German', 'Russian', 'Russian', 'Dutch', 'Russian', 'Scottish', 'Italian', 'Japanese', 'English', 'Russian', 'Irish', 'English', 'Spanish', 'English', 'Italian', 'Russian', 'Russian', 'Russian', 'Japanese', 'Russian', 'English', 'English', 'German', 'German', 'Russian', 'Russian', 'Polish', 'Russian', 'English', 'English', 'Russian', 'Russian', 'Russian', 'Polish', 'Czech', 'Russian', 'Russian', 'Arabic', 'Czech', 'Russian', 'Russian', 'Chinese', 'Russian', 'Chinese', 'Russian', 'Russian', 'Russian', 'Russian', 'Russian', 'English', 'Russian', 'Russian', 'Russian', 'French', 'Russian', 'Arabic', 'Russian', 'English', 'Russian', 'Chinese', 'Russian', 'Czech', 'Russian', 'English', 'Russian', 'Russian', 'Russian', 'Russian', 'Irish', 'Arabic', 'Russian', 'Russian', 'English', 'Russian', 'Russian', 'Russian', 'Russian', 'Russian', 'Dutch', 'Arabic', 'Russian', 'Russian', 'Russian', 'Japanese', 'Russian', 'Russian', 'English', 'Russian', 'English', 'Arabic', 'Russian', 'Russian', 'English', 'Italian', 'Russian', 'English', 'Russian', 'Polish', 'Russian', 'Russian', 'Russian', 'Arabic', 'Arabic', 'Russian', 'Russian', 'German', 'Russian', 'English', 'Arabic', 'Russian', 'Russian', 'Russian', 'Arabic', 'Russian', 'Russian', 'Russian', 'Russian') \n",
            "country type <class 'tuple'>\n",
            "\n",
            "forward input type <class 'torch.Tensor'> \n",
            "seq_lenght type <class 'torch.Tensor'>\n",
            "\n",
            "forwward input word torch.Size([256, 17]) \n",
            "seq_lenght torch.Size([256])\n",
            "\n",
            "1embedding input torch.Size([17, 256])\n",
            "\n",
            "2embedding torch.Size([17, 256, 100])\n",
            "\n",
            "2seq_lengths.data.cpu().numpy() [17 17 14 13 13 13 12 12 12 11 11 11 11 11 11 10 10 10 10 10 10 10 10 10\n",
            " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10  9  9  9\n",
            "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9\n",
            "  9  9  9  9  9  9  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
            "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  7  7  7  7  7  7\n",
            "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
            "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  6  6  6  6  6  6  6  6  6\n",
            "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
            "  6  6  6  6  6  6  6  6  6  6  6  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
            "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
            "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  3  2]\n",
            "\n",
            "2seq_lengths.data.cpu().numpy() <class 'numpy.ndarray'>\n",
            "\n",
            "2seq_lengths.data.cpu().numpy() (256,)\n",
            "\n",
            "3Gru input <class 'torch.nn.utils.rnn.PackedSequence'> PackedSequence(data=tensor([[ 0.2625,  0.7766,  0.1644,  ..., -0.0805,  2.1828, -1.2776],\n",
            "        [-0.5264, -0.0778,  2.0285,  ...,  0.1082,  1.8982,  0.2952],\n",
            "        [ 0.2625,  0.7766,  0.1644,  ..., -0.0805,  2.1828, -1.2776],\n",
            "        ...,\n",
            "        [-0.5547,  0.8533,  0.3328,  ...,  0.4957, -0.3961, -1.1930],\n",
            "        [-0.6039,  0.2008,  1.7329,  ...,  2.1852,  0.9744,  0.9811],\n",
            "        [-0.7987, -0.2263,  0.9242,  ...,  0.2410,  0.2122, -2.0995]],\n",
            "       grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([256, 256, 255, 254, 240, 203, 159, 114,  78,  45,  15,   9,   6,   3,\n",
            "          2,   2,   2]), sorted_indices=None, unsorted_indices=None)\n",
            "\n",
            " output torch.Size([256, 18]) <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "\n",
            "loss value type is  <class 'torch.Tensor'> <class 'float'>\n",
            "\n",
            "loss value is  tensor(2.8624) 2.8624441623687744\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-aa05572d0783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_EPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Train cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-84ce496c9f9a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nloss value type is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nloss value is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# total_loss += loss.item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or5Tr7Qg0hVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASI72vUN0hRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M18awy5T0hFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}