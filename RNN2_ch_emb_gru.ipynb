{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN2_ch emb_gru.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN80KaxHesWDENoLGu+8Rsp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayk19/Assignment/blob/master/RNN2_ch_emb_gru.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhvh4JLHXDHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeGofh2CXZsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " name = \"vinay\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0jCVYl7X7k_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def str2ascii(name):\n",
        "  arr = [ord(c) for c in name] #print(ord(\"v\")) value is 118\n",
        "  return arr, len(arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO3xDpamak5a",
        "colab_type": "text"
      },
      "source": [
        " send char > ascii > embedding (in , out) > gru (in, out_hidden) > fully Nural network > : model, criterian, optimizer, training ( loss ( input and label) > is print loss.data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtMFZaMUiCn_",
        "colab_type": "text"
      },
      "source": [
        "model: put no of layer inside it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTr0QQjkX94C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class net(nn.Module):\n",
        "#   def __init__(self, input_size, hidden_size, n_class, n_layer =1):\n",
        "#     super(net, self).__init__()\n",
        "\n",
        "#     # self.ascii_out = arr\n",
        "#     self.size_each_embed = hideen_size #6\n",
        "#     self.hidden0 = self.each_size_embed\n",
        "#     self.ascii_out = input_size #\n",
        "#     self.num_class = n_class # 18\n",
        "#     # self.seq_len = len(arr)\n",
        "#     # self.batch = 1\n",
        "#     self.num_layer = n_layer\n",
        "#     # self. each_size_embed = 6\n",
        "\n",
        "#     self.embed = nn.Embedding(num_embeddings = ascii_out, embedding_dim= size_each_embed) # input 118( asci for v) and output (6 nos )\n",
        "#     self.cell = nn.GRU(input_size=size_each_embed, hidden_size= size_each_embed, batch_first= True) # in 6 and out 6 nos\n",
        "#     self.fc = nn.Linear(in_features=size_each_embed, out_features=self.num_class)\n",
        "\n",
        "#   def forward(self, input): #input asccii of vinay ie 1*5 B*S #learning batch is required here only \n",
        "#     batch = input.size(0)\n",
        "\n",
        "\n",
        "#     out_embed = self.embed(input) #input 1*5 then out will be 1*5*6 (size_each_embed added)\n",
        "#     print(\"embedding out\", out_embed.size())\n",
        "#     input = out_embed.view(len(arr), self.batch, out_embed.shape[1,1,:]) # S,B,I seq_len = len of asciii ie 5 ch ,batch =1, input_size= output size of embed =6) mine no of character = 6\n",
        "    \n",
        "#     out, h_n = self.cell(input, hidden) # hidden num_layer*num_dir, batch, hidden_out\n",
        "#       # out(due to gru)=[5*1*6] S,B,hiddn_0  & h_n =[1*1*6]  num_layer, num_dir, batchn, hout\n",
        "#     out = fc(h_n) # in 1*1*6 out 1*1*18\n",
        "   \n",
        "#     out = out.view(-1, num_class)\n",
        "#     return out\n",
        "\n",
        "# def hidden_init(self):\n",
        "#     hidden = torch.zeros(self.num_layer, self.batch, self.hiddenO)\n",
        "#     return hidden  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7RGjWaGJHSi",
        "colab_type": "text"
      },
      "source": [
        "batch_first is false"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XwdVe4XJGzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class net(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, n_class, n_layer =1):\n",
        "    super(net, self).__init__()\n",
        "\n",
        "    # self.ascii_out = arr\n",
        "    self.size_each_embed = hidden_size #6\n",
        "    self.hiddenO = self.size_each_embed\n",
        "    self.ascii_out = input_size #\n",
        "    self.num_class = n_class # 18\n",
        "    # self.seq_len = len(arr)\n",
        "    # self.batch = 1\n",
        "    self.num_layer = n_layer\n",
        "    # self. size_each_embed = 6\n",
        "\n",
        "    # self.embed = nn.Embedding(num_embeddings = self.ascii_out, embedding_dim= self.size_each_embed) # input 118( asci for v) and output (6 nos )\n",
        "    self.embed = nn.Embedding(input_size, hidden_size ) # num_embeddings = self.ascii_out, embedding_dim= self.size_each_embed) # input 118( asci for v) and output (6 nos )\n",
        "    self.cell = nn.GRU(input_size=self.size_each_embed, hidden_size= self.size_each_embed) # in 6 and out 6 nos\n",
        "    self.fc = nn.Linear(in_features=self.size_each_embed, out_features=self.num_class)\n",
        "\n",
        "  def forward(self, input): #input asccii of vinay ie 1*5 B*S #learning batch is required here only \n",
        "    \n",
        "    batch = input.size(0)\n",
        "    \n",
        "    # importance lerning embedding does S,B --> S,B,I (i input dim) check why not B S  to B S I\n",
        "    input = input.t() # BxS --> SxB\n",
        "    print(\"input size pre embed\", input.size())\n",
        "    out_embed = self.embed(input) #input 5*1 SxB --> out will be 5*1*6 SxBxI (size_each_embed added)\n",
        "    print(\"embedding out\", out_embed.size())\n",
        "    # input = out_embed.view(len(arr), self.batch, out_embed.shape[1,1,:]) # S,B,I seq_len = len of asciii ie 5 ch ,batch =1, input_size= output size of embed =6) mine no of character = 6\n",
        "    input = out_embed # SxBxI\n",
        "    hidden = self.hidden_init(batch)\n",
        "    out, h_n = self.cell(input, hidden) # hidden num_layer*num_dir, batch, hidden_out\n",
        "      # out(due to gru)=[5*1*6] S,B,hiddn_0  & h_n =[1*1*6]  num_layer, num_dir, batchn, hout\n",
        "    out = self.fc(h_n) # in 1*1*6 out 1*1*18\n",
        "    print(\"h_n sise\", h_n.size(),\"fc out\", out.size())\n",
        "    # out = out.view(-1, self.num_class)  #why not used here : \n",
        "    return out\n",
        "\n",
        "  def hidden_init(self, batch_size): # to have underscore for each fucniton inside hidden_init\n",
        "    hidden = torch.zeros(self.num_layer, batch_size, self.hiddenO)\n",
        "    return hidden "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRG2Be8sPlIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv3U8WyVX-CS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_char = 128  #ASCII (not no of character)\n",
        "N_class = 18\n",
        "hidden_size = 6\n",
        "if __name__ == '__main__':\n",
        "  names = ['adulov', 'solan', 'solan', 'son'] \n",
        "  classifier = net(N_char, hidden_size, N_class )\n",
        "  for name in names:\n",
        "    arr, arr2 = str2ascii(name)\n",
        "    # print(\"arr\", arr, \"arr length\", arr2)\n",
        "    inp = Variable(torch.LongTensor([arr])) # learning using [] to add one dimention from 6 to 1*6\n",
        "    out = classifier(inp)\n",
        "    print(\"in\", inp.size(), \"out\", out.size())\n",
        "\n",
        "    inputs = make_variables(names)\n",
        "    out = classifier(inputs)\n",
        "    print(\"batch in\", inputs.size(), \"batch out\", out.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EWBt30rNtnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sequences(vectorized_seqs, seq_lengths):\n",
        "  seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
        "  for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "    seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
        "  return seq_tensor\n",
        "\n",
        "def make_variables(names):\n",
        "  sequence_and_length = [str2ascii(name) for name in names] \n",
        "  print(\"sequence and length\", sequence_and_length)\n",
        "  vectorized_seqs = [s1[0] for s1 in sequence_and_length]\n",
        "  seq_lengths = torch.LongTensor([s1[1] for s1 in sequence_and_length])\n",
        "  return pad_sequences(vectorized_seqs, seq_lengths)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcqcHrPcX_OS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001) # model here is classsifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAFiBmpmcuhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(5):\n",
        "  for name in range(names)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}