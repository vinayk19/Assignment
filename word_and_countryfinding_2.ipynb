{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word and countryfinding_2.ipynb",
      "provenance": [],
      "mount_file_id": "1WLaQFEWZEn5iCXJZq8y6Le2f8LNrRBT4",
      "authorship_tag": "ABX9TyM7nHqSpMkYhhjadJZZfNDa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayk19/Assignment/blob/master/word_and_countryfinding_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WigN-7SHh93O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e2e3520-b151-4cf5-df5c-f45a07c0b74d"
      },
      "source": [
        "cd /content/drive/My Drive/AI/basic/data/names_train"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/AI/basic/data/names_train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4jmwmZDk8U6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# from name_dataset import NameDataset\n",
        "# from torch.nn.utils.rnn import pack_padded_sequence, pad_paked_sequence"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP7rj3DGBMSC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d5579138-92ac-48cf-d886-f0b30fa83d01"
      },
      "source": [
        "xy1 = pd.read_csv(\"names_train.csv\") #q is it require to convert it(text) into torch\n",
        "# xy1.shape\n",
        "# name = (xy1.iloc[0,:][0])\n",
        "# name\n",
        "xy1.describe\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of               Adsit   Czech\n",
              "0            Ajdrna   Czech\n",
              "1      Antonowitsch   Czech\n",
              "2        Antonowitz   Czech\n",
              "3        Ballalatak   Czech\n",
              "4        Ballaltick   Czech\n",
              "...             ...     ...\n",
              "13368         Zabek  Polish\n",
              "13369     Zdunowski  Polish\n",
              "13370     Zdunowski  Polish\n",
              "13371      Ziemniak  Polish\n",
              "13372       Zientek  Polish\n",
              "\n",
              "[13373 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZQHLEWca7i5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cd07e287-0891-4c5e-c7be-2a51d506f4a1"
      },
      "source": [
        "word = xy1['Adsit'].tolist()\n",
        "country = xy1['Czech'].tolist()\n",
        "print(len(word))\n",
        "print(len(country))\n",
        "print(list(dict.fromkeys(country))) # it will provide only unique country name only"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13373\n",
            "13373\n",
            "['Czech', 'German', 'Arabic', 'Japanese', 'Chinese', 'Vietnamese', 'Russian', 'French', 'Irish', 'English', 'Spanish', 'Greek', 'Italian', 'Portuguese', 'Scottish', 'Dutch', 'Korean', 'Polish']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPr7qIASeOAW",
        "colab_type": "text"
      },
      "source": [
        "Tast1 : create database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI8_0khQiL9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Name(Dataset):\n",
        "  def __init__(self, csv_file):\n",
        "  # def __init__(self, csv_file):\n",
        "    self.xy = pd.read_csv(csv_file)\n",
        "    self.word = self.xy['Adsit'].tolist()\n",
        "    self.country = self.xy['Czech'].tolist()\n",
        "    self.country_list = list(dict.fromkeys(self.country))\n",
        "    \n",
        "  def __getitem__(self, id):\n",
        "    return self.word[id], self.country[id]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.word)\n",
        "\n",
        "  def get_country_list(self):\n",
        "    return self.country_list\n",
        "  #get the unique country name by its id or lebel\n",
        "  def get_country_by_id(self, id):\n",
        "    return self.country_list[id] #@learning callable meance function() but in list it is [] (as list is not callable)\n",
        "  \n",
        "  def get_id_by_country(self, country):\n",
        "    return self.country_list.index(country)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE5dHVRWgOye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d41a37cc-33b1-484d-c76f-eebafd6f4261"
      },
      "source": [
        "#test database\n",
        "if __name__ == '__main__':\n",
        "  database = Name('/content/drive/My Drive/AI/basic/data/names_train/names_train.csv')\n",
        "  print(database.get_country_list())\n",
        "  print(database.get_id_by_country('French'))\n",
        "  print(database.get_country_by_id(7))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Czech', 'German', 'Arabic', 'Japanese', 'Chinese', 'Vietnamese', 'Russian', 'French', 'Irish', 'English', 'Spanish', 'Greek', 'Italian', 'Portuguese', 'Scottish', 'Dutch', 'Korean', 'Polish']\n",
            "7\n",
            "French\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_3SaEO7eTZB",
        "colab_type": "text"
      },
      "source": [
        "tast2: Create dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln00GiUsq3Mg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7580b182-2d43-4c1b-cb8f-4684ea7d17e7"
      },
      "source": [
        "# train loader\n",
        "BS = 10\n",
        "# database = \n",
        "database = Name('/content/drive/My Drive/AI/basic/data/names_train/names_train.csv')\n",
        "train_loader = DataLoader(dataset=database, batch_size= BS, shuffle=True)\n",
        "print(len(database))\n",
        "print(database[2000])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13373\n",
            "('Samaha', 'Arabic')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6Zsgm90nzpF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4569ea38-dd37-4002-bdc3-6c55057f47d4"
      },
      "source": [
        "#train loader\n",
        "if __name__=='__main__':\n",
        "  database = Name('/content/drive/My Drive/AI/basic/data/names_train/names_train.csv')\n",
        "  # database = Name('names_test.csv')\n",
        "  print(len(database))\n",
        "  print(len(database[5]))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13373\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veyLI6eEu3sd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test loader\n",
        "database_test = Name('/content/drive/My Drive/AI/basic/data/names_train/names_test.csv')\n",
        "test_loader = DataLoader(dataset=database_test, batch_size= BS, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iowTb3mpu4Cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB7yA-tEOwqL",
        "colab_type": "text"
      },
      "source": [
        "working on asc and tranposzing .t() is it tup[le , tensor or what then next action\n",
        "1. bs is 10\n",
        "2. test funciton\n",
        "validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VwFV7Z_eWCF",
        "colab_type": "text"
      },
      "source": [
        "task3: model: embedding, rnn, linear layer \n",
        "task4: optimizer\n",
        "task5: training\n",
        "taask6: test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqEmGV5GkBD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class net(torch.nn.Module): #learning we can get fuction created inside database (it can be train or test)\n",
        "  def __init__(self):\n",
        "    super(net,self).__init__()\n",
        "    self.word_len = 18 # seq chenged from 15 to 18 for test\n",
        "    self.vocab_size = self.word_len # no of character per word\n",
        "    self.embedding_dim = 6 # how many diamention it wants\n",
        "    self.input_size = self.embedding_dim #6\n",
        "    self.hiddenO = self.input_size #6 dim\n",
        "    self.hidden_size = self.embedding_dim #6\n",
        "    self.batch = 1\n",
        "    self.num_layer =1\n",
        "    self.num_class = 18 #num of contries\n",
        "\n",
        "    #embedding  @learning use tensor conversion al last meand at GRU ot RNN time not beofore that as arugment of other function (like embed) are not tensors\n",
        "    self.embed = nn.Embedding(num_embeddings = self.vocab_size, embedding_dim = torch.LongTensor([self.embedding_dim])) # num_enn = input size word lenth and embedd is output size ie hidden size\n",
        "    #gru    \n",
        "    self.grucell = nn.GRU(input_size = self.input_size, hidden_size = self.hidden_size, num_layers =self.num_layer, batch_first = True)\n",
        "    #fc\n",
        "    self.fc = nn.Linear(in_features= self.embedding_dim, out_features= self.num_class)\n",
        "  \n",
        "  def forward(self, word, country): # 10 word and 10 country\n",
        "    # word, contry = data\n",
        "    print(\"word\", word) # word type is tuple of B*S\n",
        "    #converting work in to integer ASCII for input and outpuut ord # learning asc is not required as embedding can take words directly\n",
        "     \n",
        "    word_ascii, w_len = self.asc(word, self.word_len) #in: BS * word and out: BS * ASCII with padding\n",
        "    print(\"\\n word_ascii\", word_ascii,  \"\\n w_len\", w_len)\n",
        "    # challenge we need some padding. keep every word as 18 character long.\n",
        "    #learning embedding needs word(s) * BS  ir S*B not B*S\n",
        "    word_ascii = word_ascii.t() # in B*S ot S*B\n",
        "    word_embed = self.embed(word_ascii) #embedding e extra in matrix out : BS* word_len*E\n",
        "    #rnn in and out rnn_in BS*Seq*inu_D(e) and rnn_out BS*Seq*hiddenO\n",
        "    # input = B*Seq_len*input, hidden = num_lauyer* batch,hidden_size\n",
        "    input = word_embed # B*S*I\n",
        "    hidden = self.hidden_init()\n",
        "    out, h_n = self.grucell(input, hidden) #out B*S*hidden [1 10 6] , h_n = num_layerxBxHidden [1 1 6]\n",
        "\n",
        "    #FC linear layer in Seq to out no of classor contry(-1, nod of class or country)\n",
        "    out = self.fc(h_n) #in [1 1 6] out [1 1 18]\n",
        "    out = out.view(-1, self.num_class)\n",
        "    # out - out.t()\n",
        "    country_ids = self.country2tensor(country) # 18 contoury with each 15 word\n",
        "    # country = country.t()#view(-1)\n",
        "    # print(\"country ids\", country_ids.shape)\n",
        "    # country = country.squeeze_()\n",
        "    # out = out.squeeze_()\n",
        "    return out, country_ids\n",
        "\n",
        "    #output output of DC \n",
        "  def hidden_init(self):\n",
        "    hid = torch.zeros(self.num_layer, self.batch, self.hiddenO)\n",
        "    return hid\n",
        "  def country2tensor(self, countries):\n",
        "    # print(\"contries size\", len(countries))\n",
        "    country_ids = [database.get_id_by_country(each_country) for each_country in countries]\n",
        "    # print(\"contries ids\", len(country_ids))\n",
        "    return torch.LongTensor(country_ids)\n",
        "\n",
        "  def asc(self, words, word_len):\n",
        "    word_len = word_len\n",
        "    sorks3 =  []\n",
        "    # sorks3 = torch.zeros[len(words)]\n",
        "    for word in words:\n",
        "      # # for cha in word:\n",
        "      # sorks = [ord(cha) for cha in word]\n",
        "      # sorks2 = [sorks + [0]*(word_len-len(sorks))] \n",
        "      # sorks3 = Variable(torch.LongTensor([sorks2]))\n",
        "      # sorks4 = sorks4.append(sorks3)\n",
        "      \n",
        "      # sorks3.append(Variable(torch.LongTensor([[ord(cha) for cha in word] + [0]*(word_len-len([ord(cha) for cha in word]))])))\n",
        "      sorks3.append([[ord(cha) for cha in word] + [0]*(word_len-len([ord(cha) for cha in word]))])\n",
        "      # i = i+1\n",
        "    return sorks3, len(sorks3) \n",
        "\n",
        "\n",
        "model = net()"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlsCGYjf5XtB",
        "colab_type": "text"
      },
      "source": [
        "Process contry differently as each contry will have unique no (one number not as matrix as word (N * no of class) will be comparing or predicting the perticular class. so its a one idea to keep perticular class as one number rather than keeping it as one class as array.\n",
        "\n",
        "TODO each contry as one unique no.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bkGf6xpwy60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = .0001)\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdzgP6gTxXdp",
        "colab_type": "text"
      },
      "source": [
        "Trainig\n",
        "\n",
        "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8YXuz0AwyyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "03c33bf3-d032-40b2-a091-3285644612a9"
      },
      "source": [
        "for epoch in range(1):\n",
        "  total_loss = 0 \n",
        "  # batch_id = 0 \n",
        "  for batch_idx, (word, country) in enumerate(train_loader):\n",
        "    print(\"word database\", len(word), \"countey database\", len(country))\n",
        "    out, country = model(word, contry)\n",
        "    print(\"out shape\", out.shape, \"countey shape\", country.shape)\n",
        "    optimizer.zero_grad()\n",
        "    loss = Criterion(out, country)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 500  == 0:\n",
        "        print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(word), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    "
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word database 10 countey database 10\n",
            "word ('Kubo', 'Andryuhin', 'Teoh', 'Ville', 'Bishara', 'Baiguzov', 'Rennalls', 'Bairamkuloff', 'Neary', 'Turmanov')\n",
            "\n",
            " word_ascii [[[75, 117, 98, 111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[65, 110, 100, 114, 121, 117, 104, 105, 110, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[84, 101, 111, 104, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[86, 105, 108, 108, 101, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[66, 105, 115, 104, 97, 114, 97, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[66, 97, 105, 103, 117, 122, 111, 118, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[82, 101, 110, 110, 97, 108, 108, 115, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[66, 97, 105, 114, 97, 109, 107, 117, 108, 111, 102, 102, 0, 0, 0, 0, 0, 0]], [[78, 101, 97, 114, 121, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[84, 117, 114, 109, 97, 110, 111, 118, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]] \n",
            " w_len 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-7a38288ac487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcountry\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word database\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"countey database\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcountry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"out shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"countey shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcountry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-107-96c63e790b57>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word, country)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# challenge we need some padding. keep every word as 18 character long.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#learning embedding needs word(s) * BS  ir S*B not B*S\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mword_ascii\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_ascii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# in B*S ot S*B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mword_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_ascii\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#embedding e extra in matrix out : BS* word_len*E\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#rnn in and out rnn_in BS*Seq*inu_D(e) and rnn_out BS*Seq*hiddenO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 't'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhLo9HN7AiZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n= [\"name\"]\n",
        "# n[1]\n",
        "# ord(n[1])\n",
        "print(asc(n,10)[0].size())\n",
        "a, b = asc(n,10)\n",
        "embed=nn.Embedding(200,3)\n",
        "emb2 = embed(a)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOS2GPINm6EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(emb2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoqHLwaEAqmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  sorks= []\n",
        "  i=0\n",
        "  def asc(words, word_len):\n",
        "    word_len = word_len\n",
        "\n",
        "    for word in words:\n",
        "      # for cha in word:\n",
        "      sorks = [ord(cha) for cha in word]\n",
        "      sorks2 = sorks + [0]*(word_len-len(sorks)) \n",
        "      sorks3 = Variable(torch.LongTensor([sorks2]))\n",
        "    return sorks3, len(sorks2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e02g7x7BWQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIyELK02B22g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n1 =asc(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzTcqLLZTcoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n2 = concat(n1, zeros[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqvdvxq4Th8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "like_zero[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydUWB-moTo_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}