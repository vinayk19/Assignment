{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word and countryfinding_3.ipynb",
      "provenance": [],
      "mount_file_id": "1WLaQFEWZEn5iCXJZq8y6Le2f8LNrRBT4",
      "authorship_tag": "ABX9TyNX98HnB0e2u9C+Rzv/t04t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayk19/Assignment/blob/master/word_and_countryfinding_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WigN-7SHh93O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f45596e-69c4-409b-94bf-dfe7ff01d67f"
      },
      "source": [
        "cd /content/drive/My Drive/AI/basic/data/names_train"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/AI/basic/data/names_train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4jmwmZDk8U6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import csv\n",
        "import gzip\n",
        "\n",
        "# from name_dataset import NameDataset\n",
        "# from torch.nn.utils.rnn import pack_padded_sequence, pad_paked_sequence"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP7rj3DGBMSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# xy1 = pd.read_csv(\"names_train.csv\") #q is it require to convert it(text) into torch\n",
        "# # xy1.shape\n",
        "# # name = (xy1.iloc[0,:][0])\n",
        "# # name\n",
        "# xy1.describe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZQHLEWca7i5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word = xy1['Adsit'].tolist()\n",
        "# country = xy1['Czech'].tolist()\n",
        "# print(len(word))\n",
        "# print(len(country))\n",
        "# print(list(dict.fromkeys(country))) # it will provide only unique country name only"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8UihLr-vsrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # filename = '/content/drive/My Drive/AI/basic/data/names_train/names_train.csv.gz' if is_train_set else '/content/drive/My Drive/AI/basic/data/names_train/names_test.csv.gz'\n",
        "# if is_train_set:\n",
        "#   filename = '/content/drive/My Drive/AI/basic/data/names_train/names_train.csv.gz'\n",
        "# else :\n",
        "#   filename = '/content/drive/My Drive/AI/basic/data/names_train/names_test.csv.gz'\n",
        "\n",
        "# with open(filename, \"rt\") as f: # learning if file is zip with file the gzip.open. if only csv file then only open(filename, \"rt\")\n",
        "#   reader = csv.reader(f)\n",
        "#   rows = list(reader)\n",
        "# print(rows)\n",
        "# naam = [row[0] for row in rows]\n",
        "# country = [row[1] for row in rows]\n",
        "\n",
        "# print(len(naam), naam)\n",
        "# print(len(country), country)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPr7qIASeOAW",
        "colab_type": "text"
      },
      "source": [
        "Tast1 : create database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI8_0khQiL9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Name(Dataset):\n",
        "  def __init__(self, is_train_set = False):\n",
        "  # def __init__(self, csv_file):\n",
        "\n",
        "    if is_train_set:\n",
        "      filename = '/content/drive/My Drive/AI/basic/data/names_train/names_train.csv'\n",
        "    else :\n",
        "      filename = '/content/drive/My Drive/AI/basic/data/names_train/names_test.csv'\n",
        "\n",
        "    with open(filename, \"rt\") as f: # learning if file is zip with file the gzip.open. if only csv file then only open(filename, \"rt\")\n",
        "      reader = csv.reader(f)\n",
        "      rows = list(reader)\n",
        "    # print(rows)\n",
        "    self.word = [row[0] for row in rows]\n",
        "    self.country = [row[1] for row in rows]\n",
        "    # self.xy = pd.read_csv(csv_file)\n",
        "    # self.word = self.xy['Adsit'].tolist()\n",
        "    # self.country = self.xy['Czech'].tolist()\n",
        "    self.country_list = list(dict.fromkeys(self.country))\n",
        "    \n",
        "  def __getitem__(self, id):\n",
        "    return self.word[id], self.country[id]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.word)\n",
        "\n",
        "  def get_country_list(self):\n",
        "    return self.country_list\n",
        "  #get the unique country name by its id or lebel\n",
        "  def get_country_by_id(self, id):\n",
        "    return self.country_list[id] #@learning callable meance function() but in list it is [] (as list is not callable)\n",
        "  \n",
        "  def get_id_by_country(self, country):\n",
        "    return self.country_list.index(country)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE5dHVRWgOye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b95daa3b-1ed4-4763-c73b-8bc8c099ad41"
      },
      "source": [
        "#test database\n",
        "if __name__ == '__main__':\n",
        "  database = Name(True) # fasle for train set and true for test set\n",
        "  print(database.get_country_list())\n",
        "  print(database.get_id_by_country('French'))\n",
        "  print(database.get_country_by_id(7))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Czech', 'German', 'Arabic', 'Japanese', 'Chinese', 'Vietnamese', 'Russian', 'French', 'Irish', 'English', 'Spanish', 'Greek', 'Italian', 'Portuguese', 'Scottish', 'Dutch', 'Korean', 'Polish']\n",
            "7\n",
            "French\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_3SaEO7eTZB",
        "colab_type": "text"
      },
      "source": [
        "tast2: Create dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln00GiUsq3Mg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6c2a817a-2321-46fd-c7e6-35fd5900dbdd"
      },
      "source": [
        "# parameter s and loaderts train loader\n",
        "# net(N_chars, Hidden_size, N_countries, N_layers)\n",
        "\n",
        "N_layers = 1\n",
        "N_countries = len(Name(True).get_country_list())\n",
        "hidden_size = 20\n",
        "N_epoch =1\n",
        "N_chars = 128\n",
        "batch_size = 256\n",
        "\n",
        "database_train = Name(True)\n",
        "train_loader = DataLoader(dataset=database_train, batch_size= batch_size, shuffle=True)\n",
        "\n",
        "database_test = Name(False)\n",
        "train_loader = DataLoader(dataset=database_test, batch_size= batch_size, shuffle=True)\n",
        "print(len(database_train), len(database_test))\n",
        "print(database_train[2000],database_test[2000])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13374 6700\n",
            "('Salib', 'Arabic') ('Bakrymov', 'Russian')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6Zsgm90nzpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for val in enumerate(train_loader):\n",
        "#   print(val[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNNbrDogzyE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#common funtions \n",
        "def make_variable(words, countries):#word to asci charected then padding then pack padd\n",
        "  words_asc = []\n",
        "  for word in words:\n",
        "    words_asc = words_asc + [ascii(word)[0]] #1done now to padding (max len of batch) \n",
        "\n",
        "  print(\"ascii word type\", type(words_asc), len(words_asc))\n",
        "  print(\"ascii word value\",words_asc)\n",
        "  # learning torch.max works only on tensors not list/tuple no now onwards convert everythings in tensor only\n",
        "  # ite menans pass the tensors here which are already padded one\n",
        "\n",
        "  max_len_word = max(len(each) for each in words_asc) # torch.max works in pytorch to change words_asc to tensor\n",
        "  print(\"max_len_word value\",max_len_word) \n",
        "  # padding\n",
        "\n",
        "  words_padd = [each + [0]*(max_len_word-len(each)) for each in words_asc]\n",
        "  \n",
        "  print(\"words_padd type\", type(words_padd), len(words_padd))\n",
        "  print(\"words_padd value\",words_padd)# till now no tensor \n",
        "  #2 padding done 3 pack paadding\n",
        "  #working # input and lenght should be tensor\n",
        "  word_packed = torch.nn.utils.rnn.pack_padded_sequence(words_padd, max_len_word)\n",
        "\n",
        "  print(\"words_packed type\", type(words_packed), len(words_packed))\n",
        "  print(\"words_packed value\",words_packed)# till now no tensor #passed\n",
        "  \n",
        "  Country_ids = [database_train.get_id_by_country(country) for country in countries]\n",
        "  target = torch.LongTensor(Country_ids)\n",
        "  print(\"\\ncountry_ids\", Country_ids.shape, type(Country_ids), Country_ids)\n",
        "  print(\"\\nTarget\", target.shape, type(target), target)\n",
        "\n",
        "  return word_packed, target\n",
        "\n",
        "def ascii(word):\n",
        "  word_asc = [ord(cha) for cha in word]\n",
        "  # word_asc = [torch.LongTensor(ord(cha)) for cha in word]\n",
        "\n",
        "  # return torch.LongTensor(word_asc), len(word_asc)\n",
        "  return word_asc, len(word_asc)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siZpLl7GR1-Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb2e5d0b-af16-40ee-8f6d-40470c197c03"
      },
      "source": [
        "a = [80, 97, 118, 108, 105, 110, 115, 107, 121], [68, 97, 110, 105, 110], [70, 97, 114, 114, 105, 101, 114], [77, 97, 97, 108, 111, 117, 102]\n",
        "print(type(a[0]))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1hdGa8jSlMN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2fda1164-4c15-4c02-b777-329cb3ddff26"
      },
      "source": [
        "b = max([len(each) for each in a]) #[torch.FloatTensor(a))\n",
        "print(b)\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlrzQPwesMZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad4c22fe-c42f-4a05-e71d-9a9fa9b93d55"
      },
      "source": [
        "max(b)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnalj8X9SRhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.max(b,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LSvt7qatduk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "5d29a043-0698-4c1a-af6a-469b36825cf4"
      },
      "source": [
        "#final execution funciton\n",
        "if __name__ == '__main__':\n",
        "  model = net(N_chars, Hidden_size, N_countries, N_layers)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
        "  criterian = nn.CrossEntropyLoss()\n",
        "\n",
        "  start = time.time()\n",
        "  print(\"Traing for %d epochs ...:\" % N_epoch)\n",
        "  for epoch in range(1, N_epoch+1):\n",
        "    # Train cycle\n",
        "    train()\n",
        "    # Testing cycle\n",
        "    test()\n",
        "\n",
        "    #testing several samples\n",
        "    test(\"Sung\")"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traing for 1 epochs ...:\n",
            "type make_variable <class 'tuple'> <class 'tuple'>\n",
            "len  256 20 18 1\n",
            "ascii word type <class 'list'> 256\n",
            "ascii word value [[65, 110, 100, 114, 117, 122, 115, 107, 121], [77, 97, 116, 122, 105, 103, 117, 114, 97], [74, 101, 108, 101, 104, 111, 118, 115, 107, 121], [80, 104, 111], [80, 108, 111, 117, 118, 105, 110], [65, 112, 112, 108, 101, 98, 121], [83, 99, 104, 101, 112, 97, 116, 111, 118], [84, 105, 107, 104, 111, 110, 99, 104, 117, 107], [67, 104, 101, 114, 116, 111, 114, 105, 116, 115, 107, 121], [68, 101, 110, 98, 121], [84, 111, 110, 103, 101], [86, 97, 105, 110, 115, 104, 116, 101, 105, 110], [77, 117, 115, 97, 104, 97, 110, 111, 118], [77, 117, 122, 97, 108, 101, 118, 115, 107, 105, 104], [90, 104, 105, 108, 105, 110, 115, 107, 121], [66, 97, 99, 111, 110], [87, 101, 101, 107, 115], [65, 98, 98, 111, 117, 100], [90, 104, 105, 122, 104, 110, 111, 118], [67, 97, 116, 111, 110], [78, 97, 115, 101, 114], [65, 115, 116, 115, 97, 116, 117, 114, 111, 118], [71, 108, 117, 104, 97, 114, 101, 118], [65, 114, 105, 97, 110], [84, 114, 97, 105, 110], [71, 114, 97, 109, 101, 116, 115, 107, 121], [80, 111, 114, 111, 121, 107, 111, 118], [77, 97, 114, 107, 104, 97, 109], [74, 105, 109, 101, 110, 101, 122], [82, 101, 107, 115, 104, 105, 110, 115, 107, 121], [77, 117, 107, 97, 110, 111, 118], [71, 114, 111], [74, 97, 103, 101, 114], [80, 114, 105, 118, 101, 116, 116], [71, 111, 111, 100, 97, 99, 114, 101], [84, 104, 101, 108, 119, 101, 108, 108], [79, 108, 100, 102, 105, 101, 108, 100], [86, 97, 115, 121, 117, 114, 105, 110], [71, 101, 116, 116, 97], [86, 39, 85, 110, 110, 105, 107, 111, 118], [75, 111, 117, 114, 121], [66, 117, 116, 116], [87, 111, 111, 108, 100, 114, 105, 100, 103, 101], [83, 111, 117, 116, 104], [86, 105, 110, 99, 105], [76, 101, 118, 105, 116, 97, 110], [71, 101, 116, 109, 97, 110, 101, 110, 107, 111], [84, 104, 111, 109, 115], [70, 117, 106, 105, 109, 111, 116, 111], [70, 117, 108, 108, 101, 114, 111, 110], [76, 97, 100, 121, 106, 101, 110, 115, 107, 121], [70, 117, 114, 110, 105, 115, 115], [66, 97, 122, 97, 110, 111, 118], [72, 105, 110, 116, 122, 101, 110], [84, 111, 105, 99, 104, 107, 105, 110], [85, 103, 104, 105], [72, 111, 110, 111, 118], [80, 101, 105], [87, 97, 105, 116, 101], [77, 105, 107, 104, 110, 101, 118, 105, 99, 104], [67, 104, 97, 112, 109, 97, 110], [89, 97, 110, 111, 118, 115, 107, 121], [66, 111, 117, 116, 114, 111, 115], [73, 108, 121, 117, 115, 104, 105, 110], [65, 116, 116, 105, 97], [78, 111, 111, 100], [90, 104, 105, 100, 101, 110, 107, 111], [72, 97, 110, 107, 101, 101, 118], [81, 117, 105, 110, 110], [67, 108, 111, 117, 103, 104], [75, 97, 108, 105, 110, 107, 97], [71, 97, 110, 105, 109], [75, 105, 106, 101, 107], [83, 101, 101, 108, 101, 110], [74, 111, 104, 110, 115, 116, 111, 110], [89, 117, 107, 104, 105, 109, 117, 107], [72, 97, 115, 115], [72, 97, 105, 107], [83, 112, 101, 108, 108, 109, 101, 121, 101, 114], [74, 97, 98, 114, 101, 118], [70, 111, 114, 98, 101, 115], [84, 97, 110], [83, 104, 97, 109, 111, 111, 110], [70, 111, 115, 115], [83, 117, 97, 114, 101, 122], [75, 97, 108, 105, 116, 107, 105, 110], [77, 97, 114, 99, 104, 97, 110, 117, 107, 111, 118], [73, 110, 103, 108, 101, 100, 101, 119], [65, 110, 100, 114, 121, 117, 115, 104, 105, 110], [73, 122, 122, 97, 114, 100], [77, 111, 111, 110, 101, 121], [68, 111, 110, 107, 105, 110], [65, 98, 97, 107, 117, 109, 116, 115, 101, 118], [68, 117, 107, 101], [89, 111, 115, 104, 105, 122, 97, 119, 97], [69, 108, 108, 101, 114, 121], [77, 97, 107, 104, 97, 108, 105, 110], [68, 101, 118, 101, 114, 101, 108, 108], [84, 119, 105, 103, 103], [77, 105, 121, 97, 104, 97, 114, 97], [68, 111, 98, 114, 97, 106, 97, 110, 115, 107, 121], [89, 117, 122, 105, 110], [83, 104, 97, 105, 110], [72, 117, 100, 111, 105, 110, 97, 116, 111, 118], [66, 101, 99, 107], [83, 99, 104, 119, 97, 114, 122, 101, 110, 98, 101, 114, 103], [78, 117, 114, 105, 101, 118], [84, 114, 117, 107, 104, 97, 110, 111, 118], [71, 111, 108, 111, 115, 101, 110, 107, 111], [82, 97, 105, 107, 104, 101, 108, 103, 97, 117, 122], [78, 121, 117, 104, 116, 105, 108, 105, 110], [72, 97, 114, 98], [83, 97, 109], [66, 117, 108, 103, 97, 114, 101, 108, 108, 105], [87, 111, 111], [68, 111, 99, 104, 101, 114, 116, 121], [77, 97, 115, 116, 101, 114, 115], [74, 109, 117, 100, 115, 107, 121], [82, 111, 97, 99, 104], [83, 104, 97, 109, 114, 117, 110], [84, 115, 118, 101, 116, 110, 111, 118], [67, 104, 97, 117, 104, 97, 110], [71, 117, 108, 101, 118, 115, 107, 121], [90, 97, 115, 108, 97, 118, 101, 116, 115], [86, 105, 99, 107, 97, 114, 115], [90, 101, 108, 101, 110, 111, 105], [75, 97, 108, 111, 115, 104, 105, 110], [76, 111, 121, 111, 108, 97], [66, 101, 107, 109, 117, 114, 122, 111, 118], [81, 117, 114, 101, 115, 104, 105], [66, 105, 110, 103], [83, 101, 105, 102], [75, 104, 111, 117, 114, 121], [83, 97, 108, 105, 98, 97], [77, 105, 107, 104, 97, 105, 108, 106, 117, 107], [80, 117, 114, 100, 101, 115], [70, 101, 108, 108, 111, 119, 115], [68, 97, 104, 110, 111], [84, 101, 108, 108, 105, 115], [86, 101, 108, 105, 103, 117, 114, 97], [72, 111, 108, 115, 104, 101, 118, 110, 105, 107, 111, 118], [65, 98, 97, 110, 100, 111, 110, 97, 116, 111], [74, 117, 104, 105, 109, 101, 110, 107, 111], [66, 114, 97, 100, 101, 110], [75, 114, 101, 115, 107, 97, 115], [83, 105, 111, 100, 97], [72, 97, 121, 117, 97, 116, 97], [82, 105], [65, 108, 109, 121, 97, 115, 104, 107, 105, 110], [83, 117, 98, 101, 114, 116, 111, 118, 97], [82, 97, 108, 108, 105, 115], [84, 115, 118, 101, 105, 98, 97], [77, 111, 106, 97, 114, 111, 118, 115, 107, 121], [77, 105, 116, 115, 117, 104, 97, 114, 117], [87, 97, 115, 101, 109], [83, 104, 105, 114, 111, 121, 97, 109, 97], [80, 114, 105, 100, 118, 111, 114, 111, 118], [82, 97, 111], [65, 110, 100, 114, 117, 115, 101, 110, 107, 111], [77, 105, 99, 104, 97, 108, 111, 118, 105, 99], [66, 97, 115, 97, 114, 97], [80, 97, 115, 99, 97, 108], [83, 108, 101, 105, 109, 97, 110], [83, 105, 101, 114, 122, 97, 110, 116], [68, 97, 109, 105, 97, 110, 105], [67, 108, 97, 121, 116, 111, 110], [66, 97, 105, 99, 104, 111, 114, 111, 102, 102], [90, 104, 105, 116, 105, 110, 101, 118], [84, 111, 109, 97, 110], [69, 105, 110, 100, 111, 114, 102], [84, 97, 104, 97, 110], [68, 111, 108, 116, 111, 110], [67, 97, 109, 112, 98, 101, 108, 108], [83, 97, 105, 100, 117, 108, 97, 101, 118], [65, 98, 105, 115, 97, 108, 111, 102, 102], [66, 114, 105, 103, 103, 115], [76, 121, 111, 110], [66, 101, 108, 108], [71, 117, 100, 111, 118, 105, 99, 104], [66, 97, 103, 114, 97, 116, 105, 111, 110], [75, 104, 111, 117, 114, 121], [67, 114, 111, 119, 101], [71, 101, 114, 103, 101, 115], [86, 105, 104, 114, 101, 118], [78, 111, 109, 117, 114, 97], [83, 104, 97, 109, 98, 117, 114, 107, 105, 110], [77, 101, 115, 115, 110, 101, 114], [76, 121, 106, 101, 110, 107, 111, 118], [84, 115, 117, 99, 104, 105, 101], [68, 117, 112, 111, 110, 116], [76, 101, 98, 101, 100, 101, 118, 105, 99, 104], [82, 122, 104, 101, 115, 104, 101, 118, 115, 107, 121], [77, 105, 107, 104, 97, 105, 108, 105, 110], [67, 108, 111, 115, 101], [65, 119, 97, 108, 105, 115, 104, 119, 105, 108, 105], [75, 111, 108, 105, 104, 97], [76, 111, 107, 104, 97, 110, 105, 110], [72, 97, 108, 107, 105, 110], [74, 97, 110, 107, 111, 118, 115, 107, 121], [75, 97, 115, 115, 105, 115], [78, 105, 99, 111, 108, 108], [84, 122, 97, 107, 117, 110, 111, 118], [80, 101, 101, 114, 115], [83, 116, 97, 99, 107], [78, 97, 110, 115, 111, 110], [86, 105, 115, 107, 111, 118], [74, 97, 110, 103, 97, 114, 98, 101, 114], [68, 117, 114, 97, 115, 111, 118], [65, 108, 116, 115, 104, 117, 108, 101, 114], [69, 108, 99, 111, 99, 107], [66, 97, 107, 105, 101, 118], [84, 105, 108, 108, 101, 110, 115], [74, 100, 97, 110, 107, 105, 110], [80, 97, 118, 108, 117, 104, 105, 110], [80, 105, 115, 116, 111, 108, 107, 111, 114, 115], [78, 97, 106, 106, 97, 114], [77, 97, 107, 117, 100, 97], [79, 39, 68, 111, 104, 101, 114, 116, 121], [68, 109, 111, 104, 111, 118, 115, 107, 121], [72, 117, 97], [75, 111, 103, 97, 114, 97], [76, 101, 97, 100, 108, 101, 121], [89, 111, 111], [69, 115, 115, 97, 110], [65, 109, 97, 114, 105], [89, 97, 107, 117, 115, 104, 107, 105, 110], [74, 111, 110, 103, 111, 108, 111, 118, 105, 99, 104], [83, 99, 104, 109, 105, 100, 116], [77, 105, 104, 97, 105, 108, 111, 118, 115, 107, 121], [65, 119, 115, 101, 101, 110, 107, 111], [78, 101, 109, 101, 99], [75, 105, 108, 115, 104, 97, 119], [68, 111, 98, 114, 105, 107], [66, 114, 105, 110, 107, 101, 114, 104, 111, 102, 102], [65, 103, 97, 109, 111, 102, 102], [65, 115, 116, 114, 97, 116, 111, 118], [76, 97, 116, 104, 101, 121], [66, 97, 98, 97], [66, 97, 110], [84, 115, 105, 103, 101, 108, 110, 105, 107], [71, 114, 105, 103, 111, 114, 105, 101, 118], [86, 97, 100, 111, 118, 115, 107, 105], [84, 105, 102, 102, 116], [78, 97, 107, 97, 110, 111], [71, 97, 103, 101, 110], [86, 97, 110, 100, 97, 108, 101], [80, 101, 108, 108, 101, 110, 101, 110], [80, 97, 122, 121], [72, 117, 122, 105, 121, 97, 116, 111, 118], [65, 115, 107, 101, 114], [78, 97, 115, 115, 97, 114], [75, 111, 109, 97, 116, 115, 117, 122, 97, 107, 105], [79, 114, 108, 97, 110, 100, 111], [65, 98, 100, 117, 108, 98, 101, 107, 111, 102, 102], [82, 97, 118, 101, 110, 115, 99, 114, 111, 102, 116], [68, 97, 110, 105, 110]]\n",
            "max_len_word value 13\n",
            "words_padd type <class 'list'> 256\n",
            "words_padd value [[65, 110, 100, 114, 117, 122, 115, 107, 121, 0, 0, 0, 0], [77, 97, 116, 122, 105, 103, 117, 114, 97, 0, 0, 0, 0], [74, 101, 108, 101, 104, 111, 118, 115, 107, 121, 0, 0, 0], [80, 104, 111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [80, 108, 111, 117, 118, 105, 110, 0, 0, 0, 0, 0, 0], [65, 112, 112, 108, 101, 98, 121, 0, 0, 0, 0, 0, 0], [83, 99, 104, 101, 112, 97, 116, 111, 118, 0, 0, 0, 0], [84, 105, 107, 104, 111, 110, 99, 104, 117, 107, 0, 0, 0], [67, 104, 101, 114, 116, 111, 114, 105, 116, 115, 107, 121, 0], [68, 101, 110, 98, 121, 0, 0, 0, 0, 0, 0, 0, 0], [84, 111, 110, 103, 101, 0, 0, 0, 0, 0, 0, 0, 0], [86, 97, 105, 110, 115, 104, 116, 101, 105, 110, 0, 0, 0], [77, 117, 115, 97, 104, 97, 110, 111, 118, 0, 0, 0, 0], [77, 117, 122, 97, 108, 101, 118, 115, 107, 105, 104, 0, 0], [90, 104, 105, 108, 105, 110, 115, 107, 121, 0, 0, 0, 0], [66, 97, 99, 111, 110, 0, 0, 0, 0, 0, 0, 0, 0], [87, 101, 101, 107, 115, 0, 0, 0, 0, 0, 0, 0, 0], [65, 98, 98, 111, 117, 100, 0, 0, 0, 0, 0, 0, 0], [90, 104, 105, 122, 104, 110, 111, 118, 0, 0, 0, 0, 0], [67, 97, 116, 111, 110, 0, 0, 0, 0, 0, 0, 0, 0], [78, 97, 115, 101, 114, 0, 0, 0, 0, 0, 0, 0, 0], [65, 115, 116, 115, 97, 116, 117, 114, 111, 118, 0, 0, 0], [71, 108, 117, 104, 97, 114, 101, 118, 0, 0, 0, 0, 0], [65, 114, 105, 97, 110, 0, 0, 0, 0, 0, 0, 0, 0], [84, 114, 97, 105, 110, 0, 0, 0, 0, 0, 0, 0, 0], [71, 114, 97, 109, 101, 116, 115, 107, 121, 0, 0, 0, 0], [80, 111, 114, 111, 121, 107, 111, 118, 0, 0, 0, 0, 0], [77, 97, 114, 107, 104, 97, 109, 0, 0, 0, 0, 0, 0], [74, 105, 109, 101, 110, 101, 122, 0, 0, 0, 0, 0, 0], [82, 101, 107, 115, 104, 105, 110, 115, 107, 121, 0, 0, 0], [77, 117, 107, 97, 110, 111, 118, 0, 0, 0, 0, 0, 0], [71, 114, 111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [74, 97, 103, 101, 114, 0, 0, 0, 0, 0, 0, 0, 0], [80, 114, 105, 118, 101, 116, 116, 0, 0, 0, 0, 0, 0], [71, 111, 111, 100, 97, 99, 114, 101, 0, 0, 0, 0, 0], [84, 104, 101, 108, 119, 101, 108, 108, 0, 0, 0, 0, 0], [79, 108, 100, 102, 105, 101, 108, 100, 0, 0, 0, 0, 0], [86, 97, 115, 121, 117, 114, 105, 110, 0, 0, 0, 0, 0], [71, 101, 116, 116, 97, 0, 0, 0, 0, 0, 0, 0, 0], [86, 39, 85, 110, 110, 105, 107, 111, 118, 0, 0, 0, 0], [75, 111, 117, 114, 121, 0, 0, 0, 0, 0, 0, 0, 0], [66, 117, 116, 116, 0, 0, 0, 0, 0, 0, 0, 0, 0], [87, 111, 111, 108, 100, 114, 105, 100, 103, 101, 0, 0, 0], [83, 111, 117, 116, 104, 0, 0, 0, 0, 0, 0, 0, 0], [86, 105, 110, 99, 105, 0, 0, 0, 0, 0, 0, 0, 0], [76, 101, 118, 105, 116, 97, 110, 0, 0, 0, 0, 0, 0], [71, 101, 116, 109, 97, 110, 101, 110, 107, 111, 0, 0, 0], [84, 104, 111, 109, 115, 0, 0, 0, 0, 0, 0, 0, 0], [70, 117, 106, 105, 109, 111, 116, 111, 0, 0, 0, 0, 0], [70, 117, 108, 108, 101, 114, 111, 110, 0, 0, 0, 0, 0], [76, 97, 100, 121, 106, 101, 110, 115, 107, 121, 0, 0, 0], [70, 117, 114, 110, 105, 115, 115, 0, 0, 0, 0, 0, 0], [66, 97, 122, 97, 110, 111, 118, 0, 0, 0, 0, 0, 0], [72, 105, 110, 116, 122, 101, 110, 0, 0, 0, 0, 0, 0], [84, 111, 105, 99, 104, 107, 105, 110, 0, 0, 0, 0, 0], [85, 103, 104, 105, 0, 0, 0, 0, 0, 0, 0, 0, 0], [72, 111, 110, 111, 118, 0, 0, 0, 0, 0, 0, 0, 0], [80, 101, 105, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [87, 97, 105, 116, 101, 0, 0, 0, 0, 0, 0, 0, 0], [77, 105, 107, 104, 110, 101, 118, 105, 99, 104, 0, 0, 0], [67, 104, 97, 112, 109, 97, 110, 0, 0, 0, 0, 0, 0], [89, 97, 110, 111, 118, 115, 107, 121, 0, 0, 0, 0, 0], [66, 111, 117, 116, 114, 111, 115, 0, 0, 0, 0, 0, 0], [73, 108, 121, 117, 115, 104, 105, 110, 0, 0, 0, 0, 0], [65, 116, 116, 105, 97, 0, 0, 0, 0, 0, 0, 0, 0], [78, 111, 111, 100, 0, 0, 0, 0, 0, 0, 0, 0, 0], [90, 104, 105, 100, 101, 110, 107, 111, 0, 0, 0, 0, 0], [72, 97, 110, 107, 101, 101, 118, 0, 0, 0, 0, 0, 0], [81, 117, 105, 110, 110, 0, 0, 0, 0, 0, 0, 0, 0], [67, 108, 111, 117, 103, 104, 0, 0, 0, 0, 0, 0, 0], [75, 97, 108, 105, 110, 107, 97, 0, 0, 0, 0, 0, 0], [71, 97, 110, 105, 109, 0, 0, 0, 0, 0, 0, 0, 0], [75, 105, 106, 101, 107, 0, 0, 0, 0, 0, 0, 0, 0], [83, 101, 101, 108, 101, 110, 0, 0, 0, 0, 0, 0, 0], [74, 111, 104, 110, 115, 116, 111, 110, 0, 0, 0, 0, 0], [89, 117, 107, 104, 105, 109, 117, 107, 0, 0, 0, 0, 0], [72, 97, 115, 115, 0, 0, 0, 0, 0, 0, 0, 0, 0], [72, 97, 105, 107, 0, 0, 0, 0, 0, 0, 0, 0, 0], [83, 112, 101, 108, 108, 109, 101, 121, 101, 114, 0, 0, 0], [74, 97, 98, 114, 101, 118, 0, 0, 0, 0, 0, 0, 0], [70, 111, 114, 98, 101, 115, 0, 0, 0, 0, 0, 0, 0], [84, 97, 110, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [83, 104, 97, 109, 111, 111, 110, 0, 0, 0, 0, 0, 0], [70, 111, 115, 115, 0, 0, 0, 0, 0, 0, 0, 0, 0], [83, 117, 97, 114, 101, 122, 0, 0, 0, 0, 0, 0, 0], [75, 97, 108, 105, 116, 107, 105, 110, 0, 0, 0, 0, 0], [77, 97, 114, 99, 104, 97, 110, 117, 107, 111, 118, 0, 0], [73, 110, 103, 108, 101, 100, 101, 119, 0, 0, 0, 0, 0], [65, 110, 100, 114, 121, 117, 115, 104, 105, 110, 0, 0, 0], [73, 122, 122, 97, 114, 100, 0, 0, 0, 0, 0, 0, 0], [77, 111, 111, 110, 101, 121, 0, 0, 0, 0, 0, 0, 0], [68, 111, 110, 107, 105, 110, 0, 0, 0, 0, 0, 0, 0], [65, 98, 97, 107, 117, 109, 116, 115, 101, 118, 0, 0, 0], [68, 117, 107, 101, 0, 0, 0, 0, 0, 0, 0, 0, 0], [89, 111, 115, 104, 105, 122, 97, 119, 97, 0, 0, 0, 0], [69, 108, 108, 101, 114, 121, 0, 0, 0, 0, 0, 0, 0], [77, 97, 107, 104, 97, 108, 105, 110, 0, 0, 0, 0, 0], [68, 101, 118, 101, 114, 101, 108, 108, 0, 0, 0, 0, 0], [84, 119, 105, 103, 103, 0, 0, 0, 0, 0, 0, 0, 0], [77, 105, 121, 97, 104, 97, 114, 97, 0, 0, 0, 0, 0], [68, 111, 98, 114, 97, 106, 97, 110, 115, 107, 121, 0, 0], [89, 117, 122, 105, 110, 0, 0, 0, 0, 0, 0, 0, 0], [83, 104, 97, 105, 110, 0, 0, 0, 0, 0, 0, 0, 0], [72, 117, 100, 111, 105, 110, 97, 116, 111, 118, 0, 0, 0], [66, 101, 99, 107, 0, 0, 0, 0, 0, 0, 0, 0, 0], [83, 99, 104, 119, 97, 114, 122, 101, 110, 98, 101, 114, 103], [78, 117, 114, 105, 101, 118, 0, 0, 0, 0, 0, 0, 0], [84, 114, 117, 107, 104, 97, 110, 111, 118, 0, 0, 0, 0], [71, 111, 108, 111, 115, 101, 110, 107, 111, 0, 0, 0, 0], [82, 97, 105, 107, 104, 101, 108, 103, 97, 117, 122, 0, 0], [78, 121, 117, 104, 116, 105, 108, 105, 110, 0, 0, 0, 0], [72, 97, 114, 98, 0, 0, 0, 0, 0, 0, 0, 0, 0], [83, 97, 109, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [66, 117, 108, 103, 97, 114, 101, 108, 108, 105, 0, 0, 0], [87, 111, 111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [68, 111, 99, 104, 101, 114, 116, 121, 0, 0, 0, 0, 0], [77, 97, 115, 116, 101, 114, 115, 0, 0, 0, 0, 0, 0], [74, 109, 117, 100, 115, 107, 121, 0, 0, 0, 0, 0, 0], [82, 111, 97, 99, 104, 0, 0, 0, 0, 0, 0, 0, 0], [83, 104, 97, 109, 114, 117, 110, 0, 0, 0, 0, 0, 0], [84, 115, 118, 101, 116, 110, 111, 118, 0, 0, 0, 0, 0], [67, 104, 97, 117, 104, 97, 110, 0, 0, 0, 0, 0, 0], [71, 117, 108, 101, 118, 115, 107, 121, 0, 0, 0, 0, 0], [90, 97, 115, 108, 97, 118, 101, 116, 115, 0, 0, 0, 0], [86, 105, 99, 107, 97, 114, 115, 0, 0, 0, 0, 0, 0], [90, 101, 108, 101, 110, 111, 105, 0, 0, 0, 0, 0, 0], [75, 97, 108, 111, 115, 104, 105, 110, 0, 0, 0, 0, 0], [76, 111, 121, 111, 108, 97, 0, 0, 0, 0, 0, 0, 0], [66, 101, 107, 109, 117, 114, 122, 111, 118, 0, 0, 0, 0], [81, 117, 114, 101, 115, 104, 105, 0, 0, 0, 0, 0, 0], [66, 105, 110, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0], [83, 101, 105, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [75, 104, 111, 117, 114, 121, 0, 0, 0, 0, 0, 0, 0], [83, 97, 108, 105, 98, 97, 0, 0, 0, 0, 0, 0, 0], [77, 105, 107, 104, 97, 105, 108, 106, 117, 107, 0, 0, 0], [80, 117, 114, 100, 101, 115, 0, 0, 0, 0, 0, 0, 0], [70, 101, 108, 108, 111, 119, 115, 0, 0, 0, 0, 0, 0], [68, 97, 104, 110, 111, 0, 0, 0, 0, 0, 0, 0, 0], [84, 101, 108, 108, 105, 115, 0, 0, 0, 0, 0, 0, 0], [86, 101, 108, 105, 103, 117, 114, 97, 0, 0, 0, 0, 0], [72, 111, 108, 115, 104, 101, 118, 110, 105, 107, 111, 118, 0], [65, 98, 97, 110, 100, 111, 110, 97, 116, 111, 0, 0, 0], [74, 117, 104, 105, 109, 101, 110, 107, 111, 0, 0, 0, 0], [66, 114, 97, 100, 101, 110, 0, 0, 0, 0, 0, 0, 0], [75, 114, 101, 115, 107, 97, 115, 0, 0, 0, 0, 0, 0], [83, 105, 111, 100, 97, 0, 0, 0, 0, 0, 0, 0, 0], [72, 97, 121, 117, 97, 116, 97, 0, 0, 0, 0, 0, 0], [82, 105, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [65, 108, 109, 121, 97, 115, 104, 107, 105, 110, 0, 0, 0], [83, 117, 98, 101, 114, 116, 111, 118, 97, 0, 0, 0, 0], [82, 97, 108, 108, 105, 115, 0, 0, 0, 0, 0, 0, 0], [84, 115, 118, 101, 105, 98, 97, 0, 0, 0, 0, 0, 0], [77, 111, 106, 97, 114, 111, 118, 115, 107, 121, 0, 0, 0], [77, 105, 116, 115, 117, 104, 97, 114, 117, 0, 0, 0, 0], [87, 97, 115, 101, 109, 0, 0, 0, 0, 0, 0, 0, 0], [83, 104, 105, 114, 111, 121, 97, 109, 97, 0, 0, 0, 0], [80, 114, 105, 100, 118, 111, 114, 111, 118, 0, 0, 0, 0], [82, 97, 111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [65, 110, 100, 114, 117, 115, 101, 110, 107, 111, 0, 0, 0], [77, 105, 99, 104, 97, 108, 111, 118, 105, 99, 0, 0, 0], [66, 97, 115, 97, 114, 97, 0, 0, 0, 0, 0, 0, 0], [80, 97, 115, 99, 97, 108, 0, 0, 0, 0, 0, 0, 0], [83, 108, 101, 105, 109, 97, 110, 0, 0, 0, 0, 0, 0], [83, 105, 101, 114, 122, 97, 110, 116, 0, 0, 0, 0, 0], [68, 97, 109, 105, 97, 110, 105, 0, 0, 0, 0, 0, 0], [67, 108, 97, 121, 116, 111, 110, 0, 0, 0, 0, 0, 0], [66, 97, 105, 99, 104, 111, 114, 111, 102, 102, 0, 0, 0], [90, 104, 105, 116, 105, 110, 101, 118, 0, 0, 0, 0, 0], [84, 111, 109, 97, 110, 0, 0, 0, 0, 0, 0, 0, 0], [69, 105, 110, 100, 111, 114, 102, 0, 0, 0, 0, 0, 0], [84, 97, 104, 97, 110, 0, 0, 0, 0, 0, 0, 0, 0], [68, 111, 108, 116, 111, 110, 0, 0, 0, 0, 0, 0, 0], [67, 97, 109, 112, 98, 101, 108, 108, 0, 0, 0, 0, 0], [83, 97, 105, 100, 117, 108, 97, 101, 118, 0, 0, 0, 0], [65, 98, 105, 115, 97, 108, 111, 102, 102, 0, 0, 0, 0], [66, 114, 105, 103, 103, 115, 0, 0, 0, 0, 0, 0, 0], [76, 121, 111, 110, 0, 0, 0, 0, 0, 0, 0, 0, 0], [66, 101, 108, 108, 0, 0, 0, 0, 0, 0, 0, 0, 0], [71, 117, 100, 111, 118, 105, 99, 104, 0, 0, 0, 0, 0], [66, 97, 103, 114, 97, 116, 105, 111, 110, 0, 0, 0, 0], [75, 104, 111, 117, 114, 121, 0, 0, 0, 0, 0, 0, 0], [67, 114, 111, 119, 101, 0, 0, 0, 0, 0, 0, 0, 0], [71, 101, 114, 103, 101, 115, 0, 0, 0, 0, 0, 0, 0], [86, 105, 104, 114, 101, 118, 0, 0, 0, 0, 0, 0, 0], [78, 111, 109, 117, 114, 97, 0, 0, 0, 0, 0, 0, 0], [83, 104, 97, 109, 98, 117, 114, 107, 105, 110, 0, 0, 0], [77, 101, 115, 115, 110, 101, 114, 0, 0, 0, 0, 0, 0], [76, 121, 106, 101, 110, 107, 111, 118, 0, 0, 0, 0, 0], [84, 115, 117, 99, 104, 105, 101, 0, 0, 0, 0, 0, 0], [68, 117, 112, 111, 110, 116, 0, 0, 0, 0, 0, 0, 0], [76, 101, 98, 101, 100, 101, 118, 105, 99, 104, 0, 0, 0], [82, 122, 104, 101, 115, 104, 101, 118, 115, 107, 121, 0, 0], [77, 105, 107, 104, 97, 105, 108, 105, 110, 0, 0, 0, 0], [67, 108, 111, 115, 101, 0, 0, 0, 0, 0, 0, 0, 0], [65, 119, 97, 108, 105, 115, 104, 119, 105, 108, 105, 0, 0], [75, 111, 108, 105, 104, 97, 0, 0, 0, 0, 0, 0, 0], [76, 111, 107, 104, 97, 110, 105, 110, 0, 0, 0, 0, 0], [72, 97, 108, 107, 105, 110, 0, 0, 0, 0, 0, 0, 0], [74, 97, 110, 107, 111, 118, 115, 107, 121, 0, 0, 0, 0], [75, 97, 115, 115, 105, 115, 0, 0, 0, 0, 0, 0, 0], [78, 105, 99, 111, 108, 108, 0, 0, 0, 0, 0, 0, 0], [84, 122, 97, 107, 117, 110, 111, 118, 0, 0, 0, 0, 0], [80, 101, 101, 114, 115, 0, 0, 0, 0, 0, 0, 0, 0], [83, 116, 97, 99, 107, 0, 0, 0, 0, 0, 0, 0, 0], [78, 97, 110, 115, 111, 110, 0, 0, 0, 0, 0, 0, 0], [86, 105, 115, 107, 111, 118, 0, 0, 0, 0, 0, 0, 0], [74, 97, 110, 103, 97, 114, 98, 101, 114, 0, 0, 0, 0], [68, 117, 114, 97, 115, 111, 118, 0, 0, 0, 0, 0, 0], [65, 108, 116, 115, 104, 117, 108, 101, 114, 0, 0, 0, 0], [69, 108, 99, 111, 99, 107, 0, 0, 0, 0, 0, 0, 0], [66, 97, 107, 105, 101, 118, 0, 0, 0, 0, 0, 0, 0], [84, 105, 108, 108, 101, 110, 115, 0, 0, 0, 0, 0, 0], [74, 100, 97, 110, 107, 105, 110, 0, 0, 0, 0, 0, 0], [80, 97, 118, 108, 117, 104, 105, 110, 0, 0, 0, 0, 0], [80, 105, 115, 116, 111, 108, 107, 111, 114, 115, 0, 0, 0], [78, 97, 106, 106, 97, 114, 0, 0, 0, 0, 0, 0, 0], [77, 97, 107, 117, 100, 97, 0, 0, 0, 0, 0, 0, 0], [79, 39, 68, 111, 104, 101, 114, 116, 121, 0, 0, 0, 0], [68, 109, 111, 104, 111, 118, 115, 107, 121, 0, 0, 0, 0], [72, 117, 97, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [75, 111, 103, 97, 114, 97, 0, 0, 0, 0, 0, 0, 0], [76, 101, 97, 100, 108, 101, 121, 0, 0, 0, 0, 0, 0], [89, 111, 111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [69, 115, 115, 97, 110, 0, 0, 0, 0, 0, 0, 0, 0], [65, 109, 97, 114, 105, 0, 0, 0, 0, 0, 0, 0, 0], [89, 97, 107, 117, 115, 104, 107, 105, 110, 0, 0, 0, 0], [74, 111, 110, 103, 111, 108, 111, 118, 105, 99, 104, 0, 0], [83, 99, 104, 109, 105, 100, 116, 0, 0, 0, 0, 0, 0], [77, 105, 104, 97, 105, 108, 111, 118, 115, 107, 121, 0, 0], [65, 119, 115, 101, 101, 110, 107, 111, 0, 0, 0, 0, 0], [78, 101, 109, 101, 99, 0, 0, 0, 0, 0, 0, 0, 0], [75, 105, 108, 115, 104, 97, 119, 0, 0, 0, 0, 0, 0], [68, 111, 98, 114, 105, 107, 0, 0, 0, 0, 0, 0, 0], [66, 114, 105, 110, 107, 101, 114, 104, 111, 102, 102, 0, 0], [65, 103, 97, 109, 111, 102, 102, 0, 0, 0, 0, 0, 0], [65, 115, 116, 114, 97, 116, 111, 118, 0, 0, 0, 0, 0], [76, 97, 116, 104, 101, 121, 0, 0, 0, 0, 0, 0, 0], [66, 97, 98, 97, 0, 0, 0, 0, 0, 0, 0, 0, 0], [66, 97, 110, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [84, 115, 105, 103, 101, 108, 110, 105, 107, 0, 0, 0, 0], [71, 114, 105, 103, 111, 114, 105, 101, 118, 0, 0, 0, 0], [86, 97, 100, 111, 118, 115, 107, 105, 0, 0, 0, 0, 0], [84, 105, 102, 102, 116, 0, 0, 0, 0, 0, 0, 0, 0], [78, 97, 107, 97, 110, 111, 0, 0, 0, 0, 0, 0, 0], [71, 97, 103, 101, 110, 0, 0, 0, 0, 0, 0, 0, 0], [86, 97, 110, 100, 97, 108, 101, 0, 0, 0, 0, 0, 0], [80, 101, 108, 108, 101, 110, 101, 110, 0, 0, 0, 0, 0], [80, 97, 122, 121, 0, 0, 0, 0, 0, 0, 0, 0, 0], [72, 117, 122, 105, 121, 97, 116, 111, 118, 0, 0, 0, 0], [65, 115, 107, 101, 114, 0, 0, 0, 0, 0, 0, 0, 0], [78, 97, 115, 115, 97, 114, 0, 0, 0, 0, 0, 0, 0], [75, 111, 109, 97, 116, 115, 117, 122, 97, 107, 105, 0, 0], [79, 114, 108, 97, 110, 100, 111, 0, 0, 0, 0, 0, 0], [65, 98, 100, 117, 108, 98, 101, 107, 111, 102, 102, 0, 0], [82, 97, 118, 101, 110, 115, 99, 114, 111, 102, 116, 0, 0], [68, 97, 110, 105, 110, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-107-7b4ec669b809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Train cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Testing cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-0790e9648d21>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"type make_variable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"len \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_countries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcountries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-106-b96b23e281a6>\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(words, countries)\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;31m#2 padding done 3 pack paadding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mword_packed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_padd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"words_packed type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_packed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_packed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_packed_sequence_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: _pack_padded_sequence(): argument 'input' (position 1) must be Tensor, not list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ah7s-6iA706",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class net(nn.Module):\n",
        "  # model = net(N_chars, Hidden_size, N_countries, N_layers)\n",
        "  def __init__(self, batch_size, hidden_size, N_countries, N_layers=1):\n",
        "    super(net, self).__init__()\n",
        "    # print(\"type\", type(batch_size), type(hidden_size), type(N_countries), type(N_layers))\n",
        "    # print(\"value are\", batch_size, hidden_size, N_countries, N_layers)\n",
        "\n",
        "    self.vocab_size = batch_size #(it is no of unique words now its a whole batch)\n",
        "    Embessing_dim = hidden_size # how many input hudden layer can take for each word.\n",
        "    #lenth of each word ie after padding  no (its acharacter)\n",
        "    self.num_layer = N_layers\n",
        "    self.num_bidirection =1\n",
        "    self.embed = torch.nn.Embedding(batch_size , hidden_size ) # for each word we will get many output = (eq = input to hidden) \n",
        " \n",
        "    self.grucell = nn.GRU(input_size=hidden_size, hidden_size= hidden_size, num_layers= N_layers )\n",
        " \n",
        "    self.fc = nn.Linear(hidden_size, N_countries)\n",
        " \n",
        "  def forward(self, word_packed):\n",
        "    print(\"word_packed type\", type(word_packed))\n",
        "    self.input = word_packed #B*S to S*B\n",
        "    self.input = input.t() # #B*S to S*B\n",
        "\n",
        "    #S*B ==> S *B *E\n",
        "    print(\"\\nTarget\", word_packed.shape, type(word_packed), word_packed)\n",
        "    input = self.embed(self.input) \\\n",
        "\n",
        "    print(\"\\nTarget\", output.shape, type(output), output)\n",
        "\n",
        "    hidden_0 = hidden_init(batch_size)\n",
        "\n",
        "    output, h_n = self.grucell(input, hidden_0)\n",
        "    print(\"output shape\", output.shape, \"h_n shape\", h_n.shape)\n",
        "    print(\"output type\", type(output), \"h_n type\", type(h_n))\n",
        "\n",
        "    logit = self.fc(h_n)\n",
        "\n",
        "    return logit\n",
        "\n",
        "  def hiddden_init(self, batch_size):\n",
        "    h_0 = torch.zeros(self.num_layer*self.num_bidirection, batch_size, self.hidden_size)\n",
        "    return h_0\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7m7E_KmFjUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(len(N_countries))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9tn9StnzxD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "  # for epoch in rancge(N_epoch):\n",
        "  total_loss = 0\n",
        "  for id, (words, countries) in enumerate(train_loader):\n",
        "    # make the word proper format ie: words, countryuy to  nos then padding then tensor. \n",
        "    print(\"type make_variable\", type(words), type(countries))\n",
        "    print(\"len \", batch_size, hidden_size, N_countries, N_layers)\n",
        "    words, target = make_variable(words, countries)\n",
        "\n",
        "    output = model(words)\n",
        "      \n",
        "    loss = criterion(output, target)\n",
        "    total_loss += loss.item()\n",
        "    model.zeros_grad() # learnig we are making gradindier parameter of model not loss \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i % 10 == 0:\n",
        "          print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.2f}'.format(\n",
        "              time_since(start), epoch,  i *\n",
        "              len(names), len(train_loader.dataset),\n",
        "              100. * i * len(names) / len(train_loader.dataset),\n",
        "              total_loss / i * len(names)))\n",
        "          \n",
        "    return total_loss     "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76IibzwSU-Eu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "  loss_value = 0\n",
        "  for id, (words, countries) in enumerate(test_loader):\n",
        "    words, target = make_variable(words, countries)\n",
        "\n",
        "    pred = model(words)\n",
        "    pred_country = pred.max(1) # we got cuntry id now\n",
        "\n",
        "    # find loss\n",
        "\n",
        "    if (pred_country == target):\n",
        "      loss_value = loss_value\n",
        "    else:\n",
        "      loss_value = loss_value +1\n",
        "\n",
        "  return loss_value"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C114-pbMtdpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eETP6bqmXu4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pred(word):\n",
        "  output = model(word)\n",
        "  country = train_database.get_country_by_id(output.max(1))\n",
        "  return country"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veyLI6eEu3sd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #test loader\n",
        "# database_test = Name('/content/drive/My Drive/AI/basic/data/names_train/names_test.csv')\n",
        "# test_loader = DataLoader(dataset=database_test, batch_size= BS, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iowTb3mpu4Cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB7yA-tEOwqL",
        "colab_type": "text"
      },
      "source": [
        "working on asc and tranposzing .t() is it tup[le , tensor or what then next action\n",
        "1. remove asc use direct embedding\n",
        "2. use asc only noo emabeddinbg as asx itself is embedding\n",
        "by my case ASC is streamline word length as well. so try to keep asx alive for time being untill batch size is 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VwFV7Z_eWCF",
        "colab_type": "text"
      },
      "source": [
        "task3: model: embedding, rnn, linear layer \n",
        "task4: optimizer\n",
        "task5: training\n",
        "taask6: test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqEmGV5GkBD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class net(torch.nn.Module): #learning we can get fuction created inside database (it can be train or test)\n",
        "#   def __init__(self):\n",
        "#     super(net,self).__init__()\n",
        "#     self.word_len = 18 # seq chenged from 15 to 18 for test\n",
        "#     self.vocab_size = self.word_len # no of character per word\n",
        "#     self.embedding_dim = 6 # how many diamention it wants\n",
        "#     self.input_size = self.embedding_dim #6\n",
        "#     self.hiddenO = self.input_size #6 dim\n",
        "#     self.hidden_size = self.embedding_dim #6\n",
        "#     self.batch = 1\n",
        "#     self.num_layer =1\n",
        "#     self.num_class = 18 #num of contries\n",
        "\n",
        "#     #embedding  @learning use tensor conversion al last meand at GRU ot RNN time not beofore that as arugment of other function (like embed) are not tensors\n",
        "#     self.embed = nn.Embedding(num_embeddings = self.vocab_size, embedding_dim = self.embedding_dim) # num_enn = input size word lenth and embedd is output size ie hidden size\n",
        "#     #gru    \n",
        "#     self.grucell = nn.GRU(input_size = self.input_size, hidden_size = self.hidden_size, num_layers =self.num_layer, batch_first = True)\n",
        "#     #fc\n",
        "#     self.fc = nn.Linear(in_features= self.embedding_dim, out_features= self.num_class)\n",
        "  \n",
        "#   def forward(self, word, country): # 10 word and 10 country\n",
        "#     # word, contry = data\n",
        "#     print(\"word\", word) # word type is tuple of B*S\n",
        "#     #converting work in to integer ASCII for input and outpuut ord # learning asc is not required as embedding can take words directly\n",
        "     \n",
        "#     word_ascii, w_len = self.asc(word, self.word_len) #in: BS * word and out: BS * ASCII with padding\n",
        "#     print(\"\\n word_ascii\", word_ascii,  \"\\n w_len\", w_len)\n",
        "#       # challenge we need some padding. keep every word as 18 character long.\n",
        "#       #learning embedding needs word(s) * BS  ir S*B not B*S\n",
        "      \n",
        "#     word_ascii = list(zip(*word_ascii)) # in B*S ot S*B\n",
        "#     # print(\"word ascii \", word_ascii)\n",
        "#     word_ascii = torch.LongTensor(word_ascii)\n",
        "\n",
        "#     #@ learning so word equalization you shoiuld do after embedfding  not before that. otherwise embedding will not ass anything\n",
        "\n",
        "#     word_embed = self.embed(word_ascii) #embedding e extra in matrix out : BS* word_len*E\n",
        "#       #rnn in and out rnn_in BS*Seq*inu_D(e) and rnn_out BS*Seq*hiddenO\n",
        "#       # input = B*Seq_len*input, hidden = num_lauyer* batch,hidden_size\n",
        "#     input = word_embed # B*S*I\n",
        "#     hidden = self.hidden_init()\n",
        "#     out, h_n = self.grucell(input, hidden) #out B*S*hidden [1 10 6] , h_n = num_layerxBxHidden [1 1 6]\n",
        "\n",
        "#     #FC linear layer in Seq to out no of classor contry(-1, nod of class or country)\n",
        "#     out = self.fc(h_n) #in [1 1 6] out [1 1 18]\n",
        "#     out = out.view(-1, self.num_class)\n",
        "#     # out - out.t()\n",
        "#     country_ids = self.country2tensor(country) # 18 contoury with each 15 word\n",
        "#     # country = country.t()#view(-1)\n",
        "#     # print(\"country ids\", country_ids.shape)\n",
        "#     # country = country.squeeze_()\n",
        "#     # out = out.squeeze_()\n",
        "#     return out, country_ids\n",
        "\n",
        "#     #output output of DC \n",
        "#   def hidden_init(self):\n",
        "#     hid = torch.zeros(self.num_layer, self.batch, self.hiddenO)\n",
        "#     return hid\n",
        "#   def country2tensor(self, countries):\n",
        "#     # print(\"contries size\", len(countries))\n",
        "#     country_ids = [database.get_id_by_country(each_country) for each_country in countries]\n",
        "#     # print(\"contries ids\", len(country_ids))\n",
        "#     return torch.LongTensor(country_ids)\n",
        "\n",
        "#   def asc(self, words, word_len):\n",
        "#     word_len = word_len\n",
        "#     sorks3 =  []\n",
        "#     # sorks3 = torch.zeros[len(words)]\n",
        "#     for word in words:\n",
        "#       # # for cha in word:\n",
        "#       # sorks = [ord(cha) for cha in word]\n",
        "#       # sorks2 = [sorks + [0]*(word_len-len(sorks))] \n",
        "#       # sorks3 = Variable(torch.LongTensor([sorks2]))\n",
        "#       # sorks4 = sorks4.append(sorks3)\n",
        "      \n",
        "#       # sorks3.append(Variable(torch.LongTensor([[ord(cha) for cha in word] + [0]*(word_len-len([ord(cha) for cha in word]))])))\n",
        "#       sorks3.append([[ord(cha) for cha in word] + [0]*(word_len-len([ord(cha) for cha in word]))])\n",
        "#       # i = i+1\n",
        "#     return sorks3, len(sorks3) \n",
        "\n",
        "\n",
        "# model = net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMdjdhGMs1FQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class net(torch.nn.Module): #learning we can get fuction created inside database (it can be train or test)\n",
        "#   def __init__(self):\n",
        "#     super(net,self).__init__()\n",
        "#     self.word_len = 18 # seq chenged from 15 to 18 for test\n",
        "#     self.vocab_size = self.word_len # no of character per word\n",
        "#     self.embedding_dim = 6 # how many diamention it wants\n",
        "#     self.input_size = self.embedding_dim #6\n",
        "#     self.hiddenO = self.input_size #6 dim\n",
        "#     self.hidden_size = self.embedding_dim #6\n",
        "#     self.batch = 1\n",
        "#     self.num_layer =1\n",
        "#     self.num_class = 18 #num of contries\n",
        "\n",
        "#     #embedding  @learning use tensor conversion al last meand at GRU ot RNN time not beofore that as arugment of other function (like embed) are not tensors\n",
        "#     self.embed = nn.Embedding(num_embeddings = self.vocab_size, embedding_dim = self.embedding_dim) # num_enn = input size word lenth and embedd is output size ie hidden size\n",
        "#     #gru    \n",
        "#     self.grucell = nn.GRU(input_size = self.input_size, hidden_size = self.hidden_size, num_layers =self.num_layer, batch_first = True)\n",
        "#     #fc\n",
        "#     self.fc = nn.Linear(in_features= self.embedding_dim, out_features= self.num_class)\n",
        "  \n",
        "#   def forward(self, word, country): # 10 word and 10 country\n",
        "#     # word, contry = data\n",
        "#     print(\"word\", word) # word type is tuple of B*S\n",
        "\n",
        "#     # challenge we need some padding. keep every word as 18 character long.\n",
        "#     #learning embedding needs word(s) * BS  ir S*B not B*S\n",
        "      \n",
        "#     word_ascii = list(zip(*word_ascii)) # in B*S ot S*B\n",
        "#     # print(\"word ascii \", word_ascii)\n",
        "#     word_ascii = torch.LongTensor(word_ascii)\n",
        "\n",
        "#     #@ learning so word equalization you shoiuld do after embedfding  not before that. otherwise embedding will not ass anything\n",
        "    \n",
        "#     word_embed = self.embed(word_ascii) #embedding e extra in matrix out : BS* word_len*E\n",
        "#       #rnn in and out rnn_in BS*Seq*inu_D(e) and rnn_out BS*Seq*hiddenO\n",
        "#       # input = B*Seq_len*input, hidden = num_lauyer* batch,hidden_size\n",
        "#     input = word_embed # B*S*I\n",
        "#     hidden = self.hidden_init()\n",
        "#     out, h_n = self.grucell(input, hidden) #out B*S*hidden [1 10 6] , h_n = num_layerxBxHidden [1 1 6]\n",
        "\n",
        "#     #FC linear layer in Seq to out no of classor contry(-1, nod of class or country)\n",
        "#     out = self.fc(h_n) #in [1 1 6] out [1 1 18]\n",
        "#     out = out.view(-1, self.num_class)\n",
        "#     # out - out.t()\n",
        "#     country_ids = self.country2tensor(country) # 18 contoury with each 15 word\n",
        "#     # country = country.t()#view(-1)\n",
        "#     # print(\"country ids\", country_ids.shape)\n",
        "#     # country = country.squeeze_()\n",
        "#     # out = out.squeeze_()\n",
        "#     return out, country_ids\n",
        "\n",
        "#     #output output of DC \n",
        "#   def hidden_init(self):\n",
        "#     hid = torch.zeros(self.num_layer, self.batch, self.hiddenO)\n",
        "#     return hid\n",
        "#   def country2tensor(self, countries):\n",
        "#     # print(\"contries size\", len(countries))\n",
        "#     country_ids = [database.get_id_by_country(each_country) for each_country in countries]\n",
        "#     # print(\"contries ids\", len(country_ids))\n",
        "#     return torch.LongTensor(country_ids)\n",
        "\n",
        "#   def asc(self, words, word_len):\n",
        "#     word_len = word_len\n",
        "#     sorks3 =  []\n",
        "#     # sorks3 = torch.zeros[len(words)]\n",
        "#     for word in words:\n",
        "#       # # for cha in word:\n",
        "#       # sorks = [ord(cha) for cha in word]\n",
        "#       # sorks2 = [sorks + [0]*(word_len-len(sorks))] \n",
        "#       # sorks3 = Variable(torch.LongTensor([sorks2]))\n",
        "#       # sorks4 = sorks4.append(sorks3)\n",
        "      \n",
        "#       # sorks3.append(Variable(torch.LongTensor([[ord(cha) for cha in word] + [0]*(word_len-len([ord(cha) for cha in word]))])))\n",
        "#       sorks3.append([[ord(cha) for cha in word] + [0]*(word_len-len([ord(cha) for cha in word]))])\n",
        "#       # i = i+1\n",
        "#     return sorks3, len(sorks3) \n",
        "\n",
        "# model = net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlsCGYjf5XtB",
        "colab_type": "text"
      },
      "source": [
        "Process contry differently as each contry will have unique no (one number not as matrix as word (N * no of class) will be comparing or predicting the perticular class. so its a one idea to keep perticular class as one number rather than keeping it as one class as array.\n",
        "\n",
        "TODO each contry as one unique no.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bkGf6xpwy60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = .0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdzgP6gTxXdp",
        "colab_type": "text"
      },
      "source": [
        "Trainig\n",
        "\n",
        "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8YXuz0AwyyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for epoch in range(1):\n",
        "#   total_loss = 0 \n",
        "#   # batch_id = 0 \n",
        "#   for batch_idx, (word, country) in enumerate(train_loader): #@ learning loader is sending tuple on each count\n",
        "#     print(\"word database\", type(word), \"countey database\", type(country))\n",
        "#     out, country = model(word, country)\n",
        "#     print(\"out shape\", out.shape, \"countey shape\", country.shape)\n",
        "#     optimizer.zero_grad()\n",
        "#     loss = Criterion(out, country)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     if batch_idx % 500  == 0:\n",
        "#         print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "#                 epoch, batch_idx * len(word), len(train_loader.dataset),\n",
        "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhLo9HN7AiZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n= [\"name\"]\n",
        "# # n[1]\n",
        "# # ord(n[1])\n",
        "# print(asc(n,10)[0].size())\n",
        "# a, b = asc(n,10)\n",
        "# embed=nn.Embedding(200,3)\n",
        "# emb2 = embed(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOS2GPINm6EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(emb2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoqHLwaEAqmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # sorks= []\n",
        "  # i=0\n",
        "  # def asc(words, word_len):\n",
        "  #   word_len = word_len\n",
        "\n",
        "  #   for word in words:\n",
        "  #     # for cha in word:\n",
        "  #     sorks = [ord(cha) for cha in word]\n",
        "  #     sorks2 = sorks + [0]*(word_len-len(sorks)) \n",
        "  #     sorks3 = Variable(torch.LongTensor([sorks2]))\n",
        "  #   return sorks3, len(sorks2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e02g7x7BWQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIyELK02B22g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n1 =asc(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzTcqLLZTcoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n2 = concat(n1, zeros[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqvdvxq4Th8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# like_zero[2]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}