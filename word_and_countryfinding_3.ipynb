{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word and countryfinding_3.ipynb",
      "provenance": [],
      "mount_file_id": "1WLaQFEWZEn5iCXJZq8y6Le2f8LNrRBT4",
      "authorship_tag": "ABX9TyM+U9Eq266CGHsF9sLmB//q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayk19/Assignment/blob/master/word_and_countryfinding_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WigN-7SHh93O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f05613d8-1a9a-4ecf-f35d-ad1c49395ef4"
      },
      "source": [
        "cd /content/drive/My Drive/AI/basic/data/names_train"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/AI/basic/data/names_train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4jmwmZDk8U6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import csv\n",
        "import gzip\n",
        "\n",
        "# from name_dataset import NameDataset\n",
        "# from torch.nn.utils.rnn import pack_padded_sequence, pad_paked_sequence"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP7rj3DGBMSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# xy1 = pd.read_csv(\"names_train.csv\") #q is it require to convert it(text) into torch\n",
        "# # xy1.shape\n",
        "# # name = (xy1.iloc[0,:][0])\n",
        "# # name\n",
        "# xy1.describe\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZQHLEWca7i5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word = xy1['Adsit'].tolist()\n",
        "# country = xy1['Czech'].tolist()\n",
        "# print(len(word))\n",
        "# print(len(country))\n",
        "# print(list(dict.fromkeys(country))) # it will provide only unique country name only"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8UihLr-vsrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # filename = '/content/drive/My Drive/AI/basic/data/names_train/names_train.csv.gz' if is_train_set else '/content/drive/My Drive/AI/basic/data/names_train/names_test.csv.gz'\n",
        "# if is_train_set:\n",
        "#   filename = '/content/drive/My Drive/AI/basic/data/names_train/names_train.csv.gz'\n",
        "# else :\n",
        "#   filename = '/content/drive/My Drive/AI/basic/data/names_train/names_test.csv.gz'\n",
        "\n",
        "# with open(filename, \"rt\") as f: # learning if file is zip with file the gzip.open. if only csv file then only open(filename, \"rt\")\n",
        "#   reader = csv.reader(f)\n",
        "#   rows = list(reader)\n",
        "# print(rows)\n",
        "# naam = [row[0] for row in rows]\n",
        "# country = [row[1] for row in rows]\n",
        "\n",
        "# print(len(naam), naam)\n",
        "# print(len(country), country)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPr7qIASeOAW",
        "colab_type": "text"
      },
      "source": [
        "Tast1 : create database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI8_0khQiL9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Name(Dataset):\n",
        "  def __init__(self, is_train_set = False):\n",
        "  # def __init__(self, csv_file):\n",
        "\n",
        "    if is_train_set:\n",
        "      filename = '/content/drive/My Drive/AI/basic/data/names_train/names_train.csv'\n",
        "    else :\n",
        "      filename = '/content/drive/My Drive/AI/basic/data/names_train/names_test.csv'\n",
        "\n",
        "    with open(filename, \"rt\") as f: # learning if file is zip with file the gzip.open. if only csv file then only open(filename, \"rt\")\n",
        "      reader = csv.reader(f)\n",
        "      rows = list(reader)\n",
        "    # print(rows)\n",
        "    self.word = [row[0] for row in rows]\n",
        "    self.country = [row[1] for row in rows]\n",
        "    # self.xy = pd.read_csv(csv_file)\n",
        "    # self.word = self.xy['Adsit'].tolist()\n",
        "    # self.country = self.xy['Czech'].tolist()\n",
        "    self.country_list = list(dict.fromkeys(self.country))\n",
        "    \n",
        "  def __getitem__(self, id):\n",
        "    return self.word[id], self.country[id]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.word)\n",
        "\n",
        "  def get_country_list(self):\n",
        "    return self.country_list\n",
        "  #get the unique country name by its id or lebel\n",
        "  def get_country_by_id(self, id):\n",
        "    return self.country_list[id] #@learning callable meance function() but in list it is [] (as list is not callable)\n",
        "  \n",
        "  def get_id_by_country(self, country):\n",
        "    return self.country_list.index(country)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE5dHVRWgOye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "44cb455a-9364-413d-d99d-0ab3abab2320"
      },
      "source": [
        "#test database\n",
        "if __name__ == '__main__':\n",
        "  database = Name(True) # fasle for train set and true for test set\n",
        "  print(database.get_country_list())\n",
        "  print(database.get_id_by_country('French'))\n",
        "  print(database.get_country_by_id(7))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Czech', 'German', 'Arabic', 'Japanese', 'Chinese', 'Vietnamese', 'Russian', 'French', 'Irish', 'English', 'Spanish', 'Greek', 'Italian', 'Portuguese', 'Scottish', 'Dutch', 'Korean', 'Polish']\n",
            "7\n",
            "French\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_3SaEO7eTZB",
        "colab_type": "text"
      },
      "source": [
        "tast2: Create dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln00GiUsq3Mg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e64d48fc-6289-4452-a8ce-8afb62a35bcb"
      },
      "source": [
        "# parameter s and loaderts train loader\n",
        "# net(N_chars, Hidden_size, N_countries, N_layers)\n",
        "\n",
        "N_layers = 1\n",
        "N_countries = len(Name(True).get_country_list())\n",
        "Hidden_size = 20\n",
        "N_epoch =1\n",
        "N_chars = 128\n",
        "batch_size = 256\n",
        "\n",
        "database_train = Name(True)\n",
        "train_loader = DataLoader(dataset=database_train, batch_size= batch_size, shuffle=True)\n",
        "\n",
        "database_test = Name(False)\n",
        "train_loader = DataLoader(dataset=database_test, batch_size= batch_size, shuffle=True)\n",
        "print(len(database_train), len(database_test))\n",
        "print(database_train[2000],database_test[2000])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13374 6700\n",
            "('Salib', 'Arabic') ('Bakrymov', 'Russian')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6Zsgm90nzpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for val in enumerate(train_loader):\n",
        "#   print(val[0])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNNbrDogzyE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#common funtions \n",
        "def make_variable(words, countries):#word to asci charected then padding then pack padd\n",
        "  words_asc = []\n",
        "  for word in words:\n",
        "    words_asc = words_asc + [ascii(word)[0]] #1done now to padding (max len of batch) \n",
        "\n",
        "  print(\"ascii word type\", type(words_asc), len(words_asc))\n",
        "  print(\"ascii word value\",words_asc)\n",
        "  max_len_word = max(word_asc) # torch.max works in pytorch to change words_asc to tensor\n",
        "  print(\"max_len_word value\",max_len_word)\n",
        "  # padding\n",
        "  for each in words_asc[0]:\n",
        "    words_padd = words_padd + [0]*(max_len_word-len(each))\n",
        "  \n",
        "  #2 padding done 3 pack paadding\n",
        "\n",
        "  word_packed = torch.nn.utils.rnn.pack_padded_sequence(words_padd, max_len_word)\n",
        "\n",
        "  Country_ids = [database_train.get_id_by_country(country) for country in countries]\n",
        "  target = torch.LongTensor(Country_ids)\n",
        "  print(\"\\ncountry_ids\", Country_ids.shape, type(Country_ids), Country_ids)\n",
        "  print(\"\\nTarget\", target.shape, type(target), target)\n",
        "\n",
        "  return word_packed, target\n",
        "\n",
        "def ascii(word):\n",
        "  word_asc = [ord(cha) for cha in word]\n",
        "  # return torch.LongTensor(word_asc), len(word_asc)\n",
        "  return word_asc, len(word_asc)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LSvt7qatduk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "90a8ed93-6ac1-4ee0-fcc5-1d68bb2d6b88"
      },
      "source": [
        "#final execution funciton\n",
        "if __name__ == '__main__':\n",
        "  model = net(N_chars, Hidden_size, N_countries, N_layers)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
        "  criterian = nn.CrossEntropyLoss()\n",
        "\n",
        "  start = time.time()\n",
        "  print(\"Traing for %d epochs ...:\" % N_epoch)\n",
        "  for epoch in range(1, N_epoch+1):\n",
        "    # Train cycle\n",
        "    train()\n",
        "    # Testing cycle\n",
        "    test()\n",
        "\n",
        "    #testing several samples\n",
        "    test(\"Sung\")"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type <class 'int'> <class 'int'>\n",
            "value are 18 1\n",
            "to emedding\n",
            "to grucell\n",
            "to linear\n",
            "to linear\n",
            "Traing for 1 epochs ...:\n",
            "type make_variable <class 'tuple'> <class 'tuple'>\n",
            "ascii word type <class 'list'> 256\n",
            "ascii word value [[80, 111, 99, 111, 99, 107], [77, 105, 107, 104, 97, 105, 108], [90, 104, 105, 103, 97, 105, 108, 111], [74, 105, 114, 100, 101, 116, 115, 107, 121], [67, 104, 117, 103, 97, 105, 110, 111, 118], [67, 111, 109, 112, 116, 111, 110], [77, 97, 108, 111, 117, 102], [72, 101, 97, 116, 108, 101, 121], [66, 101, 105, 108, 105, 110], [84, 111, 118, 98, 105, 99, 104], [86, 101, 100, 101, 114, 110, 105, 107, 111, 118], [65, 109, 97, 114, 105], [66, 105, 108, 105, 97, 115], [89, 101, 97, 116, 101, 115], [83, 97, 108, 108, 105, 115], [84, 99, 104, 101, 107, 104, 111, 101, 118], [77, 105, 104], [75, 111, 122, 97], [68, 111, 98, 114, 111, 118, 115, 107, 121], [85, 114, 111, 103, 97, 116, 97, 121, 97], [83, 97, 114, 114, 97, 102], [80, 111, 107, 104, 105, 108, 99, 104, 117, 107], [89, 97, 110, 105, 110], [89, 111, 107, 111, 121, 97, 109, 97], [65, 103, 97, 109, 111, 102, 102], [75, 97, 108, 105, 107, 104, 109, 97, 110], [65, 98, 98, 111, 117, 100], [65, 119, 122, 97, 108, 111, 102, 102], [72, 105, 110, 116, 122, 101, 110], [75, 97, 117], [79, 108, 105, 118, 101, 114], [72, 97, 114, 98], [82, 101, 103, 105, 115, 116, 101, 114], [83, 104, 97, 108, 100, 121, 98, 105, 110], [77, 97, 116, 116, 104, 101, 119, 115], [90, 104, 97, 116, 107, 111, 118], [84, 117, 108, 107, 105, 110], [65, 115, 108, 97, 109], [83, 104, 101, 114, 114, 121], [77, 111, 104, 114, 101, 110], [74, 105, 108, 116, 115, 111, 118], [83, 99, 105, 97, 99, 99, 104, 105, 116, 97, 110, 111], [76, 105, 118, 97, 100, 110, 121], [71, 117, 100, 122], [66, 105, 115, 104, 97, 114, 97], [80, 117, 115, 99, 104, 105, 110], [75, 105, 114, 107, 109, 97, 110], [65, 115, 104, 119, 101, 108, 108], [83, 99, 104, 101, 98, 108, 121, 107, 105, 110], [77, 101, 110, 101, 110, 100, 101, 122], [85, 107, 105, 121, 111], [90, 111, 98, 110, 105, 110], [73, 114, 111, 110, 109, 97, 110], [83, 104, 97, 100, 107, 104, 97, 110], [90, 104, 101, 114, 110, 111, 118], [68, 97, 118, 105, 100], [69, 121, 101, 116, 116], [65, 108, 98, 97, 116, 115], [77, 111, 114, 116, 105, 109, 101, 114], [66, 101, 108, 109, 111, 110, 116, 101], [77, 97, 116, 101, 117, 115], [75, 97, 108, 105, 110, 107, 97], [77, 97, 114, 116, 121, 110, 111, 118, 115, 107, 121], [68, 101, 32, 115, 97, 117, 118, 101, 116, 101, 114, 114, 101], [65, 114, 101, 99, 104, 97, 118, 97, 108, 101, 116, 97], [84, 115, 97, 114, 101, 110, 107, 111], [82, 97, 100, 101, 109, 97, 107, 101, 114], [78, 97, 122, 97, 114, 105], [72, 101, 114, 114, 105, 111, 116], [66, 97, 106, 117, 116, 107, 105, 110], [80, 111, 107, 104, 118, 105, 115, 110, 101, 118], [82, 117, 122, 104, 105, 108, 111], [68, 114, 105, 116, 115, 97, 115], [77, 105, 110, 103], [71, 114, 111, 115, 102, 101, 108, 100], [71, 114, 111, 111, 115], [67, 97, 108, 118, 101, 114, 116], [86, 105, 108, 108, 105, 97, 109], [71, 114, 111, 115, 104, 107, 111, 118], [82, 117, 100, 97], [75, 97, 122, 97], [86, 97, 110, 107, 101], [75, 108, 101, 105, 100], [66, 97, 98, 111, 115, 104, 107, 105, 110], [84, 105, 109, 107, 111, 118], [84, 117, 114, 103, 101, 110, 101, 118], [72, 97, 119, 107, 101, 115], [80, 111, 108, 105, 101, 118], [68, 101, 109, 105, 99, 104], [66, 97, 107, 104, 97, 110, 111, 118], [66, 101, 108, 111, 103, 108, 97, 122, 111, 118], [69, 103, 103, 98, 121], [84, 114, 97, 118, 105, 101, 115, 111], [77, 105, 107, 104, 97, 108, 107, 105, 110], [66, 114, 111, 116, 122], [75, 97, 115, 115, 97, 98], [74, 105, 116, 105, 110], [70, 111, 110, 115, 101, 99, 97], [74, 111, 110, 103, 111, 108, 111, 118, 105, 99, 104], [86, 111, 122, 103, 105, 108, 101, 118, 105, 99, 104], [83, 104, 97, 108, 104, 111, 117, 98], [71, 111, 114, 102, 105, 110, 107, 101, 108], [76, 105, 99, 104, 110, 111, 118], [67, 104, 117, 110, 103], [65, 110, 116, 121, 115, 104, 101, 118], [68, 111, 118, 108, 97, 100, 98, 101, 103, 121, 97, 110], [67, 104, 97, 112, 107, 111], [68, 111, 119, 110, 101, 115], [65, 118, 104, 105, 109, 111, 118, 105, 99, 104], [77, 97, 104, 97, 110, 107, 111, 118], [90, 97, 115, 121, 97, 100, 39, 75, 111], [74, 97, 107, 105, 114], [86, 105, 103, 111], [84, 117, 114, 111, 118], [65, 110, 116, 97, 114], [68, 97, 118, 121, 100, 111, 118], [75, 97, 98, 97, 110, 111, 118], [68, 122, 104, 97, 110, 117, 109, 111, 118], [89, 117, 107, 97, 108, 111, 118], [77, 97, 116, 101, 106, 107, 97], [66, 97], [65, 100, 101, 108, 104, 97, 110, 111, 102, 102], [66, 97, 107, 117, 114, 111, 118], [66, 97, 106, 101, 110, 111, 118], [70, 105, 110, 97, 110], [77, 97, 114, 111, 118], [77, 117, 110, 115, 116, 101, 114], [69, 97, 118, 101, 115], [85, 115, 116, 105, 108, 111, 118, 115, 107, 121], [72, 105, 114, 97], [90, 97, 98, 97, 108, 97], [83, 104, 105, 98, 97, 115, 97, 107, 105], [74, 97, 107, 104, 111, 116], [76, 105, 108, 108, 121], [66, 101, 105, 122, 101, 114, 111, 118], [72, 97, 114, 97, 122], [67, 97, 112, 101, 108, 108, 111], [84, 111, 109, 97], [76, 101, 97, 100, 108, 101, 121], [65, 98, 117, 108, 107, 104, 97, 110, 111, 118], [84, 97, 108, 121, 103, 105, 110], [65, 113, 117, 97], [80, 108, 105, 115, 111, 118], [75, 101, 114, 114, 105, 115, 111, 110], [90, 104, 105, 116, 105, 110, 101, 118], [71, 117, 108, 100, 114, 101, 105, 104], [76, 101, 118, 105, 99, 104, 101, 118], [71, 117, 107, 97, 115, 111, 118], [90, 97, 115, 117, 104, 97], [84, 119, 105, 103, 103], [85, 114, 98, 105, 110, 97], [72, 97, 114, 105, 116, 111, 110, 101, 110, 107, 111], [77, 105, 99, 104, 97, 108, 111, 118, 105, 99], [74, 101, 114, 110, 111, 118, 111, 121], [68, 97, 108, 101], [69, 114, 117, 115, 97, 108, 105, 109, 99, 104, 105, 107], [68, 101, 109, 105, 100, 111, 102, 102], [72, 111, 108, 121, 97, 118, 105, 110], [75, 108, 101, 105, 110], [67, 111, 110, 116, 105], [77, 99, 97, 114, 116, 104, 117, 114], [75, 111, 108, 98, 101], [84, 115, 121, 117, 114, 107, 111], [65, 116, 105, 121, 101, 104], [66, 111, 116, 114, 111, 115], [66, 101, 114, 101, 122, 111, 118, 105, 107, 111, 118], [67, 97, 109, 112, 111, 115], [86, 97, 108, 98, 101, 114, 104], [74, 105, 103, 117, 110], [90, 104, 101, 100, 114, 105, 110, 115, 107, 121], [71, 111, 108, 111, 118, 97, 99, 104, 101, 118], [75, 97, 108, 97, 99, 104, 105, 104, 105, 110], [83, 104, 97, 107, 104, 111, 114, 105, 110], [66, 114, 105, 110, 107, 101, 114, 104, 111, 102, 102], [83, 99, 104, 117, 99, 104, 97, 114, 100, 116], [65, 115, 102, 111, 117, 114], [67, 104, 105, 122, 104, 111, 118], [74, 97, 104, 97, 101, 118], [67, 117, 110, 110, 105, 110, 103, 104, 97, 109], [66, 97, 117, 109], [86, 105, 115, 108, 111, 103, 117, 122, 111, 118], [72, 97, 103, 117, 114, 111, 118], [70, 111, 115, 116, 101, 114], [65, 108, 101, 120, 97, 110, 100, 101, 114], [80, 111, 104, 105, 116, 111, 110, 111, 118], [84, 115, 117, 110, 111, 100, 97], [83, 104, 97, 105, 100, 117, 108, 108, 105, 110], [66, 97, 107, 104, 111, 118, 107, 105, 110], [67, 111, 108, 101], [83, 104, 105, 114, 105, 110, 121, 97, 110, 116, 115], [77, 105, 108, 108, 97, 114], [78, 97, 105, 102, 101, 104], [78, 111, 114, 116, 104], [83, 122, 119, 101, 100, 97], [83, 97, 98, 111, 108], [77, 97, 114, 116, 105, 110, 101, 122], [84, 111, 100, 111, 114, 115, 107, 121], [86, 105, 110, 101, 115], [90, 105, 109, 97, 114, 105, 110], [84, 122, 101, 98, 114, 105, 107, 111, 118], [71, 97, 106, 111, 115], [76, 97, 110, 103, 98, 114, 111, 101, 107], [77, 97, 114, 116, 115, 101, 110, 107, 111, 118], [66, 97, 105, 114, 97, 109, 107, 117, 108, 111, 118], [80, 97, 116, 122, 101, 118], [65, 108, 108, 105, 111, 116, 116], [68, 97, 108, 97, 99, 104], [85, 115, 116, 121, 117, 106, 97, 110, 105, 110], [71, 111, 114, 105], [65, 116, 39, 75, 111, 118], [65, 98, 100, 114, 97, 107, 104, 109, 97, 110, 111, 118], [65, 118, 103, 101, 114, 105, 110, 111, 115], [67, 114, 97, 98, 116, 114, 101, 101], [83, 104, 97, 108, 110, 105, 107, 111, 118], [86, 105, 108, 121, 97, 109, 111, 118, 115, 107, 121], [75, 97, 115, 115, 97, 98], [77, 117, 122, 105, 112, 111, 118], [83, 99, 104, 114, 111, 116, 101, 114], [80, 108, 111, 116, 110, 105, 99, 107, 121], [86, 97, 108, 101, 110, 116, 105, 110, 111, 118, 105, 99, 104], [67, 104, 97, 109], [78, 111, 121, 101, 115], [65, 98, 100, 117, 108, 97, 122, 105, 122, 111, 118], [72, 97, 110, 105, 107, 97], [78, 97, 105, 116, 111], [89, 97, 110, 97, 108, 111, 118], [77, 111, 107, 104, 111, 115, 111, 101, 118], [84, 117, 109, 97], [82, 111, 109, 101, 105, 106, 110], [84, 111, 32, 84, 104, 101, 32, 70, 105, 114, 115, 116, 32, 80, 97, 103, 101], [72, 111, 109, 121, 97, 107, 111, 118], [81, 117, 114, 97, 105, 115, 104, 105], [75, 97, 104, 108, 101, 114], [71, 105, 114, 97, 114, 100], [82, 111, 116, 105, 110], [84, 122, 97, 114, 97, 107, 111, 118], [69, 97, 114, 108, 101, 121], [65, 115, 116, 114, 97, 104, 97, 110, 107, 105, 110], [65, 109, 97, 110, 116, 101, 97], [80, 105, 115, 107, 111, 112, 112, 101, 108], [77, 105, 110, 100, 108, 105, 110], [74, 101, 115, 116, 111, 118, 115, 107, 121], [86, 121, 103, 117, 122, 111, 118], [68, 111, 110, 110, 97, 99, 104, 105, 101], [65, 108, 108, 101, 103, 114, 105], [82, 97], [83, 104, 97, 104, 78, 97, 122, 97, 114, 111, 102, 102], [75, 97, 121], [71, 117, 97, 110], [84, 115, 118, 105, 103, 117, 110], [80, 111, 114, 101, 116], [74, 97, 110, 98, 97, 114, 105, 115, 111, 118], [74, 100, 97, 110, 107, 105, 110], [83, 108, 101, 105, 109, 97, 110], [66, 105, 110, 100, 121, 117, 107, 111, 118], [65, 119, 101, 114, 98, 97, 107, 104]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-7b4ec669b809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Train cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Testing cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-4e0e2c8670a7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"type make_variable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# print(\"len \", batch_size, hidden_size, N_countries, N_layers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcountries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-91-0021b171d1a1>\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(words, countries)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ascii word type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_asc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_asc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ascii word value\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwords_asc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mmax_len_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_asc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# torch.max works in pytorch to change words_asc to tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_len_word value\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_asc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ah7s-6iA706",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class net(nn.Module):\n",
        "  # model = net(N_chars, Hidden_size, N_countries, N_layers)\n",
        "  def __init__(self, batch_size, hidden_size, N_countries, N_layers=1):\n",
        "    super(net, self).__init__()\n",
        "    # print(\"type\", type(batch_size), type(hidden_size), type(N_countries), type(N_layers))\n",
        "    # print(\"value are\", batch_size, hidden_size, N_countries, N_layers)\n",
        "\n",
        "    self.vocab_size = batch_size #(it is no of unique words now its a whole batch)\n",
        "    Embessing_dim = hidden_size # how many input hudden layer can take for each word.\n",
        "    #lenth of each word ie after padding  no (its acharacter)\n",
        "    self.num_layer = N_layers\n",
        "    self.num_bidirection =1\n",
        "     self.embed = torch.nn.Embedding(batch_size , hidden_size ) # for each word we will get many output = (eq = input to hidden) \n",
        " \n",
        "    self.grucell = nn.GRU(input_size=hidden_size, hidden_size= hidden_size, num_layers= N_layers )\n",
        " \n",
        "    self.fc = nn.Linear(hidden_size, N_countries)\n",
        " \n",
        "  def forward(self, word_packed):\n",
        "    print(\"word_packed type\", type(word_packed))\n",
        "    self.input = word_packed #B*S to S*B\n",
        "    self.input = input.t() # #B*S to S*B\n",
        "\n",
        "    #S*B ==> S *B *E\n",
        "    print(\"\\nTarget\", word_packed.shape, type(word_packed), word_packed)\n",
        "    input = self.embed(self.input) \\\n",
        "\n",
        "    print(\"\\nTarget\", output.shape, type(output), output)\n",
        "\n",
        "    hidden_0 = hidden_init(batch_size)\n",
        "\n",
        "    output, h_n = self.grucell(input, hidden_0)\n",
        "    print(\"output shape\", output.shape, \"h_n shape\", h_n.shape)\n",
        "    print(\"output type\", type(output), \"h_n type\", type(h_n))\n",
        "\n",
        "    logit = self.fc(h_n)\n",
        "\n",
        "    return logit\n",
        "\n",
        "  def hiddden_init(self, batch_size):\n",
        "    h_0 = torch.zeros(self.num_layer*self.num_bidirection, batch_size, self.hidden_size)\n",
        "    return h_0\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7m7E_KmFjUG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6bf16203-ba96-4830-f45b-9e36eb151fb1"
      },
      "source": [
        "type(len(N_countries))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9tn9StnzxD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "  # for epoch in rancge(N_epoch):\n",
        "  total_loss = 0\n",
        "  for id, (words, countries) in enumerate(train_loader):\n",
        "      # make the word proper format ie: words, countryuy to  nos then padding then tensor. \n",
        "    print(\"type make_variable\", type(words), type(countries))\n",
        "    # print(\"len \", batch_size, hidden_size, N_countries, N_layers)\n",
        "    words, target = make_variable(words, countries)\n",
        "\n",
        "\n",
        "    output = model(words)\n",
        "      \n",
        "    loss = criterion(output, target)\n",
        "    total_loss += loss.item()\n",
        "    model.zeros_grad() # learnig we are making gradindier parameter of model not loss \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i % 10 == 0:\n",
        "          print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.2f}'.format(\n",
        "              time_since(start), epoch,  i *\n",
        "              len(names), len(train_loader.dataset),\n",
        "              100. * i * len(names) / len(train_loader.dataset),\n",
        "              total_loss / i * len(names)))\n",
        "          \n",
        "    return total_loss     "
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76IibzwSU-Eu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "  loss_value = 0\n",
        "  for id, (words, countries) in enumerate(test_loader):\n",
        "    words, target = make_variable(words, countries)\n",
        "\n",
        "    pred = model(words)\n",
        "    pred_country = pred.max(1) # we got cuntry id now\n",
        "\n",
        "    # find loss\n",
        "\n",
        "    if (pred_country == target):\n",
        "      loss_value = loss_value\n",
        "    else:\n",
        "      loss_value = loss_value +1\n",
        "\n",
        "  return loss_value"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C114-pbMtdpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eETP6bqmXu4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pred(word):\n",
        "  output = model(word)\n",
        "  country = train_database.get_country_by_id(output.max(1))\n",
        "return country"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veyLI6eEu3sd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #test loader\n",
        "# database_test = Name('/content/drive/My Drive/AI/basic/data/names_train/names_test.csv')\n",
        "# test_loader = DataLoader(dataset=database_test, batch_size= BS, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iowTb3mpu4Cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB7yA-tEOwqL",
        "colab_type": "text"
      },
      "source": [
        "working on asc and tranposzing .t() is it tup[le , tensor or what then next action\n",
        "1. remove asc use direct embedding\n",
        "2. use asc only noo emabeddinbg as asx itself is embedding\n",
        "by my case ASC is streamline word length as well. so try to keep asx alive for time being untill batch size is 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VwFV7Z_eWCF",
        "colab_type": "text"
      },
      "source": [
        "task3: model: embedding, rnn, linear layer \n",
        "task4: optimizer\n",
        "task5: training\n",
        "taask6: test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqEmGV5GkBD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class net(torch.nn.Module): #learning we can get fuction created inside database (it can be train or test)\n",
        "#   def __init__(self):\n",
        "#     super(net,self).__init__()\n",
        "#     self.word_len = 18 # seq chenged from 15 to 18 for test\n",
        "#     self.vocab_size = self.word_len # no of character per word\n",
        "#     self.embedding_dim = 6 # how many diamention it wants\n",
        "#     self.input_size = self.embedding_dim #6\n",
        "#     self.hiddenO = self.input_size #6 dim\n",
        "#     self.hidden_size = self.embedding_dim #6\n",
        "#     self.batch = 1\n",
        "#     self.num_layer =1\n",
        "#     self.num_class = 18 #num of contries\n",
        "\n",
        "#     #embedding  @learning use tensor conversion al last meand at GRU ot RNN time not beofore that as arugment of other function (like embed) are not tensors\n",
        "#     self.embed = nn.Embedding(num_embeddings = self.vocab_size, embedding_dim = self.embedding_dim) # num_enn = input size word lenth and embedd is output size ie hidden size\n",
        "#     #gru    \n",
        "#     self.grucell = nn.GRU(input_size = self.input_size, hidden_size = self.hidden_size, num_layers =self.num_layer, batch_first = True)\n",
        "#     #fc\n",
        "#     self.fc = nn.Linear(in_features= self.embedding_dim, out_features= self.num_class)\n",
        "  \n",
        "#   def forward(self, word, country): # 10 word and 10 country\n",
        "#     # word, contry = data\n",
        "#     print(\"word\", word) # word type is tuple of B*S\n",
        "#     #converting work in to integer ASCII for input and outpuut ord # learning asc is not required as embedding can take words directly\n",
        "     \n",
        "#     word_ascii, w_len = self.asc(word, self.word_len) #in: BS * word and out: BS * ASCII with padding\n",
        "#     print(\"\\n word_ascii\", word_ascii,  \"\\n w_len\", w_len)\n",
        "#       # challenge we need some padding. keep every word as 18 character long.\n",
        "#       #learning embedding needs word(s) * BS  ir S*B not B*S\n",
        "      \n",
        "#     word_ascii = list(zip(*word_ascii)) # in B*S ot S*B\n",
        "#     # print(\"word ascii \", word_ascii)\n",
        "#     word_ascii = torch.LongTensor(word_ascii)\n",
        "\n",
        "#     #@ learning so word equalization you shoiuld do after embedfding  not before that. otherwise embedding will not ass anything\n",
        "\n",
        "#     word_embed = self.embed(word_ascii) #embedding e extra in matrix out : BS* word_len*E\n",
        "#       #rnn in and out rnn_in BS*Seq*inu_D(e) and rnn_out BS*Seq*hiddenO\n",
        "#       # input = B*Seq_len*input, hidden = num_lauyer* batch,hidden_size\n",
        "#     input = word_embed # B*S*I\n",
        "#     hidden = self.hidden_init()\n",
        "#     out, h_n = self.grucell(input, hidden) #out B*S*hidden [1 10 6] , h_n = num_layerxBxHidden [1 1 6]\n",
        "\n",
        "#     #FC linear layer in Seq to out no of classor contry(-1, nod of class or country)\n",
        "#     out = self.fc(h_n) #in [1 1 6] out [1 1 18]\n",
        "#     out = out.view(-1, self.num_class)\n",
        "#     # out - out.t()\n",
        "#     country_ids = self.country2tensor(country) # 18 contoury with each 15 word\n",
        "#     # country = country.t()#view(-1)\n",
        "#     # print(\"country ids\", country_ids.shape)\n",
        "#     # country = country.squeeze_()\n",
        "#     # out = out.squeeze_()\n",
        "#     return out, country_ids\n",
        "\n",
        "#     #output output of DC \n",
        "#   def hidden_init(self):\n",
        "#     hid = torch.zeros(self.num_layer, self.batch, self.hiddenO)\n",
        "#     return hid\n",
        "#   def country2tensor(self, countries):\n",
        "#     # print(\"contries size\", len(countries))\n",
        "#     country_ids = [database.get_id_by_country(each_country) for each_country in countries]\n",
        "#     # print(\"contries ids\", len(country_ids))\n",
        "#     return torch.LongTensor(country_ids)\n",
        "\n",
        "#   def asc(self, words, word_len):\n",
        "#     word_len = word_len\n",
        "#     sorks3 =  []\n",
        "#     # sorks3 = torch.zeros[len(words)]\n",
        "#     for word in words:\n",
        "#       # # for cha in word:\n",
        "#       # sorks = [ord(cha) for cha in word]\n",
        "#       # sorks2 = [sorks + [0]*(word_len-len(sorks))] \n",
        "#       # sorks3 = Variable(torch.LongTensor([sorks2]))\n",
        "#       # sorks4 = sorks4.append(sorks3)\n",
        "      \n",
        "#       # sorks3.append(Variable(torch.LongTensor([[ord(cha) for cha in word] + [0]*(word_len-len([ord(cha) for cha in word]))])))\n",
        "#       sorks3.append([[ord(cha) for cha in word] + [0]*(word_len-len([ord(cha) for cha in word]))])\n",
        "#       # i = i+1\n",
        "#     return sorks3, len(sorks3) \n",
        "\n",
        "\n",
        "# model = net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMdjdhGMs1FQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class net(torch.nn.Module): #learning we can get fuction created inside database (it can be train or test)\n",
        "#   def __init__(self):\n",
        "#     super(net,self).__init__()\n",
        "#     self.word_len = 18 # seq chenged from 15 to 18 for test\n",
        "#     self.vocab_size = self.word_len # no of character per word\n",
        "#     self.embedding_dim = 6 # how many diamention it wants\n",
        "#     self.input_size = self.embedding_dim #6\n",
        "#     self.hiddenO = self.input_size #6 dim\n",
        "#     self.hidden_size = self.embedding_dim #6\n",
        "#     self.batch = 1\n",
        "#     self.num_layer =1\n",
        "#     self.num_class = 18 #num of contries\n",
        "\n",
        "#     #embedding  @learning use tensor conversion al last meand at GRU ot RNN time not beofore that as arugment of other function (like embed) are not tensors\n",
        "#     self.embed = nn.Embedding(num_embeddings = self.vocab_size, embedding_dim = self.embedding_dim) # num_enn = input size word lenth and embedd is output size ie hidden size\n",
        "#     #gru    \n",
        "#     self.grucell = nn.GRU(input_size = self.input_size, hidden_size = self.hidden_size, num_layers =self.num_layer, batch_first = True)\n",
        "#     #fc\n",
        "#     self.fc = nn.Linear(in_features= self.embedding_dim, out_features= self.num_class)\n",
        "  \n",
        "#   def forward(self, word, country): # 10 word and 10 country\n",
        "#     # word, contry = data\n",
        "#     print(\"word\", word) # word type is tuple of B*S\n",
        "\n",
        "#     # challenge we need some padding. keep every word as 18 character long.\n",
        "#     #learning embedding needs word(s) * BS  ir S*B not B*S\n",
        "      \n",
        "#     word_ascii = list(zip(*word_ascii)) # in B*S ot S*B\n",
        "#     # print(\"word ascii \", word_ascii)\n",
        "#     word_ascii = torch.LongTensor(word_ascii)\n",
        "\n",
        "#     #@ learning so word equalization you shoiuld do after embedfding  not before that. otherwise embedding will not ass anything\n",
        "    \n",
        "#     word_embed = self.embed(word_ascii) #embedding e extra in matrix out : BS* word_len*E\n",
        "#       #rnn in and out rnn_in BS*Seq*inu_D(e) and rnn_out BS*Seq*hiddenO\n",
        "#       # input = B*Seq_len*input, hidden = num_lauyer* batch,hidden_size\n",
        "#     input = word_embed # B*S*I\n",
        "#     hidden = self.hidden_init()\n",
        "#     out, h_n = self.grucell(input, hidden) #out B*S*hidden [1 10 6] , h_n = num_layerxBxHidden [1 1 6]\n",
        "\n",
        "#     #FC linear layer in Seq to out no of classor contry(-1, nod of class or country)\n",
        "#     out = self.fc(h_n) #in [1 1 6] out [1 1 18]\n",
        "#     out = out.view(-1, self.num_class)\n",
        "#     # out - out.t()\n",
        "#     country_ids = self.country2tensor(country) # 18 contoury with each 15 word\n",
        "#     # country = country.t()#view(-1)\n",
        "#     # print(\"country ids\", country_ids.shape)\n",
        "#     # country = country.squeeze_()\n",
        "#     # out = out.squeeze_()\n",
        "#     return out, country_ids\n",
        "\n",
        "#     #output output of DC \n",
        "#   def hidden_init(self):\n",
        "#     hid = torch.zeros(self.num_layer, self.batch, self.hiddenO)\n",
        "#     return hid\n",
        "#   def country2tensor(self, countries):\n",
        "#     # print(\"contries size\", len(countries))\n",
        "#     country_ids = [database.get_id_by_country(each_country) for each_country in countries]\n",
        "#     # print(\"contries ids\", len(country_ids))\n",
        "#     return torch.LongTensor(country_ids)\n",
        "\n",
        "#   def asc(self, words, word_len):\n",
        "#     word_len = word_len\n",
        "#     sorks3 =  []\n",
        "#     # sorks3 = torch.zeros[len(words)]\n",
        "#     for word in words:\n",
        "#       # # for cha in word:\n",
        "#       # sorks = [ord(cha) for cha in word]\n",
        "#       # sorks2 = [sorks + [0]*(word_len-len(sorks))] \n",
        "#       # sorks3 = Variable(torch.LongTensor([sorks2]))\n",
        "#       # sorks4 = sorks4.append(sorks3)\n",
        "      \n",
        "#       # sorks3.append(Variable(torch.LongTensor([[ord(cha) for cha in word] + [0]*(word_len-len([ord(cha) for cha in word]))])))\n",
        "#       sorks3.append([[ord(cha) for cha in word] + [0]*(word_len-len([ord(cha) for cha in word]))])\n",
        "#       # i = i+1\n",
        "#     return sorks3, len(sorks3) \n",
        "\n",
        "# model = net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlsCGYjf5XtB",
        "colab_type": "text"
      },
      "source": [
        "Process contry differently as each contry will have unique no (one number not as matrix as word (N * no of class) will be comparing or predicting the perticular class. so its a one idea to keep perticular class as one number rather than keeping it as one class as array.\n",
        "\n",
        "TODO each contry as one unique no.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bkGf6xpwy60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = .0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdzgP6gTxXdp",
        "colab_type": "text"
      },
      "source": [
        "Trainig\n",
        "\n",
        "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8YXuz0AwyyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for epoch in range(1):\n",
        "#   total_loss = 0 \n",
        "#   # batch_id = 0 \n",
        "#   for batch_idx, (word, country) in enumerate(train_loader): #@ learning loader is sending tuple on each count\n",
        "#     print(\"word database\", type(word), \"countey database\", type(country))\n",
        "#     out, country = model(word, country)\n",
        "#     print(\"out shape\", out.shape, \"countey shape\", country.shape)\n",
        "#     optimizer.zero_grad()\n",
        "#     loss = Criterion(out, country)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     if batch_idx % 500  == 0:\n",
        "#         print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "#                 epoch, batch_idx * len(word), len(train_loader.dataset),\n",
        "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhLo9HN7AiZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n= [\"name\"]\n",
        "# # n[1]\n",
        "# # ord(n[1])\n",
        "# print(asc(n,10)[0].size())\n",
        "# a, b = asc(n,10)\n",
        "# embed=nn.Embedding(200,3)\n",
        "# emb2 = embed(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOS2GPINm6EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(emb2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoqHLwaEAqmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # sorks= []\n",
        "  # i=0\n",
        "  # def asc(words, word_len):\n",
        "  #   word_len = word_len\n",
        "\n",
        "  #   for word in words:\n",
        "  #     # for cha in word:\n",
        "  #     sorks = [ord(cha) for cha in word]\n",
        "  #     sorks2 = sorks + [0]*(word_len-len(sorks)) \n",
        "  #     sorks3 = Variable(torch.LongTensor([sorks2]))\n",
        "  #   return sorks3, len(sorks2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e02g7x7BWQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIyELK02B22g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n1 =asc(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzTcqLLZTcoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n2 = concat(n1, zeros[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqvdvxq4Th8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# like_zero[2]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}