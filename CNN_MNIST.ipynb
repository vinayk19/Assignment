{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_MNIST.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOtY0pPPvhtGGamG7IUpNXj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayk19/Assignment/blob/master/CNN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAWVKFs2mRnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH00x6CHm6f-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = datasets.MNIST(root = \"MNIST/processed/training.pt\", train = True, transform=transforms.ToTensor(), download= True)\n",
        "test_data = datasets.MNIST(root = \"MNIST/processed/test.pt\", train = False, transform=transforms.ToTensor(), download = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxYpROwKn-zI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(dataset = train_data, batch_size= 64, shuffle= True)\n",
        "test_loader = DataLoader(dataset = test_data, batch_size= 64, shuffle= False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ougSTN8FtcIX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3854ef42-5d86-4190-9138-80c279f814e7"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"device is \", device)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device is  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTbSITbtpJcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class mnist(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(mnist, self).__init__()\n",
        "    self.conv1 = torch.nn.Conv2d(1, 10 , kernel_size= 5, stride= 1, padding= 0)\n",
        "    self.conv2 = torch.nn.Conv2d(10, 20, kernel_size= 5, stride= 1, padding= 0)\n",
        "    self.maxpool = torch.nn.MaxPool2d(kernel_size=2)\n",
        "    self.linear = torch.nn.Linear(320,10) #how 320 comes up\n",
        "\n",
        "  def forward(self, data):\n",
        "    in_size = data.size(0)\n",
        "    x = F.relu(self.maxpool(self.conv1(data)))\n",
        "    x = F.relu(self.maxpool(self.conv2(x)))  \n",
        "    x = x.view(in_size, -1) \n",
        "    x = self.linear(x)\n",
        "    out = F.log_softmax(x)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBZ_p5G68QVA",
        "colab_type": "text"
      },
      "source": [
        "needs to undedrstand mistake below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPKU1Sx312AC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class mnist(torch.nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(mnist, self).__init__()\n",
        "#     self.conv1 = torch.nn.Conv2d(1, 10 , kernel_size= 5, stride= 1, padding= 0)\n",
        "#     self.conv2 = torch.nn.Conv2d(10, 20, kernel_size= 5, stride= 1, padding= 0) ()\n",
        "#     self.maxpool = torch.nn.MaxPool2d(kernel_size=2)\n",
        "#     self.linear = torch.nn.Linear(320,10) #how 320 comes up\n",
        "\n",
        "#   def forward(self, data):\n",
        "#     in_size = data.size(0)\n",
        "#     x = torch.nn.ReLU(self.maxpool(self.conv1(data))) # this relu is not givenng tensort output\n",
        "#     x = torch.nn.ReLU(self.maxpool(self.conv2(x)))  \n",
        "#     x = torch.view(in_size, -1)  # x.view\n",
        "#     x = self.linear(x)\n",
        "#     out = torch.nn.functional.log_softmax(x)\n",
        "#     return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsgsZp7Zsayn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "e4a00afe-101b-4fac-e39c-8c9a59583ce3"
      },
      "source": [
        "model = mnist()\n",
        "model.to(device)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "mnist(\n",
              "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (linear): Linear(in_features=320, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRRUONdxtmob",
        "colab_type": "text"
      },
      "source": [
        "Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC3BMRJftj1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=.01, momentum=.4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBPKydJiuZXn",
        "colab_type": "text"
      },
      "source": [
        "trainig"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRL0jyZauYWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def train(epoch):\n",
        "#   model.train()\n",
        "#   for Epo in range(epoch):\n",
        "#     for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "#       images, labels = images.to(device), labels.to(device)\n",
        "#       targets = model(images)\n",
        "#       optimizer.zero_grad()\n",
        "#       loss = criterian(targets, labels)\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "#       if batch_idx % 500  == 0:\n",
        "#         # print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}% | Loss: {:.6f}', format(\n",
        "#         #     epoch, batch_idx*len(data), len(train_loader.dataset), 100. *batch_idx / len(train_loader), loss.item()\n",
        "#         # ))\n",
        "#         print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZXOuJgA09iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for Epoch in range(5):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      logit = model(data)\n",
        "      loss = criterion(logit, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if batch_idx % 500  == 0:\n",
        "        # print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}% | Loss: {:.6f}', format(\n",
        "        #     epoch, batch_idx*len(data), len(train_loader.dataset), 100. *batch_idx / len(train_loader), loss.item()\n",
        "        # ))\n",
        "        print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX_Te0uOtYdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "  model.eval()  # what is meaning of it ?\n",
        "  with torch.no_grad(): # its not compulsary but it will save some merory\n",
        "    test_loss =0\n",
        "    correct = 0\n",
        "    for data, target in test_loader: # enumeration is not required as this is a single batch\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)  # output.shape = 64,10\n",
        "      #sum up the match loss\n",
        "      test_loss += criterion(output, target).item()\n",
        "      #get the index of the max value\n",
        "      pred = output.data.max(1, keepdim=True)[1]  #output.data is ouput data/value ? what is max function https://www.journaldev.com/39463/pytorch-torch-max\n",
        "      #1 is axis: 0 is row (ie colum wise max:> 1, 10, and 1 is colum ie row wise max)\n",
        "      # provides max data and and its index\n",
        "      correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "            f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "    # print(f'====================\\n test set : Average loss: {test_loss: .4f}, Accuracy: {correct}/{len(test_loader.dataset)} ' \n",
        "            # f'({100. * correct/len(test_loader.dataset):. 0f}%)')\n",
        "            \n",
        "  #         ===========================\n",
        "  # Test set: Average loss: 0.0016, Accuracy: 9787/10000 (98%)\n",
        "  # Testing timr: 0m 25s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gpXOtmQx8lr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "b2789c14-c9e7-48cb-dec6-3df0e0b9380f"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  since = time.time()\n",
        "  for epoch in range(1,2):\n",
        "    epoch_start = time.time()\n",
        "    train(epoch)\n",
        "    m, s = divmod(time.time() - epoch_start, 60)\n",
        "    print(f'Training timr: {m:.0f}m {s:.0f}s')\n",
        "    test()\n",
        "    m, s = divmod(time.time() - epoch_start, 60)\n",
        "    print(f'Testing timr: {m:.0f}m {s:.0f}s')\n",
        "  m, s = divmod(time.time() - since, 60)\n",
        "  print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.304240\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 0.240961\n",
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 0.229553\n",
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 0.073775\n",
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 0.169380\n",
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 0.051236\n",
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 0.075967\n",
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 0.181949\n",
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 0.182886\n",
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 0.065528\n",
            "Training timr: 0m 24s\n",
            "===========================\n",
            "Test set: Average loss: 0.0011, Accuracy: 9789/10000 (98%)\n",
            "Testing timr: 0m 25s\n",
            "Total Time: 0m 25s\n",
            "Model was trained on cuda!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}