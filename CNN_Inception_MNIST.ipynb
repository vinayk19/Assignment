{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN Inception_MNIST.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMorQ2Umv2Vl/hoPsBuNK/2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayk19/Assignment/blob/master/CNN_Inception_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAWVKFs2mRnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH00x6CHm6f-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = datasets.MNIST(root = \"MNIST/processed/training.pt\", train = True, transform=transforms.ToTensor(), download= True)\n",
        "test_data = datasets.MNIST(root = \"MNIST/processed/test.pt\", train = False, transform=transforms.ToTensor(), download = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxYpROwKn-zI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(dataset = train_data, batch_size= 64, shuffle= True)\n",
        "test_loader = DataLoader(dataset = test_data, batch_size= 64, shuffle= False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ougSTN8FtcIX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "905ca1c2-1591-47bd-d016-7d66627f88a9"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"device is \", device)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device is  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TroLtBT8GDnl",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKUjNFhRqCy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class inception(torch.nn.Module):\n",
        "#   def __init__(self, in_channel):\n",
        "#     super(inception, self).__init__()\n",
        "\n",
        "#     self.in_channel = in_channel\n",
        "\n",
        "#     self.b1x1 = torch.nn.Conv2d(in_channels= self.in_channel, out_channels=16, kernel_size=1 )\n",
        "\n",
        "#     self.b241x1 = torch.nn.Conv2d(in_channels=self.in_channel, out_channels= 24, kernel_size=1 )\n",
        "\n",
        "#     self.b5x5_1 = torch.nn.Conv2d(in_channels=self.in_channel, out_channels=16, kernel_size=1 )\n",
        "#     self.b5x5_2 = torch.nn.Conv2d(in_channels=16, out_channels=24, kernel_size=5, padding =2 )\n",
        "\n",
        "#     self.b3x3_1 = torch.nn.Conv2d(in_channels=self.in_channel, out_channels=16, kernel_size=1 )\n",
        "#     self.b3x3_2 = torch.nn.Conv2d(in_channels=16, out_channels=24, kernel_size=3 )\n",
        "#     self.b3x3_3 = torch.nn.Conv2d(in_channels=24, out_channels=24, kernel_size=3 )\n",
        "    \n",
        "#     def forward(self, image):\n",
        "#       in_size = image.size(0)# it will give input shape\n",
        "\n",
        "#       branch1x1 = self.b1x1(image)\n",
        "\n",
        "#       branch_pool = F.avg_pool2d(image, kernel_size=3, stride=1, padding=1)\n",
        "#       branch241x1 = self.b241x1(branch_pool)\n",
        "\n",
        "#       branch5x5 = self.b5x5_2(self.b5x5_1(image))\n",
        "\n",
        "#       branch3x3 = self.b3x3_3(self.b3x3_2(self.b3x3_1(image)))\n",
        "\n",
        "#       output = [branch1x1, branch5x5, branch3x3, branch241x1] #what is sape of each\n",
        "\n",
        "#       return torch.cat(output, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iqo_cA3bRWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InceptionA(torch.nn.Module): #https://cs231n.github.io/convolutional-networks/\n",
        "    def __init__(self, in_channel):\n",
        "      super(InceptionA, self).__init__()\n",
        "\n",
        "      self.in_channel = in_channel\n",
        "\n",
        "      self.b1x1 = torch.nn.Conv2d(in_channels= self.in_channel, out_channels=16, kernel_size=1 )\n",
        "\n",
        "      self.b241x1 = torch.nn.Conv2d(in_channels=self.in_channel, out_channels= 24, kernel_size=1 )\n",
        "\n",
        "      self.b5x5_1 = torch.nn.Conv2d(in_channels=self.in_channel, out_channels=16, kernel_size=1 )\n",
        "      self.b5x5_2 = torch.nn.Conv2d(in_channels=16, out_channels=24, kernel_size=5, padding =2 )\n",
        "\n",
        "      self.b3x3_1 = torch.nn.Conv2d(in_channels=self.in_channel, out_channels=16, kernel_size=1 )\n",
        "      self.b3x3_2 = torch.nn.Conv2d(in_channels=16, out_channels=24, kernel_size=3, padding = 1 )\n",
        "      self.b3x3_3 = torch.nn.Conv2d(in_channels=24, out_channels=24, kernel_size=3, padding = 1 )\n",
        "    \n",
        "    def forward(self, image): # IMAGE [64, 10, 12, 12] [64, 20, 4, 4]) &  BS, channel, h w\n",
        "      in_size = image.size(0)# it will give input shape\n",
        "      # print(\"image detal\", type(image), image.shape )\n",
        "\n",
        "      branch1x1 = self.b1x1(image) # out 64 16 12 12 (W2=(W1−F+2P)/S)+1\n",
        "      # print(\"1x1 details\", type(branch1x1), branch1x1.shape )\n",
        "\n",
        "      branch_pool = F.avg_pool2d(image, kernel_size=3, stride=1, padding=1) # 64 10 (W1−F)/S+1=12 12 how 12?  \n",
        "      # print(\"brach pool 24 details\", type(branch_pool), branch_pool.shape )\n",
        "      branch241x1 = self.b241x1(branch_pool)  #out 64 24 12 12\n",
        "      # print(\"brach24 details\", type(branch241x1), branch241x1.shape )\n",
        "\n",
        "      branch5x5 = self.b5x5_2(self.b5x5_1(image)) #out 64 16 12 12 :> 64 24 W2=(W1−F+2P)/S+1=(12-5+4) +1=12 12 \n",
        "      # print(\"5x5 details\", type(branch5x5), branch5x5.shape )\n",
        "\n",
        "      branch3x3 = self.b3x3_3(self.b3x3_2(self.b3x3_1(image))) #1 64 16 12 12 :> 64 24 12 12 : 64 24 12 12 \n",
        "      # print(\"3x3 details\", type(branch3x3), branch3x3.shape )\n",
        "\n",
        "      output = [branch1x1, branch5x5, branch3x3, branch241x1] # out [bs=64 chanell=[16+24+24+24]=88 12 12] 2nd 64 88 4 4 \n",
        "      return torch.cat(output, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZNdhLbhzjff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1,10,kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(88,20,kernel_size=5 ) #why 88 becase 88 is out from incept 1\n",
        "\n",
        "    self.incept1 = InceptionA(in_channel=10)\n",
        "    self.incept2 = InceptionA(in_channel=20)\n",
        "\n",
        "    self.mp = nn.MaxPool2d(2)\n",
        "    self.fc = nn.Linear(1408,10) # 64 88 4 4 : 88*4*4 =1408\n",
        "\n",
        "  def forward (self,x): #image 64 1 28 28\n",
        "    in_size = x.size(0) # 64\n",
        "    x = F.relu(self.mp(self.conv1(x))) # mp(64 10 24 24) : > 64 10 12 12\n",
        "    x = self.incept1(x) # input 64 10 12 12 & out 64 88 12 12\n",
        "    # print(\"x incept1 detail\", type(x), x.shape)\n",
        "    x = F.relu(self.mp(self.conv2(x))) # mp (64 20 12 12):> 64 20 6 6 :> 64 20 4 4 ?\n",
        "    x = self.incept2(x) # input 64 20 4 4 & out 64 88 4 4\n",
        "    # print(\"x incept2 detail\", type(x), x.shape)\n",
        "    x = x.view(in_size, -1) \n",
        "    x = self.fc(x)\n",
        "    return F.log_softmax(x)\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzD5ib-8G-hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class mnist(torch.nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(mnist,self).__init__()\n",
        "#     self.incepA= inception(input_channel=1)\n",
        "#     self.linear = torch.nn.Linear(24,10)\n",
        "\n",
        "#   def forward(self, image):\n",
        "#     in_size = image.size(0)\n",
        "#     outputA = self.incepA(image)\n",
        "#     outputA = self.linear(outputA)\n",
        "#     outputA = F.relu(outputA)\n",
        "#     outputA = outputA.view(in_size, -1)\n",
        "#     outputA = self.linear(outputA)\n",
        "#     return F.log_softmax(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsgsZp7Zsayn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "05af862d-0a67-4623-dfe4-7ec93c43c497"
      },
      "source": [
        "model = Net()\n",
        "model.to(device)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(88, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (incept1): InceptionA(\n",
              "    (b1x1): Conv2d(10, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (b241x1): Conv2d(10, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (b5x5_1): Conv2d(10, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (b5x5_2): Conv2d(16, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (b3x3_1): Conv2d(10, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (b3x3_2): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (b3x3_3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              "  (incept2): InceptionA(\n",
              "    (b1x1): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (b241x1): Conv2d(20, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (b5x5_1): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (b5x5_2): Conv2d(16, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (b3x3_1): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (b3x3_2): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (b3x3_3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              "  (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc): Linear(in_features=1408, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRRUONdxtmob",
        "colab_type": "text"
      },
      "source": [
        "Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC3BMRJftj1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=.01, momentum=.4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBPKydJiuZXn",
        "colab_type": "text"
      },
      "source": [
        "trainig"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZXOuJgA09iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for Epoch in range(5):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      logit = model(data)\n",
        "      loss = criterion(logit, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if batch_idx % 500  == 0:\n",
        "        # print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}% | Loss: {:.6f}', format(\n",
        "        #     epoch, batch_idx*len(data), len(train_loader.dataset), 100. *batch_idx / len(train_loader), loss.item()\n",
        "        # ))\n",
        "        print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX_Te0uOtYdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "  model.eval()  # what is meaning of it ?\n",
        "  with torch.no_grad(): # its not compulsary but it will save some merory\n",
        "    test_loss =0\n",
        "    correct = 0\n",
        "    for data, target in test_loader: # enumeration is not required as this is a single batch\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)  # output.shape = 64,10\n",
        "      #sum up the match loss\n",
        "      test_loss += criterion(output, target).item()\n",
        "      #get the index of the max value\n",
        "      pred = output.data.max(1, keepdim=True)[1]  #output.data is ouput data/value ? what is max function https://www.journaldev.com/39463/pytorch-torch-max\n",
        "      #1 is axis: 0 is row (ie colum wise max:> 1, 10, and 1 is colum ie row wise max)\n",
        "      # provides max data and and its index\n",
        "      correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "            f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "    # print(f'====================\\n test set : Average loss: {test_loss: .4f}, Accuracy: {correct}/{len(test_loader.dataset)} ' \n",
        "            # f'({100. * correct/len(test_loader.dataset):. 0f}%)')\n",
        "            \n",
        "  #         ===========================\n",
        "  # Test set: Average loss: 0.0016, Accuracy: 9787/10000 (98%)\n",
        "  # Testing timr: 0m 25s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gpXOtmQx8lr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "outputId": "d0f298bf-828e-479c-935b-94d82ea8e0f5"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  since = time.time()\n",
        "  for epoch in range(1,4):\n",
        "    epoch_start = time.time()\n",
        "    train(epoch)\n",
        "    m, s = divmod(time.time() - epoch_start, 60)\n",
        "    print(f'Training timr: {m:.0f}m {s:.0f}s')\n",
        "    test()\n",
        "    m, s = divmod(time.time() - epoch_start, 60)\n",
        "    print(f'Testing timr: {m:.0f}m {s:.0f}s')\n",
        "  m, s = divmod(time.time() - since, 60)\n",
        "  print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.308220\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 0.490040\n",
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 0.178098\n",
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 0.139638\n",
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 0.062934\n",
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 0.097600\n",
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 0.091277\n",
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 0.123872\n",
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 0.036129\n",
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 0.117809\n",
            "Training timr: 1m 19s\n",
            "===========================\n",
            "Test set: Average loss: 0.0010, Accuracy: 9801/10000 (98%)\n",
            "Testing timr: 1m 20s\n",
            "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 0.023398\n",
            "Train Epoch: 2 | Batch Status: 32000/60000 (53%) | Loss: 0.079696\n",
            "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 0.041699\n",
            "Train Epoch: 2 | Batch Status: 32000/60000 (53%) | Loss: 0.059996\n",
            "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 0.079186\n",
            "Train Epoch: 2 | Batch Status: 32000/60000 (53%) | Loss: 0.028310\n",
            "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 0.015606\n",
            "Train Epoch: 2 | Batch Status: 32000/60000 (53%) | Loss: 0.022754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-e519553b2c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training timr: {m:.0f}m {s:.0f}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-90-b8146cfd5720>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mEpoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m       \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2699\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2701\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2634\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_MAPMODES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2636\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m             \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadonly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   2542\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImagePalette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2543\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2544\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}