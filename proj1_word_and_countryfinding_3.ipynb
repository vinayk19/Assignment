{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word and countryfinding_3.ipynb",
      "provenance": [],
      "mount_file_id": "1WLaQFEWZEn5iCXJZq8y6Le2f8LNrRBT4",
      "authorship_tag": "ABX9TyNIcD7ZAWDWw87DB9xotN55",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayk19/Assignment/blob/master/proj1_word_and_countryfinding_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WigN-7SHh93O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2003f2f5-d6a2-4cf3-9e5c-3aa3254dc35b"
      },
      "source": [
        "cd /content/drive/My Drive/AI/basic/data/names_train"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/AI/basic/data/names_train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4jmwmZDk8U6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import csv\n",
        "import gzip\n",
        "\n",
        "# from name_dataset import NameDataset\n",
        "# from torch.nn.utils.rnn import pack_padded_sequence, pad_paked_sequence"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP7rj3DGBMSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# xy1 = pd.read_csv(\"names_train.csv\") #q is it require to convert it(text) into torch\n",
        "# # xy1.shape\n",
        "# # name = (xy1.iloc[0,:][0])\n",
        "# # name\n",
        "# xy1.describe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZQHLEWca7i5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word = xy1['Adsit'].tolist()\n",
        "# country = xy1['Czech'].tolist()\n",
        "# print(len(word))\n",
        "# print(len(country))\n",
        "# print(list(dict.fromkeys(country))) # it will provide only unique country name only"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8UihLr-vsrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # filename = '/content/drive/My Drive/AI/basic/data/names_train/names_train.csv.gz' if is_train_set else '/content/drive/My Drive/AI/basic/data/names_train/names_test.csv.gz'\n",
        "# if is_train_set:\n",
        "#   filename = '/content/drive/My Drive/AI/basic/data/names_train/names_train.csv.gz'\n",
        "# else :\n",
        "#   filename = '/content/drive/My Drive/AI/basic/data/names_train/names_test.csv.gz'\n",
        "\n",
        "# with open(filename, \"rt\") as f: # learning if file is zip with file the gzip.open. if only csv file then only open(filename, \"rt\")\n",
        "#   reader = csv.reader(f)\n",
        "#   rows = list(reader)\n",
        "# print(rows)\n",
        "# naam = [row[0] for row in rows]\n",
        "# country = [row[1] for row in rows]\n",
        "\n",
        "# print(len(naam), naam)\n",
        "# print(len(country), country)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPr7qIASeOAW",
        "colab_type": "text"
      },
      "source": [
        "Tast1 : create database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI8_0khQiL9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Name(Dataset):\n",
        "  def __init__(self, is_train_set = False):\n",
        "  # def __init__(self, csv_file):\n",
        "\n",
        "    if is_train_set:\n",
        "      filename = '/content/drive/My Drive/AI/basic/data/names_train/names_train.csv'\n",
        "    else :\n",
        "      filename = '/content/drive/My Drive/AI/basic/data/names_train/names_test.csv'\n",
        "\n",
        "    with open(filename, \"rt\") as f: # learning if file is zip with file the gzip.open. if only csv file then only open(filename, \"rt\")\n",
        "      reader = csv.reader(f)\n",
        "      rows = list(reader)\n",
        "    # print(rows)\n",
        "    self.word = [row[0] for row in rows]\n",
        "    self.country = [row[1] for row in rows]\n",
        "    # self.xy = pd.read_csv(csv_file)\n",
        "    # self.word = self.xy['Adsit'].tolist()\n",
        "    # self.country = self.xy['Czech'].tolist()\n",
        "    self.country_list = list(dict.fromkeys(self.country))\n",
        "    \n",
        "  def __getitem__(self, id):\n",
        "    return self.word[id], self.country[id]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.word)\n",
        "\n",
        "  def get_country_list(self):\n",
        "    return self.country_list\n",
        "  #get the unique country name by its id or lebel\n",
        "  def get_country_by_id(self, id):\n",
        "    return self.country_list[id] #@learning callable meance function() but in list it is [] (as list is not callable)\n",
        "  \n",
        "  def get_id_by_country(self, country):\n",
        "    return self.country_list.index(country)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE5dHVRWgOye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "3ba18857-af00-4c2d-faef-1d58020bf248"
      },
      "source": [
        "#test database\n",
        "if __name__ == '__main__':\n",
        "  database = Name(True) # fasle for train set and true for test set\n",
        "  print(database.get_country_list())\n",
        "  print(database.get_id_by_country('French'))\n",
        "  print(database.get_country_by_id(7))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Czech', 'German', 'Arabic', 'Japanese', 'Chinese', 'Vietnamese', 'Russian', 'French', 'Irish', 'English', 'Spanish', 'Greek', 'Italian', 'Portuguese', 'Scottish', 'Dutch', 'Korean', 'Polish']\n",
            "7\n",
            "French\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_3SaEO7eTZB",
        "colab_type": "text"
      },
      "source": [
        "tast2: Create dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln00GiUsq3Mg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2af04b29-ecf3-4b7b-b513-44804e7f8f04"
      },
      "source": [
        "# parameter s and loaderts train loader\n",
        "# net(N_chars, Hidden_size, N_countries, N_layers)\n",
        "\n",
        "N_layers = 1\n",
        "N_countries = len(Name(True).get_country_list())\n",
        "hidden_size = 20\n",
        "N_epoch =5\n",
        "N_chars = 128\n",
        "batch_size = 256\n",
        "\n",
        "database_train = Name(True)\n",
        "train_loader = DataLoader(dataset=database_train, batch_size= batch_size, shuffle=True)\n",
        "\n",
        "database_test = Name(False)\n",
        "test_loader = DataLoader(dataset=database_test, batch_size= batch_size, shuffle=True)\n",
        "print(len(database_train), len(database_test))\n",
        "print(database_train[2000],database_test[2000])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13374 6700\n",
            "('Salib', 'Arabic') ('Bakrymov', 'Russian')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6Zsgm90nzpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for val in enumerate(train_loader):\n",
        "#   print(val[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNNbrDogzyE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#common funtions \n",
        "def make_variable(words, countries):#word to asci charected then padding then pack padd\n",
        "  words_asc = []\n",
        "  for word in words:\n",
        "    words_asc = words_asc + [ascii(word)[0]] #1done now to padding (max len of batch) \n",
        "\n",
        "  # print(\"ascii word type\", type(words_asc), len(words_asc)) #ascii word type <class 'list'> 256\n",
        "  # # print(\"ascii word value\",words_asc)\n",
        "  # # learning torch.max works only on tensors not list/tuple no now onwards convert everythings in tensor only\n",
        "  # # ite menans pass the tensors here which are already padded one\n",
        "\n",
        "  max_len_word = max(len(each) for each in words_asc) # torch.max works in pytorch to change words_asc to tensor\n",
        "  # print(\"max_len_word value\",type(max_len_word), max_len_word)  #max_len_word value <class 'int'> 13\n",
        " \n",
        "  # # padding\n",
        "\n",
        "  words_padd = [each + [0]*(max_len_word-len(each)) for each in words_asc]\n",
        "  \n",
        "  # print(\"words_padd type\", type(words_padd), len(words_padd))\n",
        "  # print(\"words_padd value\",words_padd)# till now no tensor \n",
        "  #2 padding done 3 pack paadding\n",
        "  #working # input and lenght should be tensor\n",
        "  words_padd = torch.LongTensor(words_padd)\n",
        "  max_len_word = torch.FloatTensor([max_len_word])\n",
        "\n",
        "  # print(\"\\n 2 words_padd type\", type(words_padd), words_padd.shape) #2 words_padd type <class 'torch.Tensor'> torch.Size([1, 256, 13])\n",
        "  # # print(\"\\n 2 words_padd value\",words_padd)# till now no tensor \n",
        "  # # print(\"\\n 2 MAX_LEN_type\", type(max_len_word), max_len_word)# 2 MAX_LEN_type <class 'torch.Tensor'> tensor([13.])\n",
        "  \n",
        "  # # removing pack_padding from here to after later embedding \n",
        "  # # words_packed = torch.nn.utils.rnn.pack_padded_sequence(words_padd, max_len_word, batch_first=True)\n",
        "  # #qustion as batch_first= true means input as T*B*x: x is diamention, B is Batach_size, T is lenth of longest sequence\n",
        "  # #note T = max_len_ward[0] ??\n",
        "\n",
        "  # # print(\"words_packed type\", type(words_packed), words_packed.__sizeof__()) #words_packed type <class 'torch.nn.utils.rnn.PackedSequence'> 64\n",
        "  # # print(\"words_packed value\",words_packed)# till now no tensor #passed\n",
        "  \n",
        "  Country_ids = [database_train.get_id_by_country(country) for country in countries]\n",
        "  target = torch.LongTensor(Country_ids)\n",
        "  # print(\"\\ncountry_ids\", Country_ids.__sizeof__(), type(Country_ids), Country_ids) #Country_ids,\n",
        "  # print(\"\\nTarget\", target.shape, type(target))# Target torch.Size([256]) <class 'torch.Tensor'>\n",
        "\n",
        "  return words_padd, target # we are returning Words>words_padd seq  and country > \n",
        "\n",
        "def ascii(word):\n",
        "  word_asc = [ord(cha) for cha in word]\n",
        "  # word_asc = [torch.LongTensor(ord(cha)) for cha in word]\n",
        "\n",
        "  # return torch.LongTensor(word_asc), len(word_asc)\n",
        "  return word_asc, len(word_asc)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnalj8X9SRhG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "5478b2fa-1599-45ab-8463-b22451976b26"
      },
      "source": [
        "torch.max(b,1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-89d3aec1f192>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'b' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ah7s-6iA706",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class net(nn.Module):\n",
        "  # model = net(N_chars, Hidden_size, N_countries, N_layers)\n",
        "  def __init__(self, batch_size, hidden_size, N_countries, N_layers=1):\n",
        "    super(net, self).__init__()\n",
        "    # print(\"type\", type(batch_size), type(hidden_size), type(N_countries), type(N_layers))\n",
        "    # print(\"value are\", batch_size, hidden_size, N_countries, N_layers)\n",
        "\n",
        "    self.vocab_size = batch_size #(it is no of unique words now its a whole batch)\n",
        "    Embessing_dim = hidden_size # how many input hudden layer can take for each word.\n",
        "    #lenth of each word ie after padding  no (its acharacter)\n",
        "    self.num_layer = N_layers\n",
        "    self.num_bidirection =1\n",
        "    self.embed = torch.nn.Embedding(batch_size , hidden_size ) # for each word we will get many output = (eq = input to hidden) \n",
        " \n",
        "    self.grucell = nn.GRU(input_size=hidden_size, hidden_size= hidden_size, num_layers= N_layers, batch_first=True )\n",
        " \n",
        "    self.fc = nn.Linear(hidden_size, N_countries)\n",
        " \n",
        "  def forward(self, word_packed):\n",
        "    # print(\"NOW in Model\")\n",
        "    \n",
        "    self.input = word_packed #B*S to S*B\n",
        "    # self.input = self.input.t() # #B*S to S*B\n",
        "    batch_size = len(word_packed) #learning this BS helps with last batch is comming of small size\n",
        "    #S*B ==> S *B *E\n",
        "    # print(\"\\nTarget\", word_packed.__sizeof__(), type(word_packed))#, word_packed\n",
        "    # print(\"word_packed type\", type(word_packed), word_packed.shape, \"length\", len(word_packed)) #word_packed type <class 'torch.Tensor'> torch.Size([256, 13])\n",
        "    input = self.embed(self.input) #learning as emdedding takes tensor as input not pack padd sequence so pack_pad needs to be done after embedding\n",
        "\n",
        "    # print(\"\\nTarget_embedding\", input.shape, type(input))# Target_embedding torch.Size([256, 13, 20]) <class 'torch.Tensor'>\n",
        "\n",
        "    # print(\"batch size and hidden_size\", batch_size, hidden_size)#batch size and hidden_size 256 20\n",
        "    hidden_0 = self.hidden_init(batch_size)#learnig as defined fucntion is inside this class so use self.funcition to call that function.\n",
        "    # print(\"\\n input and Hidden_0 size\", input.shape, hidden_0.shape)#torch.Size([256, 13, 20]) torch.Size([1, 256, 20])\n",
        "\n",
        "\n",
        "    output, h_n = self.grucell(input, hidden_0)\n",
        "    # print(\"output shape\", output.shape, \"h_n shape\", h_n.shape)#output shape torch.Size([256, 13, 20]) h_n shape torch.Size([1, 256, 20])\n",
        "    # print(\"output type\", type(output), \"h_n type\", type(h_n))\n",
        "\n",
        "    logit = self.fc(h_n)[0]\n",
        "    # logit2 =logit[0]\n",
        "    # print(\"logit value and size\", logit.shape, type(logit))\n",
        "    return logit\n",
        "\n",
        "  def hidden_init(self, batch_size):\n",
        "    h_0 = torch.zeros(self.num_layer*self.num_bidirection, batch_size, hidden_size)\n",
        "    return h_0\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lGNxdHHMdkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some utility functions\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def create_variable(tensor):\n",
        "    # Do cuda() before wrapping with variable\n",
        "    if torch.cuda.is_available():\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7m7E_KmFjUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(len(N_countries))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9tn9StnzxD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "  # for epoch in rancge(N_epoch+1):\n",
        "  total_loss = 0\n",
        "  for i, (words, countries) in enumerate(train_loader):\n",
        "    # make the word proper format ie: words, countryuy to  nos then padding then tensor. \n",
        "    # print(\"type make_variable\", type(words), type(countries))\n",
        "    # print(\"len \", batch_size, hidden_size, N_countries, N_layers)\n",
        "    words, target = make_variable(words, countries)\n",
        "\n",
        "    output = model(words)\n",
        "    # print(\"shape of output and target\", output.shape, target.shape)# output and target torch.Size([1, 256, 18]) torch.Size([256])\n",
        "    loss = criterion(output, target) # llearning it should be : output shape and type torch.Size([256, 18]) torch.Size([256])\n",
        "    total_loss += loss.item()\n",
        "    print(\"\\nloss\", total_loss)\n",
        "    optimizer.zero_grad() # learnig we are making gradindier parameter of model not loss \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        " \n",
        "    # if i % 10 == 0:\n",
        "    #       print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.2f}'.format(\n",
        "    #           time_since(start), epoch,  i*len(words), len(train_loader.dataset),\n",
        "    #           100. * i *len(words) )) #len(words) / len(train_loader.dataset) total_loss / i * len(words)\n",
        "          \n",
        "    \n",
        "    if i % 2 == 0:\n",
        "       print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.2f}'.format(time_since(start), epoch,  i *\n",
        "                len(words), len(train_loader.dataset), 100. * i * len(words) / len(train_loader.dataset),\n",
        "                total_loss ))#/ i * len(words)   \n",
        "    return total_loss "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76IibzwSU-Eu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(name=None): # learning name= None we can pass a variable or not as per out wish\n",
        "  # print(\"\\n\\nNow we are in TEST\")\n",
        "  if name:\n",
        "    wordm, target = make_variable(name, [])\n",
        "    output = model(wordm)\n",
        "    prediction = output.data.max(1,keepdim=True)[1]\n",
        "    country_id = prediction.numpy()[0][0]\n",
        "    print(name, \"is \", database_train.get_country_by_id(country_id))\n",
        "    # pred_country = output.max(1) # we got cuntry id now\n",
        "    # print(\"prediction\", prediction.shape, prediction)\n",
        "    # print(\"\\npred_country\", pred_country)\n",
        "    return\n",
        "\n",
        "    \n",
        "    # print(\"shape of output and target\", output.shape, target.shape)# output and target torch.Size([1, 256, 18]) torch.Size([256])\n",
        "  print(\"evaluating the trained model....\")\n",
        "  correct = 0\n",
        "  train_data_size = len(test_loader.dataset)\n",
        "\n",
        "  for id, (wordm, countries) in enumerate(test_loader):\n",
        "    words, target = make_variable(wordm, countries)\n",
        "\n",
        "    output = model(words)\n",
        "    #mine pred_country = pred.max(1) # we got cuntry id now\n",
        "    pred = output.data.max(1, keepdim=True)[1]\n",
        "    # correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "    correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "    \n",
        "  print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        correct, train_data_size, 100. * correct / train_data_size))\n",
        "  # correct += \n",
        "\n",
        "  #   # find loss\n",
        "\n",
        "  #   if (pred_country == target):\n",
        "  #     loss_value = loss_value\n",
        "  #   else:\n",
        "  #     loss_value = loss_value +1\n",
        "\n",
        "  # return loss_value"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LSvt7qatduk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "bdc5d22c-9904-4738-cde7-f26a04d40157"
      },
      "source": [
        "#final execution funciton\n",
        "if __name__ == '__main__':\n",
        "  model = net(N_chars, hidden_size, N_countries, N_layers)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  start = time.time()\n",
        "  print(\"Traing for %d epochs ...:\" % N_epoch)\n",
        "  for epoch in range(1, N_epoch+1):\n",
        "    # Train cycle\n",
        "    train()\n",
        "    # Testing cycle\n",
        "    # print(\"Moving to the test\")\n",
        "    test()\n",
        "\n",
        "    #testing several samples\n",
        "  test(\"Sung\")\n",
        "  test(\"Jungwoo\")\n",
        "  test(\"Soojin\")\n",
        "  test(\"Nako\")\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traing for 5 epochs ...:\n",
            "\n",
            "loss 3.008824348449707\n",
            "[0m 0s] Train Epoch: 1 [0/13374 (0%)]\tLoss: 3.01\n",
            "evaluating the trained model....\n",
            "\n",
            "Test set: Accuracy: 25/6700 (0%)\n",
            "\n",
            "\n",
            "loss 2.978205680847168\n",
            "[0m 0s] Train Epoch: 2 [0/13374 (0%)]\tLoss: 2.98\n",
            "evaluating the trained model....\n",
            "\n",
            "Test set: Accuracy: 25/6700 (0%)\n",
            "\n",
            "\n",
            "loss 2.9648897647857666\n",
            "[0m 0s] Train Epoch: 3 [0/13374 (0%)]\tLoss: 2.96\n",
            "evaluating the trained model....\n",
            "\n",
            "Test set: Accuracy: 25/6700 (0%)\n",
            "\n",
            "\n",
            "loss 2.940828323364258\n",
            "[0m 0s] Train Epoch: 4 [0/13374 (0%)]\tLoss: 2.94\n",
            "evaluating the trained model....\n",
            "\n",
            "Test set: Accuracy: 25/6700 (0%)\n",
            "\n",
            "\n",
            "loss 2.9435672760009766\n",
            "[0m 0s] Train Epoch: 5 [0/13374 (0%)]\tLoss: 2.94\n",
            "evaluating the trained model....\n",
            "\n",
            "Test set: Accuracy: 25/6700 (0%)\n",
            "\n",
            "Sung is  French\n",
            "Jungwoo is  Korean\n",
            "Soojin is  French\n",
            "Nako is  Polish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C114-pbMtdpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eETP6bqmXu4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pred(word):\n",
        "  output = model(word)\n",
        "  country = train_database.get_country_by_id(output.max(1))\n",
        "  return country"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veyLI6eEu3sd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #test loader\n",
        "# database_test = Name('/content/drive/My Drive/AI/basic/data/names_train/names_test.csv')\n",
        "# test_loader = DataLoader(dataset=database_test, batch_size= BS, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iowTb3mpu4Cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB7yA-tEOwqL",
        "colab_type": "text"
      },
      "source": [
        "working on asc and tranposzing .t() is it tup[le , tensor or what then next action\n",
        "1. remove asc use direct embedding\n",
        "2. use asc only noo emabeddinbg as asx itself is embedding\n",
        "by my case ASC is streamline word length as well. so try to keep asx alive for time being untill batch size is 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VwFV7Z_eWCF",
        "colab_type": "text"
      },
      "source": [
        "task3: model: embedding, rnn, linear layer \n",
        "task4: optimizer\n",
        "task5: training\n",
        "taask6: test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqEmGV5GkBD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class net(torch.nn.Module): #learning we can get fuction created inside database (it can be train or test)\n",
        "#   def __init__(self):\n",
        "#     super(net,self).__init__()\n",
        "#     self.word_len = 18 # seq chenged from 15 to 18 for test\n",
        "#     self.vocab_size = self.word_len # no of character per word\n",
        "#     self.embedding_dim = 6 # how many diamention it wants\n",
        "#     self.input_size = self.embedding_dim #6\n",
        "#     self.hiddenO = self.input_size #6 dim\n",
        "#     self.hidden_size = self.embedding_dim #6\n",
        "#     self.batch = 1\n",
        "#     self.num_layer =1\n",
        "#     self.num_class = 18 #num of contries\n",
        "\n",
        "#     #embedding  @learning use tensor conversion al last meand at GRU ot RNN time not beofore that as arugment of other function (like embed) are not tensors\n",
        "#     self.embed = nn.Embedding(num_embeddings = self.vocab_size, embedding_dim = self.embedding_dim) # num_enn = input size word lenth and embedd is output size ie hidden size\n",
        "#     #gru    \n",
        "#     self.grucell = nn.GRU(input_size = self.input_size, hidden_size = self.hidden_size, num_layers =self.num_layer, batch_first = True)\n",
        "#     #fc\n",
        "#     self.fc = nn.Linear(in_features= self.embedding_dim, out_features= self.num_class)\n",
        "  \n",
        "#   def forward(self, word, country): # 10 word and 10 country\n",
        "#     # word, contry = data\n",
        "#     print(\"word\", word) # word type is tuple of B*S\n",
        "#     #converting work in to integer ASCII for input and outpuut ord # learning asc is not required as embedding can take words directly\n",
        "     \n",
        "#     word_ascii, w_len = self.asc(word, self.word_len) #in: BS * word and out: BS * ASCII with padding\n",
        "#     print(\"\\n word_ascii\", word_ascii,  \"\\n w_len\", w_len)\n",
        "#       # challenge we need some padding. keep every word as 18 character long.\n",
        "#       #learning embedding needs word(s) * BS  ir S*B not B*S\n",
        "      \n",
        "#     word_ascii = list(zip(*word_ascii)) # in B*S ot S*B\n",
        "#     # print(\"word ascii \", word_ascii)\n",
        "#     word_ascii = torch.LongTensor(word_ascii)\n",
        "\n",
        "#     #@ learning so word equalization you shoiuld do after embedfding  not before that. otherwise embedding will not ass anything\n",
        "\n",
        "#     word_embed = self.embed(word_ascii) #embedding e extra in matrix out : BS* word_len*E\n",
        "#       #rnn in and out rnn_in BS*Seq*inu_D(e) and rnn_out BS*Seq*hiddenO\n",
        "#       # input = B*Seq_len*input, hidden = num_lauyer* batch,hidden_size\n",
        "#     input = word_embed # B*S*I\n",
        "#     hidden = self.hidden_init()\n",
        "#     out, h_n = self.grucell(input, hidden) #out B*S*hidden [1 10 6] , h_n = num_layerxBxHidden [1 1 6]\n",
        "\n",
        "#     #FC linear layer in Seq to out no of classor contry(-1, nod of class or country)\n",
        "#     out = self.fc(h_n) #in [1 1 6] out [1 1 18]\n",
        "#     out = out.view(-1, self.num_class)\n",
        "#     # out - out.t()\n",
        "#     country_ids = self.country2tensor(country) # 18 contoury with each 15 word\n",
        "#     # country = country.t()#view(-1)\n",
        "#     # print(\"country ids\", country_ids.shape)\n",
        "#     # country = country.squeeze_()\n",
        "#     # out = out.squeeze_()\n",
        "#     return out, country_ids\n",
        "\n",
        "#     #output output of DC \n",
        "#   def hidden_init(self):\n",
        "#     hid = torch.zeros(self.num_layer, self.batch, self.hiddenO)\n",
        "#     return hid\n",
        "#   def country2tensor(self, countries):\n",
        "#     # print(\"contries size\", len(countries))\n",
        "#     country_ids = [database.get_id_by_country(each_country) for each_country in countries]\n",
        "#     # print(\"contries ids\", len(country_ids))\n",
        "#     return torch.LongTensor(country_ids)\n",
        "\n",
        "#   def asc(self, words, word_len):\n",
        "#     word_len = word_len\n",
        "#     sorks3 =  []\n",
        "#     # sorks3 = torch.zeros[len(words)]\n",
        "#     for word in words:\n",
        "#       # # for cha in word:\n",
        "#       # sorks = [ord(cha) for cha in word]\n",
        "#       # sorks2 = [sorks + [0]*(word_len-len(sorks))] \n",
        "#       # sorks3 = Variable(torch.LongTensor([sorks2]))\n",
        "#       # sorks4 = sorks4.append(sorks3)\n",
        "      \n",
        "#       # sorks3.append(Variable(torch.LongTensor([[ord(cha) for cha in word] + [0]*(word_len-len([ord(cha) for cha in word]))])))\n",
        "#       sorks3.append([[ord(cha) for cha in word] + [0]*(word_len-len([ord(cha) for cha in word]))])\n",
        "#       # i = i+1\n",
        "#     return sorks3, len(sorks3) \n",
        "\n",
        "\n",
        "# model = net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMdjdhGMs1FQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class net(torch.nn.Module): #learning we can get fuction created inside database (it can be train or test)\n",
        "#   def __init__(self):\n",
        "#     super(net,self).__init__()\n",
        "#     self.word_len = 18 # seq chenged from 15 to 18 for test\n",
        "#     self.vocab_size = self.word_len # no of character per word\n",
        "#     self.embedding_dim = 6 # how many diamention it wants\n",
        "#     self.input_size = self.embedding_dim #6\n",
        "#     self.hiddenO = self.input_size #6 dim\n",
        "#     self.hidden_size = self.embedding_dim #6\n",
        "#     self.batch = 1\n",
        "#     self.num_layer =1\n",
        "#     self.num_class = 18 #num of contries\n",
        "\n",
        "#     #embedding  @learning use tensor conversion al last meand at GRU ot RNN time not beofore that as arugment of other function (like embed) are not tensors\n",
        "#     self.embed = nn.Embedding(num_embeddings = self.vocab_size, embedding_dim = self.embedding_dim) # num_enn = input size word lenth and embedd is output size ie hidden size\n",
        "#     #gru    \n",
        "#     self.grucell = nn.GRU(input_size = self.input_size, hidden_size = self.hidden_size, num_layers =self.num_layer, batch_first = True)\n",
        "#     #fc\n",
        "#     self.fc = nn.Linear(in_features= self.embedding_dim, out_features= self.num_class)\n",
        "  \n",
        "#   def forward(self, word, country): # 10 word and 10 country\n",
        "#     # word, contry = data\n",
        "#     print(\"word\", word) # word type is tuple of B*S\n",
        "\n",
        "#     # challenge we need some padding. keep every word as 18 character long.\n",
        "#     #learning embedding needs word(s) * BS  ir S*B not B*S\n",
        "      \n",
        "#     word_ascii = list(zip(*word_ascii)) # in B*S ot S*B\n",
        "#     # print(\"word ascii \", word_ascii)\n",
        "#     word_ascii = torch.LongTensor(word_ascii)\n",
        "\n",
        "#     #@ learning so word equalization you shoiuld do after embedfding  not before that. otherwise embedding will not ass anything\n",
        "    \n",
        "#     word_embed = self.embed(word_ascii) #embedding e extra in matrix out : BS* word_len*E\n",
        "#       #rnn in and out rnn_in BS*Seq*inu_D(e) and rnn_out BS*Seq*hiddenO\n",
        "#       # input = B*Seq_len*input, hidden = num_lauyer* batch,hidden_size\n",
        "#     input = word_embed # B*S*I\n",
        "#     hidden = self.hidden_init()\n",
        "#     out, h_n = self.grucell(input, hidden) #out B*S*hidden [1 10 6] , h_n = num_layerxBxHidden [1 1 6]\n",
        "\n",
        "#     #FC linear layer in Seq to out no of classor contry(-1, nod of class or country)\n",
        "#     out = self.fc(h_n) #in [1 1 6] out [1 1 18]\n",
        "#     out = out.view(-1, self.num_class)\n",
        "#     # out - out.t()\n",
        "#     country_ids = self.country2tensor(country) # 18 contoury with each 15 word\n",
        "#     # country = country.t()#view(-1)\n",
        "#     # print(\"country ids\", country_ids.shape)\n",
        "#     # country = country.squeeze_()\n",
        "#     # out = out.squeeze_()\n",
        "#     return out, country_ids\n",
        "\n",
        "#     #output output of DC \n",
        "#   def hidden_init(self):\n",
        "#     hid = torch.zeros(self.num_layer, self.batch, self.hiddenO)\n",
        "#     return hid\n",
        "#   def country2tensor(self, countries):\n",
        "#     # print(\"contries size\", len(countries))\n",
        "#     country_ids = [database.get_id_by_country(each_country) for each_country in countries]\n",
        "#     # print(\"contries ids\", len(country_ids))\n",
        "#     return torch.LongTensor(country_ids)\n",
        "\n",
        "#   def asc(self, words, word_len):\n",
        "#     word_len = word_len\n",
        "#     sorks3 =  []\n",
        "#     # sorks3 = torch.zeros[len(words)]\n",
        "#     for word in words:\n",
        "#       # # for cha in word:\n",
        "#       # sorks = [ord(cha) for cha in word]\n",
        "#       # sorks2 = [sorks + [0]*(word_len-len(sorks))] \n",
        "#       # sorks3 = Variable(torch.LongTensor([sorks2]))\n",
        "#       # sorks4 = sorks4.append(sorks3)\n",
        "      \n",
        "#       # sorks3.append(Variable(torch.LongTensor([[ord(cha) for cha in word] + [0]*(word_len-len([ord(cha) for cha in word]))])))\n",
        "#       sorks3.append([[ord(cha) for cha in word] + [0]*(word_len-len([ord(cha) for cha in word]))])\n",
        "#       # i = i+1\n",
        "#     return sorks3, len(sorks3) \n",
        "\n",
        "# model = net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlsCGYjf5XtB",
        "colab_type": "text"
      },
      "source": [
        "Process contry differently as each contry will have unique no (one number not as matrix as word (N * no of class) will be comparing or predicting the perticular class. so its a one idea to keep perticular class as one number rather than keeping it as one class as array.\n",
        "\n",
        "TODO each contry as one unique no.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bkGf6xpwy60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = .0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdzgP6gTxXdp",
        "colab_type": "text"
      },
      "source": [
        "Trainig\n",
        "\n",
        "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8YXuz0AwyyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for epoch in range(1):\n",
        "#   total_loss = 0 \n",
        "#   # batch_id = 0 \n",
        "#   for batch_idx, (word, country) in enumerate(train_loader): #@ learning loader is sending tuple on each count\n",
        "#     print(\"word database\", type(word), \"countey database\", type(country))\n",
        "#     out, country = model(word, country)\n",
        "#     print(\"out shape\", out.shape, \"countey shape\", country.shape)\n",
        "#     optimizer.zero_grad()\n",
        "#     loss = Criterion(out, country)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     if batch_idx % 500  == 0:\n",
        "#         print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "#                 epoch, batch_idx * len(word), len(train_loader.dataset),\n",
        "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhLo9HN7AiZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n= [\"name\"]\n",
        "# # n[1]\n",
        "# # ord(n[1])\n",
        "# print(asc(n,10)[0].size())\n",
        "# a, b = asc(n,10)\n",
        "# embed=nn.Embedding(200,3)\n",
        "# emb2 = embed(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOS2GPINm6EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(emb2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoqHLwaEAqmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # sorks= []\n",
        "  # i=0\n",
        "  # def asc(words, word_len):\n",
        "  #   word_len = word_len\n",
        "\n",
        "  #   for word in words:\n",
        "  #     # for cha in word:\n",
        "  #     sorks = [ord(cha) for cha in word]\n",
        "  #     sorks2 = sorks + [0]*(word_len-len(sorks)) \n",
        "  #     sorks3 = Variable(torch.LongTensor([sorks2]))\n",
        "  #   return sorks3, len(sorks2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e02g7x7BWQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIyELK02B22g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n1 =asc(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzTcqLLZTcoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n2 = concat(n1, zeros[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqvdvxq4Th8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# like_zero[2]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}